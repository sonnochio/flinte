citations_fetched,id,published,publisher,source,categories,pdf_url,doi,title,abstract,all_categories,authors,citations
True,http://arxiv.org/abs/1909.03550v1,2019-09-08T21:49:42Z,,arXiv,cs.LG,http://arxiv.org/pdf/1909.03550v1.pdf,1909.03550v1,Lecture Notes: Optimization for Machine Learning,"Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley.","cs.LG, stat.ML",Elad Hazan,[]
True,http://arxiv.org/abs/1811.04422v1,2018-11-11T14:28:34Z,,arXiv,cs.LG,http://arxiv.org/pdf/1811.04422v1.pdf,1811.04422v1,An Optimal Control View of Adversarial Machine Learning,"I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.","cs.LG, stat.ML",Xiaojin Zhu,[]
True,http://arxiv.org/abs/1707.04849v1,2017-07-16T09:15:08Z,,arXiv,cs.LG,http://arxiv.org/pdf/1707.04849v1.pdf,1707.04849v1,Minimax deviation strategies for machine learning and recognition with   short learning samples,The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.,cs.LG,"Michail Schlesinger, Evgeniy Vodolazskiy",[]
True,http://arxiv.org/abs/1909.09246v1,2019-09-19T22:02:00Z,,arXiv,cs.LG,http://arxiv.org/pdf/1909.09246v1.pdf,1909.09246v1,Machine Learning for Clinical Predictive Analytics,"In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.","cs.LG, stat.ML",Wei-Hung Weng,[]
True,http://arxiv.org/abs/2301.09753v1,2023-01-23T22:54:34Z,,arXiv,cs.LG,http://arxiv.org/pdf/2301.09753v1.pdf,2301.09753v1,Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs,"Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit.","cs.LG, cs.SE","Samiyuru Menik, Lakshmish Ramaswamy",[]
True,http://arxiv.org/abs/0904.3664v1,2009-04-23T11:40:57Z,,arXiv,cs.LG,http://arxiv.org/pdf/0904.3664v1.pdf,0904.3664v1,Introduction to Machine Learning: Class Notes 67577,"Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",cs.LG,Amnon Shashua,[]
True,http://arxiv.org/abs/2012.04105v1,2020-12-07T23:10:51Z,,arXiv,cs.LG,http://arxiv.org/pdf/2012.04105v1.pdf,2012.04105v1,The Tribes of Machine Learning and the Realm of Computer Architecture,"Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.","cs.LG, cs.AR","Ayaz Akram, Jason Lowe-Power",[]
True,http://arxiv.org/abs/2204.07492v2,2022-04-15T14:48:04Z,American Meteorological Society,arXiv,physics.ao-ph,http://arxiv.org/pdf/2204.07492v2.pdf,10.1175/waf-d-22-0070.1,"A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning","Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.","physics.ao-ph, cs.LG","Randy J. Chase, David R. Harrison, Amanda Burke, Gary M. Lackmann, Amy McGovern","['10.25080/majora-92bf1922-00a', '10.1117/12.795737', '10.1117/12.795570']"
True,http://arxiv.org/abs/1911.06612v1,2019-11-12T10:49:55Z,,arXiv,cs.LG,http://arxiv.org/pdf/1911.06612v1.pdf,1911.06612v1,Position Paper: Towards Transparent Machine Learning,"Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.",cs.LG,Dustin Juliano,[]
True,http://arxiv.org/abs/1909.01866v1,2019-09-02T20:36:19Z,,arXiv,cs.LG,http://arxiv.org/pdf/1909.01866v1.pdf,1909.01866v1,Understanding Bias in Machine Learning,"Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.","cs.LG, stat.ML","Jindong Gu, Daniela Oelke",[]
True,http://arxiv.org/abs/1903.08801v1,2019-03-21T02:17:08Z,,arXiv,cs.LG,http://arxiv.org/pdf/1903.08801v1.pdf,1903.08801v1,A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain,"Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between machine learning technology and blockchain technology. Previously, machine learning and blockchain have been considered two independent technologies without an obvious link. Second, it proposes a unified analytical framework for trustable machine learning by using blockchain technology. This unified framework solves both the trustability and automation issues in machine learning. Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach. The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis.","cs.LG, cs.CR",Tao Wang,[]
True,http://arxiv.org/abs/1707.09562v3,2017-07-29T21:59:18Z,,arXiv,cs.DC,http://arxiv.org/pdf/1707.09562v3.pdf,1707.09562v3,MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?,"We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current machine learning clouds on real-world machine learning workloads?   We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing machine learning clouds and points out potential future directions for improvement.","cs.DC, cs.LG, stat.ML","Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, Ce Zhang",[]
True,http://arxiv.org/abs/2108.07915v1,2021-08-18T00:57:06Z,,arXiv,cs.LG,http://arxiv.org/pdf/2108.07915v1.pdf,2108.07915v1,Data Pricing in Machine Learning Pipelines,"Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.",cs.LG,"Zicun Cong, Xuan Luo, Pei Jian, Feida Zhu, Yong Zhang",[]
True,http://arxiv.org/abs/1907.08908v1,2019-07-21T04:03:36Z,,arXiv,cs.LG,http://arxiv.org/pdf/1907.08908v1.pdf,1907.08908v1,Techniques for Automated Machine Learning,"Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.","cs.LG, cs.AI, stat.ML","Yi-Wei Chen, Qingquan Song, Xia Hu",[]
True,http://arxiv.org/abs/2312.03120v1,2023-12-05T20:40:05Z,,arXiv,cs.LG,http://arxiv.org/pdf/2312.03120v1.pdf,2312.03120v1,"The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning","With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.","cs.LG, cs.AI, cs.DC","Omer Subasi, Oceane Bel, Joseph Manzano, Kevin Barker",[]
True,http://arxiv.org/abs/2206.07090v2,2022-05-08T03:47:30Z,,arXiv,cs.DC,http://arxiv.org/pdf/2206.07090v2.pdf,2206.07090v2,Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark,"With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform. The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms.",cs.DC,Jiajun Shen,[]
True,http://arxiv.org/abs/1507.02188v1,2015-07-08T15:07:39Z,,arXiv,stat.ML,http://arxiv.org/pdf/1507.02188v1.pdf,1507.02188v1,AutoCompete: A Framework for Machine Learning Competition,"In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches.","stat.ML, cs.LG","Abhishek Thakur, Artus Krohn-Grimberghe",[]
True,http://arxiv.org/abs/1212.2686v1,2012-12-12T01:59:27Z,,arXiv,stat.ML,http://arxiv.org/pdf/1212.2686v1.pdf,1212.2686v1,Joint Training of Deep Boltzmann Machines,"We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.","stat.ML, cs.LG","Ian Goodfellow, Aaron Courville, Yoshua Bengio",[]
True,http://arxiv.org/abs/2001.04942v2,2020-01-14T17:56:16Z,,arXiv,cs.LG,http://arxiv.org/pdf/2001.04942v2.pdf,2001.04942v2,Private Machine Learning via Randomised Response,We introduce a general learning framework for private machine learning based on randomised response. Our assumption is that all actors are potentially adversarial and as such we trust only to release a single noisy version of an individual's datapoint. We discuss a general approach that forms a consistent way to estimate the true underlying machine learning model and demonstrate this in the case of logistic regression.,"cs.LG, stat.ML",David Barber,[]
True,http://arxiv.org/abs/1607.02450v2,2016-07-08T16:55:31Z,,arXiv,stat.ML,http://arxiv.org/pdf/1607.02450v2.pdf,1607.02450v2,Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications,"This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York.","stat.ML, cs.CY, cs.LG",Kush R. Varshney,[]
True,http://arxiv.org/abs/2007.01503v1,2020-07-03T05:26:02Z,,arXiv,cs.LG,http://arxiv.org/pdf/2007.01503v1.pdf,2007.01503v1,Mathematical Perspective of Machine Learning,"We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective.","cs.LG, stat.ML, 68T07",Yarema Boryshchak,[]
True,http://arxiv.org/abs/1911.00776v1,2019-11-02T19:53:32Z,,arXiv,cs.LG,http://arxiv.org/pdf/1911.00776v1.pdf,1911.00776v1,Ten-year Survival Prediction for Breast Cancer Patients,This report assesses different machine learning approaches to 10-year survival prediction of breast cancer patients.,"cs.LG, stat.ML","Changmao Li, Han He, Yunze Hao, Caleb Ziems",[]
True,http://arxiv.org/abs/1906.06821v2,2019-06-17T02:54:51Z,,arXiv,cs.LG,http://arxiv.org/pdf/1906.06821v2.pdf,1906.06821v2,A Survey of Optimization Methods from a Machine Learning Perspective,"Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.","cs.LG, math.OC, stat.ML","Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao",[]
True,http://arxiv.org/abs/2011.11819v1,2020-11-24T00:52:49Z,,arXiv,cs.LG,http://arxiv.org/pdf/2011.11819v1.pdf,2011.11819v1,When Machine Learning Meets Privacy: A Survey and Outlook,"The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.","cs.LG, cs.AI, cs.CR","Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin",[]
True,http://arxiv.org/abs/2004.00993v2,2020-03-31T18:08:23Z,,arXiv,cs.LG,http://arxiv.org/pdf/2004.00993v2.pdf,2004.00993v2,Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.","cs.LG, cs.AI","Xiao Lei Zhang, Anish Agarwal",[]
True,http://arxiv.org/abs/2009.11087v1,2020-09-23T12:14:05Z,,arXiv,stat.ML,http://arxiv.org/pdf/2009.11087v1.pdf,2009.11087v1,Probabilistic Machine Learning for Healthcare,"Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.","stat.ML, cs.CY, cs.LG","Irene Y. Chen, Shalmali Joshi, Marzyeh Ghassemi, Rajesh Ranganath",[]
True,http://arxiv.org/abs/2303.18087v1,2023-03-31T14:24:06Z,,arXiv,cs.LG,http://arxiv.org/pdf/2303.18087v1.pdf,2303.18087v1,Evaluation Challenges for Geospatial ML,"As geospatial machine learning models and maps derived from their predictions are increasingly used for downstream analyses in science and policy, it is imperative to evaluate their accuracy and applicability. Geospatial machine learning has key distinctions from other learning paradigms, and as such, the correct way to measure performance of spatial machine learning outputs has been a topic of debate. In this paper, I delineate unique challenges of model evaluation for geospatial machine learning with global or remotely sensed datasets, culminating in concrete takeaways to improve evaluations of geospatial model performance.","cs.LG, stat.ML",Esther Rolf,[]
True,http://arxiv.org/abs/2401.11351v2,2024-01-21T00:19:16Z,IOP Publishing,arXiv,quant-ph,http://arxiv.org/pdf/2401.11351v2.pdf,10.1088/1361-6633/ad7f69,A comprehensive review of Quantum Machine Learning: from NISQ to Fault   Tolerance,"Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.","quant-ph, cs.AI, cs.LG, stat.ML","Yunfei Wang, Junyu Liu","['10.1038/nphys3272', '10.1137/18m120275x', '10.1145/3313276.3316378', '10.1162/089976698300017746', '10.1088/2058-9565/ac3e54', '10.1088/2632-2153/abf3ac', '10.1140/epjc/s10052-019-6607-9', '10.22331/q-2021-10-05-558', '10.1088/2058-9565/ac7d06', '10.1088/1367-2630/17/12/123010', '10.22331/q-2020-04-20-256', '10.21203/rs.3.rs-3023549/v1', '10.1038/s41534-019-0157-8', '10.1103/physrevresearch.3.033083', '10.1038/nature23474', '10.1007/s42484-023-00132-1', '10.1103/physrevlett.127.120502', '10.22331/q-2023-11-22-1188', '10.1103/physreva.105.062431', '10.1088/2058-9565/acb56a', '10.1038/s41534-023-00801-w', '10.22331/q-2023-07-13-1060', '10.1038/s41467-021-21728-w', '10.1038/s41534-022-00611-6', '10.1038/s43588-022-00311-3', '10.22331/q-2023-04-13-974', '10.1103/prxquantum.2.030348', '10.3390/e20080583', '10.1145/3357713.3384314', '10.1137/16m1087072', '10.1098/rspa.2017.0551', '10.1103/prxquantum.2.010324', '10.1007/jhep11(2017)048', '10.22331/q-2021-11-26-592', '10.1103/prxquantum.3.040329', '10.22331/qv-2020-03-17-32', '10.1038/s42254-022-00535-2', '10.7566/jpsj.90.032001', '10.1038/s41534-019-0240-1', '10.1145/3313276.3316366', '10.1088/1367-2630/14/10/103013', '10.1038/s41467-019-10988-2', '10.1145/237814.237866', '10.1126/science.1257219', '10.1016/j.physrep.2008.09.003', '10.1103/physrevresearch.4.043100', '10.1103/physrevlett.123.250501', '10.1103/physrevlett.126.140502', '10.1103/physrevlett.103.150502', '10.1137/1.9781611974782.105', '10.1038/s41586-019-0980-2', '10.1103/prxquantum.3.010313', '10.1038/s41567-020-0932-7', '10.1038/s41467-021-22539-9', '10.1103/physrevlett.127.030503', '10.1103/physrevlett.126.190505', '10.1126/science.abn7293', '10.1126/science.abk3333', '10.1088/2058-9565/abdbc9', '10.1561/2200000058', '10.1038/s41467-023-36159-y', '10.1038/nature23879', '10.1103/physrevlett.120.110501', '10.22331/q-2022-08-16-776', '10.1088/2058-9565/abfc94', '10.1063/1.5089550', '10.1038/s42254-020-0230-4', '10.1038/s43588-023-00467-6', '10.1103/physreva.102.032420', '10.1038/s41534-019-0167-6', '10.1021/acs.jctc.8b01004', '10.1103/physrevresearch.2.023074', '10.1103/physrevd.98.086026', '10.1103/physrevresearch.2.043164', '10.1103/prxquantum.3.030323', '10.1073/pnas.2026805118', '10.1038/s41567-021-01287-z', '10.1126/science.273.5278.1073', '10.1038/nphys3029', '10.1103/physreva.101.010301', '10.1038/s41534-019-0187-2', '10.1088/1367-2630/18/2/023023', '10.1038/s41467-018-07090-4', '10.1038/nature00801', '10.1103/physrevresearch.1.013006', '10.1088/2058-9565/aab822', '10.1103/physrevresearch.2.043158', '10.1038/nature08967', '10.22331/q-2021-01-28-391', '10.1103/physrevresearch.3.033090', '10.22331/q-2020-02-06-226', '10.1038/ncomms5213', '10.22331/q-2018-08-06-79', '10.1103/physrevlett.113.130503', '10.1007/jhep04(2017)121', '10.1080/00107514.2019.1667078', '10.1103/physreva.94.022342', '10.1088/1367-2630/ab784c', '10.1103/physrevlett.128.180505', '10.1103/physrevlett.128.070501', '10.1103/physrevlett.128.070501aaaa', '10.1103/physrevlett.119.180511', '10.22331/q-2020-05-25-269', '10.1103/prxquantum.2.010307', '10.1103/physrevlett.122.060504', '10.1007/bf01609348', '10.22331/q-2020-08-31-314', '10.1103/physrevlett.127.060503', '10.1103/physrevresearch.2.043364', '10.1002/qua.21198', '10.1103/physrevlett.119.180509', '10.1038/s41467-024-49287-w', '10.1088/1751-8121/abfac7', '10.1038/s41467-021-27045-6', '10.22331/q-2021-08-30-531', '10.1103/physrevresearch.2.033446', '10.1103/physreva.92.042303', '10.1103/physrevlett.112.190501', '10.1088/1367-2630/17/2/022005', '10.1103/physrevlett.120.050502', '10.21203/rs.3.rs-3691498/v1', '10.1109/tkde.2019.2937491', '10.1038/srep03589', '10.1088/2632-2153/abcb50']"
True,http://arxiv.org/abs/2003.05155v2,2020-03-11T08:25:49Z,,arXiv,cs.LG,http://arxiv.org/pdf/2003.05155v2.pdf,2003.05155v2,Towards CRISP-ML(Q): A Machine Learning Process Model with Quality   Assurance Methodology,"Machine learning is an established and frequently used technique in industry and academia but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners have a need for guidance throughout the life cycle of a machine learning application to meet business expectations. We therefore propose a process model for the development of machine learning applications, that covers six phases from defining the scope to maintaining the deployed machine learning application. The first phase combines business and data understanding as data availability oftentimes affects the feasibility of the project. The sixth phase covers state-of-the-art approaches for monitoring and maintenance of a machine learning applications, as the risk of model degradation in a changing environment is eminent. With each task of the process, we propose quality assurance methodology that is suitable to adress challenges in machine learning development that we identify in form of risks. The methodology is drawn from practical experience and scientific literature and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support but lacks to address machine learning specific tasks. Our work proposes an industry and application neutral process model tailored for machine learning applications with focus on technical tasks for quality assurance.","cs.LG, cs.SE, stat.ML","Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, Klaus-Robert Mueller",[]
True,http://arxiv.org/abs/1706.08001v1,2017-06-24T20:56:27Z,,arXiv,cs.AI,http://arxiv.org/pdf/1706.08001v1.pdf,1706.08001v1,Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of   learning relational order via reinforcement learning procedure?,"In this article, we extend the conventional framework of convolutional-Restricted-Boltzmann-Machine to learn highly abstract features among abitrary number of time related input maps by constructing a layer of multiplicative units, which capture the relations among inputs. In many cases, more than two maps are strongly related, so it is wise to make multiplicative unit learn relations among more input maps, in other words, to find the optimal relational-order of each unit. In order to enable our machine to learn relational order, we developed a reinforcement-learning method whose optimality is proven to train the network.","cs.AI, cs.LG, stat.ML",Zizhuang Wang,[]
True,http://arxiv.org/abs/2405.03720v1,2024-05-05T20:39:15Z,,arXiv,cs.LG,http://arxiv.org/pdf/2405.03720v1.pdf,2405.03720v1,Spatial Transfer Learning with Simple MLP,First step to investigate the potential of transfer learning applied to the field of spatial statistics,"cs.LG, stat.ME, stat.ML",Hongjian Yang,[]
True,http://arxiv.org/abs/1603.02185v1,2016-03-07T18:11:54Z,,arXiv,cs.LG,http://arxiv.org/pdf/1603.02185v1.pdf,1603.02185v1,Distributed Multi-Task Learning with Shared Representation,"We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure.","cs.LG, stat.ML","Jialei Wang, Mladen Kolar, Nathan Srebro",[]
True,http://arxiv.org/abs/1207.4676v2,2012-07-19T14:08:22Z,,arXiv,cs.LG,http://arxiv.org/pdf/1207.4676v2.pdf,1207.4676v2,Proceedings of the 29th International Conference on Machine Learning   (ICML-12),"This is an index to the papers that appear in the Proceedings of the 29th International Conference on Machine Learning (ICML-12). The conference was held in Edinburgh, Scotland, June 27th - July 3rd, 2012.","cs.LG, stat.ML","John Langford, Joelle Pineau",[]
True,http://arxiv.org/abs/1910.12387v2,2019-10-25T17:33:33Z,,arXiv,cs.LG,http://arxiv.org/pdf/1910.12387v2.pdf,1910.12387v2,Components of Machine Learning: Binding Bits and FLOPS,"Many machine learning problems and methods are combinations of three components: data, hypothesis space and loss function. Different machine learning methods are obtained as combinations of different choices for the representation of data, hypothesis space and loss function. After reviewing the mathematical structure of these three components, we discuss intrinsic trade-offs between statistical and computational properties of machine learning methods.",cs.LG,Alexander Jung,[]
True,http://arxiv.org/abs/2007.05479v1,2020-07-10T16:57:18Z,,arXiv,cs.AI,http://arxiv.org/pdf/2007.05479v1.pdf,2007.05479v1,Impact of Legal Requirements on Explainability in Machine Learning,"The requirements on explainability imposed by European laws and their implications for machine learning (ML) models are not always clear. In that perspective, our research analyzes explanation obligations imposed for private and public decision-making, and how they can be implemented by machine learning techniques.","cs.AI, cs.CY, cs.LG","Adrien Bibal, Michael Lognoul, Alexandre de Streel, Benoît Frénay",[]
True,http://arxiv.org/abs/2007.14206v1,2020-07-27T14:30:23Z,,arXiv,physics.comp-ph,http://arxiv.org/pdf/2007.14206v1.pdf,2007.14206v1,Machine Learning Potential Repository,"This paper introduces a machine learning potential repository that includes Pareto optimal machine learning potentials. It also shows the systematic development of accurate and fast machine learning potentials for a wide range of elemental systems. As a result, many Pareto optimal machine learning potentials are available in the repository from a website. Therefore, the repository will help many scientists to perform accurate and fast atomistic simulations.","physics.comp-ph, cond-mat.mtrl-sci, physics.chem-ph, physics.data-an",Atsuto Seko,[]
True,http://arxiv.org/abs/1908.04710v3,2019-08-13T15:52:31Z,,arXiv,cs.LG,http://arxiv.org/pdf/1908.04710v3.pdf,1908.04710v3,metric-learn: Metric Learning Algorithms in Python,"metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.","cs.LG, stat.ML","William de Vazelhes, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet",[]
True,http://arxiv.org/abs/2002.12364v1,2020-02-27T13:35:26Z,Springer US,arXiv,cs.LG,http://arxiv.org/pdf/2002.12364v1.pdf,10.1007/978-1-4615-5529-2,Theoretical Models of Learning to Learn,"A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an {\em environment} of related tasks, then it can {\em learn} its own bias by learning sufficiently many tasks from the environment. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.","cs.LG, stat.ML",Jonathan Baxter,[]
True,http://arxiv.org/abs/1509.00913v3,2015-09-03T01:30:29Z,,arXiv,cs.LG,http://arxiv.org/pdf/1509.00913v3.pdf,1509.00913v3,On-the-Fly Learning in a Perpetual Learning Machine,"Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic 'on the fly' learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework. We also explore the elegant duality of abstraction and synthesis: the Yin and Yang of deep learning.","cs.LG, 68Txx",Andrew J. R. Simpson,[]
True,http://arxiv.org/abs/1607.01400v1,2016-07-05T20:04:57Z,Springer Science and Business Media LLC,arXiv,stat.ML,http://arxiv.org/pdf/1607.01400v1.pdf,10.1007/s10994-016-5562-z,An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality   in Machine Learning,"We propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning, where we start the algorithm by aggregating the original data, solving the problem on aggregated data, and then in subsequent steps gradually disaggregate the aggregated data. We apply the algorithm to common machine learning problems such as the least absolute deviation regression problem, support vector machines, and semi-supervised support vector machines. We derive model-specific data aggregation and disaggregation procedures. We also show optimality, convergence, and the optimality gap of the approximated solution in each iteration. A computational study is provided.","stat.ML, cs.LG","Young Woong Park, Diego Klabjan","['10.1287/opre.13.1.82', '10.1145/1961189.1961199', '10.7551/mitpress/7503.003.0032', '10.1016/s0167-5060(08)70731-3', '10.1007/978-3-642-15880-3_28', '10.1002/net.3230130205', '10.1007/3-540-46014-4_31', '10.1287/opre.38.4.619', '10.1109/tpami.2014.2299812', '10.1007/978-1-4419-9154-6', '10.1287/opre.28.6.1450', '10.1098/rsta.1909.0016', '10.1145/1150402.1150486', '10.1287/opre.39.4.553', '10.1162/089976698300017467', '10.1016/0305-0548(87)90035-9', '10.2307/1914133', '10.1109/ijcnn.2005.1555965', '10.1145/956750.956786', '10.1007/s10618-005-0005-7', '10.1145/233269.233324']"
True,http://arxiv.org/abs/2202.10564v1,2022-02-21T22:45:59Z,,arXiv,cs.HC,http://arxiv.org/pdf/2202.10564v1.pdf,2202.10564v1,Human-in-the-loop Machine Learning: A Macro-Micro Perspective,"Though technical advance of artificial intelligence and machine learning has enabled many promising intelligent systems, many computing tasks are still not able to be fully accomplished by machine intelligence. Motivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans in the loop of machine learning and decision-making. In this paper, we provide a macro-micro review of human-in-the-loop machine learning. We first describe major machine learning challenges which can be addressed by human intervention in the loop. Then we examine closely the latest research and findings of introducing humans into each step of the lifecycle of machine learning. Finally, we analyze current research gaps and point out future research directions.",cs.HC,"Jiangtao Wang, Bin Guo, Liming Chen",[]
True,http://arxiv.org/abs/2407.05526v1,2024-07-08T00:19:43Z,,arXiv,cs.LG,http://arxiv.org/pdf/2407.05526v1.pdf,2407.05526v1,Can Machines Learn the True Probabilities?,"When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into AI models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them.","cs.LG, cs.AI, stat.ML",Jinsook Kim,[]
True,http://arxiv.org/abs/2001.09608v1,2020-01-27T07:26:12Z,,arXiv,cs.LG,http://arxiv.org/pdf/2001.09608v1.pdf,2001.09608v1,Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.","cs.LG, stat.ML",Changjian Li,[]
True,http://arxiv.org/abs/2110.12773v1,2021-10-25T10:05:11Z,,arXiv,cs.LG,http://arxiv.org/pdf/2110.12773v1.pdf,2110.12773v1,Scientific Machine Learning Benchmarks,"The breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning technologies for the analysis of very large experimental datasets. These datasets are typically generated by large-scale experimental facilities at national laboratories. In the context of science, scientific machine learning focuses on training machines to identify patterns, trends, and anomalies to extract meaningful scientific insights from such datasets. With a new generation of experimental facilities, the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis. At present, identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is still a challenge for scientists. This is due to many different machine learning frameworks, computer architectures, and machine learning models. Historically, for modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications, algorithms, and architectures. Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists. In this paper, we describe our approach to the development of scientific machine learning benchmarks and review other approaches to benchmarking scientific machine learning.","cs.LG, physics.comp-ph, I.2","Jeyan Thiyagalingam, Mallikarjun Shankar, Geoffrey Fox, Tony Hey",[]
True,http://arxiv.org/abs/1510.00633v1,2015-10-02T16:15:30Z,,arXiv,stat.ML,http://arxiv.org/pdf/1510.00633v1.pdf,1510.00633v1,Distributed Multitask Learning,"We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space,where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.","stat.ML, cs.LG","Jialei Wang, Mladen Kolar, Nathan Srebro",[]
True,http://arxiv.org/abs/2106.07032v1,2021-06-13T15:58:13Z,,arXiv,cs.LG,http://arxiv.org/pdf/2106.07032v1.pdf,2106.07032v1,Category Theory in Machine Learning,"Over the past two decades machine learning has permeated almost every realm of technology. At the same time, many researchers have begun using category theory as a unifying language, facilitating communication between different scientific disciplines. It is therefore unsurprising that there is a burgeoning interest in applying category theory to machine learning. We aim to document the motivations, goals and common themes across these applications. We touch on gradient-based learning, probability, and equivariant learning.",cs.LG,"Dan Shiebler, Bruno Gavranović, Paul Wilson",[]
True,http://arxiv.org/abs/1802.03830v1,2018-02-11T22:23:34Z,,arXiv,stat.ML,http://arxiv.org/pdf/1802.03830v1.pdf,1802.03830v1,Distributed Stochastic Multi-Task Learning with Graph Regularization,"We propose methods for distributed graph-based multi-task learning that are based on weighted averaging of messages from other machines. Uniform averaging or diminishing stepsize in these methods would yield consensus (single task) learning. We show how simply skewing the averaging weights or controlling the stepsize allows learning different, but related, tasks on the different machines.","stat.ML, cs.LG","Weiran Wang, Jialei Wang, Mladen Kolar, Nathan Srebro",[]
True,http://arxiv.org/abs/1612.04858v1,2016-12-14T22:04:33Z,,arXiv,cs.LG,http://arxiv.org/pdf/1612.04858v1.pdf,1612.04858v1,Bayesian Optimization for Machine Learning : A Practical Guidebook,"The engineering of machine learning systems is still a nascent field; relying on a seemingly daunting collection of quickly evolving tools and best practices. It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques. We outline four example machine learning problems that can be solved using open source machine learning libraries, and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications.",cs.LG,"Ian Dewancker, Michael McCourt, Scott Clark",[]
True,http://arxiv.org/abs/1702.08608v2,2017-02-28T02:19:20Z,,arXiv,stat.ML,http://arxiv.org/pdf/1702.08608v2.pdf,1702.08608v2,Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","stat.ML, cs.AI, cs.LG","Finale Doshi-Velez, Been Kim",[]
True,http://arxiv.org/abs/1705.07538v2,2017-05-22T02:28:19Z,,arXiv,cs.LG,http://arxiv.org/pdf/1705.07538v2.pdf,1705.07538v2,Infrastructure for Usable Machine Learning: The Stanford DAWN Project,"Despite incredible recent advances in machine learning, building machine learning applications remains prohibitively time-consuming and expensive for all but the best-trained, best-funded engineering organizations. This expense comes not from a need for new and improved statistical models but instead from a lack of systems and tools for supporting end-to-end machine learning application development, from data preparation and labeling to productionization and monitoring. In this document, we outline opportunities for infrastructure supporting usable, end-to-end machine learning applications in the context of the nascent DAWN (Data Analytics for What's Next) project at Stanford.","cs.LG, cs.DB, stat.ML","Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia",[]
True,http://arxiv.org/abs/1808.00033v3,2018-07-31T19:14:39Z,,arXiv,cs.LG,http://arxiv.org/pdf/1808.00033v3.pdf,1808.00033v3,Techniques for Interpretable Machine Learning,"Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.","cs.LG, cs.AI, stat.ML","Mengnan Du, Ninghao Liu, Xia Hu",[]
True,http://arxiv.org/abs/1911.08587v1,2019-11-17T17:36:41Z,,arXiv,quant-ph,http://arxiv.org/pdf/1911.08587v1.pdf,1911.08587v1,Solving machine learning optimization problems using quantum computers,Classical optimization algorithms in machine learning often take a long time to compute when applied to a multi-dimensional problem and require a huge amount of CPU and GPU resource. Quantum parallelism has a potential to speed up machine learning algorithms. We describe a generic mathematical model to leverage quantum parallelism to speed-up machine learning algorithms. We also apply quantum machine learning and quantum parallelism applied to a $3$-dimensional image that vary with time.,"quant-ph, cs.LG, stat.ML","Venkat R. Dasari, Mee Seong Im, Lubjana Beshaj",[]
True,http://arxiv.org/abs/2008.08080v2,2020-08-18T11:21:24Z,Oxford University Press (OUP),arXiv,stat.CO,http://arxiv.org/pdf/2008.08080v2.pdf,10.1093/bioinformatics/btab039,mlr3proba: An R Package for Machine Learning in Survival Analysis,"As machine learning has become increasingly popular over the last few decades, so too has the number of machine learning interfaces for implementing these models. Whilst many R libraries exist for machine learning, very few offer extended support for survival analysis. This is problematic considering its importance in fields like medicine, bioinformatics, economics, engineering, and more. mlr3proba provides a comprehensive machine learning interface for survival analysis and connects with mlr3's general model tuning and benchmarking facilities to provide a systematic infrastructure for survival modeling and evaluation.","stat.CO, cs.LG, stat.ML","Raphael Sonabend, Franz J. Király, Andreas Bender, Bernd Bischl, Michel Lang","['10.1111/j.2517-6161.1972.tb00899.x', '10.5281/zenodo.4457577', '10.18637/jss.v040.i08', '10.1002/(sici)1097-0258(19990915/30)18:17/18<2529::aid-sim274>3.0.co;2-5', '10.1001/jama.1982.03320430047030', '10.1111/j.0006-341x.2000.00337.x', '10.1214/08-aoas169', '10.21105/joss.01903', '10.1002/sim.1203', '10.1111/j.1541-0420.2010.01459.x', '10.1002/sim.4154', '10.1016/j.artmed.2011.06.006', '10.1002/1097-0258(20001230)19:24<3401::aid-sim554>3.0.co;2-2']"
True,http://arxiv.org/abs/2007.01977v1,2020-07-04T00:55:41Z,,arXiv,cs.LG,http://arxiv.org/pdf/2007.01977v1.pdf,2007.01977v1,Lale: Consistent Automated Machine Learning,"Automated machine learning makes it easier for data scientists to develop pipelines by searching over possible choices for hyperparameters, algorithms, and even pipeline topologies. Unfortunately, the syntax for automated machine learning tools is inconsistent with manual machine learning, with each other, and with error checks. Furthermore, few tools support advanced features such as topology search or higher-order operators. This paper introduces Lale, a library of high-level Python interfaces that simplifies and unifies automated machine learning in a consistent way.","cs.LG, cs.AI","Guillaume Baudart, Martin Hirzel, Kiran Kate, Parikshit Ram, Avraham Shinnar",[]
True,http://arxiv.org/abs/2007.07981v1,2020-07-15T20:26:49Z,,arXiv,cs.LG,http://arxiv.org/pdf/2007.07981v1.pdf,2007.07981v1,Differential Replication in Machine Learning,"When deployed in the wild, machine learning models are usually confronted with data and requirements that constantly vary, either because of changes in the generating distribution or because external constraints change the environment where the model operates. To survive in such an ecosystem, machine learning models need to adapt to new conditions by evolving over time. The idea of model adaptability has been studied from different perspectives. In this paper, we propose a solution based on reusing the knowledge acquired by the already deployed machine learning models and leveraging it to train future generations. This is the idea behind differential replication of machine learning models.","cs.LG, stat.ML, cs.LG, stat.ML","Irene Unceta, Jordi Nin, Oriol Pujol",[]
True,http://arxiv.org/abs/2108.08712v1,2021-08-19T14:22:17Z,,arXiv,cs.LG,http://arxiv.org/pdf/2108.08712v1.pdf,2108.08712v1,Teaching Uncertainty Quantification in Machine Learning through Use   Cases,"Uncertainty in machine learning is not generally taught as general knowledge in Machine Learning course curricula. In this paper we propose a short curriculum for a course about uncertainty in machine learning, and complement the course with a selection of use cases, aimed to trigger discussion and let students play with the concepts of uncertainty in a programming setting. Our use cases cover the concept of output uncertainty, Bayesian neural networks and weight distributions, sources of uncertainty, and out of distribution detection. We expect that this curriculum and set of use cases motivates the community to adopt these important concepts into courses for safety in AI.","cs.LG, stat.ML",Matias Valdenegro-Toro,[]
True,http://arxiv.org/abs/2212.12303v1,2022-12-23T13:08:59Z,,arXiv,cs.LG,http://arxiv.org/pdf/2212.12303v1.pdf,2212.12303v1,Introduction to Machine Learning for Physicians: A Survival Guide for   Data Deluge,"Many modern research fields increasingly rely on collecting and analysing massive, often unstructured, and unwieldy datasets. Consequently, there is growing interest in machine learning and artificial intelligence applications that can harness this `data deluge'. This broad nontechnical overview provides a gentle introduction to machine learning with a specific focus on medical and biological applications. We explain the common types of machine learning algorithms and typical tasks that can be solved, illustrating the basics with concrete examples from healthcare. Lastly, we provide an outlook on open challenges, limitations, and potential impacts of machine-learning-powered medicine.","cs.LG, stat.ML","Ričards Marcinkevičs, Ece Ozkan, Julia E. Vogt",[]
True,http://arxiv.org/abs/2305.15410v1,2023-04-28T09:29:10Z,,arXiv,cond-mat.mtrl-sci,http://arxiv.org/pdf/2305.15410v1.pdf,2305.15410v1,Machine learning-assisted close-set X-ray diffraction phase   identification of transition metals,"Machine learning has been applied to the problem of X-ray diffraction phase prediction with promising results. In this paper, we describe a method for using machine learning to predict crystal structure phases from X-ray diffraction data of transition metals and their oxides. We evaluate the performance of our method and compare the variety of its settings. Our results demonstrate that the proposed machine learning framework achieves competitive performance. This demonstrates the potential for machine learning to significantly impact the field of X-ray diffraction and crystal structure determination. Open-source implementation: https://github.com/maxnygma/NeuralXRD.","cond-mat.mtrl-sci, cs.AI, cs.LG","Maksim Zhdanov, Andrey Zhdanov",[]
True,http://arxiv.org/abs/2306.14624v2,2023-06-26T11:56:00Z,,arXiv,cs.LG,http://arxiv.org/pdf/2306.14624v2.pdf,2306.14624v2,Insights From Insurance for Fair Machine Learning,"We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.","cs.LG, cs.CY","Christian Fröhlich, Robert C. Williamson",[]
True,http://arxiv.org/abs/2407.19890v1,2024-07-07T16:30:46Z,,arXiv,quant-ph,http://arxiv.org/pdf/2407.19890v1.pdf,2407.19890v1,Quantum Dynamics of Machine Learning,"The quantum dynamic equation (QDE) of machine learning is obtained based on Schr\""odinger equation and potential energy equivalence relationship. Through Wick rotation, the relationship between quantum dynamics and thermodynamics is also established in this paper. This equation reformulates the iterative process of machine learning into a time-dependent partial differential equation with a clear mathematical structure, offering a theoretical framework for investigating machine learning iterations through quantum and mathematical theories. Within this framework, the fundamental iterative process, the diffusion model, and the Softmax and Sigmoid functions are examined, validating the proposed quantum dynamics equations. This approach not only presents a rigorous theoretical foundation for machine learning but also holds promise for supporting the implementation of machine learning algorithms on quantum computers.","quant-ph, cs.LG","Peng Wang, Maimaitiniyazi Maimaitiabudula",[]
True,http://arxiv.org/abs/2102.05639v1,2021-02-10T18:53:51Z,,arXiv,cs.LG,http://arxiv.org/pdf/2102.05639v1.pdf,2102.05639v1,Energy-Harvesting Distributed Machine Learning,"This paper provides a first study of utilizing energy harvesting for sustainable machine learning in distributed networks. We consider a distributed learning setup in which a machine learning model is trained over a large number of devices that can harvest energy from the ambient environment, and develop a practical learning framework with theoretical convergence guarantees. We demonstrate through numerical experiments that the proposed framework can significantly outperform energy-agnostic benchmarks. Our framework is scalable, requires only local estimation of the energy statistics, and can be applied to a wide range of distributed training settings, including machine learning in wireless networks, edge computing, and mobile internet of things.","cs.LG, cs.IT, math.IT, stat.ML","Basak Guler, Aylin Yener",[]
True,http://arxiv.org/abs/1909.09248v1,2019-09-19T22:12:30Z,,arXiv,cs.LG,http://arxiv.org/pdf/1909.09248v1.pdf,1909.09248v1,Representation Learning for Electronic Health Records,"Information in electronic health records (EHR), such as clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries, can be transformed into appropriate data representations that can be used for downstream clinical machine learning tasks using representation learning. Learning better representations is critical to improve the performance of downstream tasks. Due to the advances in machine learning, we now can learn better and meaningful representations from EHR through disentangling the underlying factors inside data and distilling large amounts of information and knowledge from heterogeneous EHR sources. In this chapter, we first introduce the background of learning representations and reasons why we need good EHR representations in machine learning for medicine and healthcare in Section 1. Next, we explain the commonly-used machine learning and evaluation methods for representation learning using a deep learning approach in Section 2. Following that, we review recent related studies of learning patient state representation from EHR for clinical machine learning tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies, and challenges for learning natural language representations when free texts, such as clinical notes, examination reports, or biomedical literature are used. We also discuss challenges and opportunities in these rapidly growing research fields.","cs.LG, stat.ML","Wei-Hung Weng, Peter Szolovits",[]
True,http://arxiv.org/abs/1810.03548v1,2018-10-08T16:07:11Z,,arXiv,cs.LG,http://arxiv.org/pdf/1810.03548v1.pdf,1810.03548v1,Meta-Learning: A Survey,"Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.","cs.LG, stat.ML",Joaquin Vanschoren,[]
True,http://arxiv.org/abs/1711.06552v1,2017-11-15T16:52:48Z,,arXiv,cs.LG,http://arxiv.org/pdf/1711.06552v1.pdf,1711.06552v1,Introduction to intelligent computing unit 1,This brief note highlights some basic concepts required toward understanding the evolution of machine learning and deep learning models. The note starts with an overview of artificial intelligence and its relationship to biological neuron that ultimately led to the evolution of todays intelligent models.,"cs.LG, stat.ML",Isa Inuwa-Dutse,[]
True,http://arxiv.org/abs/2004.05366v2,2020-04-11T11:00:26Z,,arXiv,cs.LG,http://arxiv.org/pdf/2004.05366v2.pdf,2004.05366v2,In-Machine-Learning Database: Reimagining Deep Learning with Old-School   SQL,"In-database machine learning has been very popular, almost being a cliche. However, can we do it the other way around? In this work, we say ""yes"" by applying plain old SQL to deep learning, in a sense implementing deep learning algorithms with SQL. Most deep learning frameworks, as well as generic machine learning ones, share a de facto standard of multidimensional array operations, underneath fancier infrastructure such as automatic differentiation. As SQL tables can be regarded as generalisations of (multi-dimensional) arrays, we have found a way to express common deep learning operations in SQL, encouraging a different way of thinking and thus potentially novel models. In particular, one of the latest trend in deep learning was the introduction of sparsity in the name of graph convolutional networks, whereas we take sparsity almost for granted in the database world. As both databases and machine learning involve transformation of datasets, we hope this work can inspire further works utilizing the large body of existing wisdom, algorithms and technologies in the database field to advance the state of the art in machine learning, rather than merely integerating machine learning into databases.","cs.LG, cs.DB, stat.ML",Len Du,[]
True,http://arxiv.org/abs/1807.06722v2,2018-07-18T00:50:18Z,,arXiv,cs.LG,http://arxiv.org/pdf/1807.06722v2.pdf,1807.06722v2,Machine Learning Interpretability: A Science rather than a tool,"The term ""interpretability"" is oftenly used by machine learning researchers each with their own intuitive understanding of it. There is no universal well agreed upon definition of interpretability in machine learning. As any type of science discipline is mainly driven by the set of formulated questions rather than by different tools in that discipline, e.g. astrophysics is the discipline that learns the composition of stars, not as the discipline that use the spectroscopes. Similarly, we propose that machine learning interpretability should be a discipline that answers specific questions related to interpretability. These questions can be of statistical, causal and counterfactual nature. Therefore, there is a need to look into the interpretability problem of machine learning in the context of questions that need to be addressed rather than different tools. We discuss about a hypothetical interpretability framework driven by a question based scientific approach rather than some specific machine learning model. Using a question based notion of interpretability, we can step towards understanding the science of machine learning rather than its engineering. This notion will also help us understanding any specific problem more in depth rather than relying solely on machine learning methods.","cs.LG, stat.ML","Abdul Karim, Avinash Mishra, MA Hakim Newton, Abdul Sattar",[]
True,http://arxiv.org/abs/2103.00742v4,2021-03-01T04:20:33Z,International Joint Conferences on Artificial Intelligence Organization,arXiv,cs.LG,http://arxiv.org/pdf/2103.00742v4.pdf,10.24963/ijcai.2021/637,Automated Machine Learning on Graphs: A Survey,"Machine learning on graphs has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To solve this critical challenge, automated machine learning (AutoML) on graphs which combines the strength of graph machine learning and AutoML together, is gaining attention from the research community. Therefore, we comprehensively survey AutoML on graphs in this paper, primarily focusing on hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We further overview libraries related to automated graph machine learning and in-depth discuss AutoGL, the first dedicated open-source library for AutoML on graphs. In the end, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive review of automated machine learning on graphs to the best of our knowledge.",cs.LG,"Ziwei Zhang, Xin Wang, Wenwu Zhu",[]
True,http://arxiv.org/abs/2201.06921v1,2021-12-13T07:20:50Z,,arXiv,cs.CY,http://arxiv.org/pdf/2201.06921v1.pdf,2201.06921v1,Can Machine Learning be Moral?,"The ethics of Machine Learning has become an unavoidable topic in the AI Community. The deployment of machine learning systems in multiple social contexts has resulted in a closer ethical scrutiny of the design, development, and application of these systems. The AI/ML community has come to terms with the imperative to think about the ethical implications of machine learning, not only as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The critical question that is troubling many debates is what can constitute an ethically accountable machine learning system. In this paper we explore possibilities for ethical evaluation of machine learning methodologies. We scrutinize techniques, methods and technical practices in machine learning from a relational ethics perspective, taking into consideration how machine learning systems are part of the world and how they relate to different forms of agency. Taking a page from Phil Agre (1997) we use the notion of a critical technical practice as a means of analysis of machine learning approaches. Our radical proposal is that supervised learning appears to be the only machine learning method that is ethically defensible.","cs.CY, cs.HC","Miguel Sicart, Irina Shklovski, Mirabelle Jones",[]
True,http://arxiv.org/abs/1812.01410v1,2018-12-04T13:50:11Z,,arXiv,cs.LG,http://arxiv.org/pdf/1812.01410v1.pdf,1812.01410v1,Compressive Classification (Machine Learning without learning),"Compressive learning is a framework where (so far unsupervised) learning tasks use not the entire dataset but a compressed summary (sketch) of it. We propose a compressive learning classification method, and a novel sketch function for images.","cs.LG, cs.CV, stat.ML","Vincent Schellekens, Laurent Jacques",[]
True,http://arxiv.org/abs/1707.03184v1,2017-07-11T09:15:46Z,,arXiv,cs.AI,http://arxiv.org/pdf/1707.03184v1.pdf,1707.03184v1,A Survey on Resilient Machine Learning,"Machine learning based system are increasingly being used for sensitive tasks such as security surveillance, guiding autonomous vehicle, taking investment decisions, detecting and blocking network intrusion and malware etc. However, recent research has shown that machine learning models are venerable to attacks by adversaries at all phases of machine learning (eg, training data collection, training, operation). All model classes of machine learning systems can be misled by providing carefully crafted inputs making them wrongly classify inputs. Maliciously created input samples can affect the learning process of a ML system by either slowing down the learning process, or affecting the performance of the learned mode, or causing the system make error(s) only in attacker's planned scenario. Because of these developments, understanding security of machine learning algorithms and systems is emerging as an important research area among computer security and machine learning researchers and practitioners. We present a survey of this emerging area in machine learning.","cs.AI, cs.CR, cs.LG","Atul Kumar, Sameep Mehta",[]
True,http://arxiv.org/abs/1611.03969v1,2016-11-12T08:18:38Z,,arXiv,stat.CO,http://arxiv.org/pdf/1611.03969v1.pdf,1611.03969v1,An Introduction to MM Algorithms for Machine Learning and Statistical,"MM (majorization--minimization) algorithms are an increasingly popular tool for solving optimization problems in machine learning and statistical estimation. This article introduces the MM algorithm framework in general and via three popular example applications: Gaussian mixture regressions, multinomial logistic regressions, and support vector machines. Specific algorithms for the three examples are derived and numerical demonstrations are presented. Theoretical and practical aspects of MM algorithm design are discussed.","stat.CO, cs.LG, stat.ML",Hien D. Nguyen,[]
True,http://arxiv.org/abs/1810.11383v2,2018-10-25T02:53:14Z,,arXiv,cs.LG,http://arxiv.org/pdf/1810.11383v2.pdf,1810.11383v2,Some Requests for Machine Learning Research from the East African Tech   Scene,"Based on 46 in-depth interviews with scientists, engineers, and CEOs, this document presents a list of concrete machine research problems, progress on which would directly benefit tech ventures in East Africa.","cs.LG, cs.CY, stat.ML",Milan Cvitkovic,[]
True,http://arxiv.org/abs/2104.05314v2,2021-04-12T09:54:12Z,Springer Science and Business Media LLC,arXiv,cs.AI,http://arxiv.org/pdf/2104.05314v2.pdf,10.1007/s12525-021-00475-2,Machine learning and deep learning,"Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.",cs.AI,"Christian Janiesch, Patrick Zschech, Kai Heinrich","['10.1109/access.2018.2870052', '10.1016/j.ijhm.2019.01.003', '10.24963/ijcai.2019/932', '10.1016/j.matcom.2008.01.028', '10.1109/cvpr.2005.177', '10.1016/0167-8655(94)90052-3', '10.1109/cvpr.2018.00175', '10.1007/s12525-019-00384-5', '10.1145/2523813', '10.1016/j.cirpj.2015.05.004', '10.1002/rob.21918', '10.1002/9780470939376.ch25', '10.1016/j.dss.2021.113494', '10.1109/arso.2017.8025197', '10.1016/j.procs.2018.10.340', '10.1126/science.aaa8415', '10.1007/s10462-007-9052-3', '10.1007/s12525-019-00351-0', '10.1038/nature14539', '10.3390/proceedings47010009', '10.1007/978-981-15-5573-2', '10.1023/b:visi.0000029664.99615.94', '10.1038/s41746-017-0013-1', '10.1016/j.artint.2018.07.007', '10.1109/access.2019.2905015', '10.1109/icmla.2016.0172', '10.25300/misq/2020/14458', '10.1007/s10994-013-5340-0', '10.1145/3234150', '10.1016/j.procs.2018.10.326', '10.1038/s42256-019-0048-x', '10.1016/0306-4573(88)90021-0', '10.1016/j.neunet.2014.09.003', '10.1017/s0140525x00005756', '10.1007/s12525-019-00393-4', '10.2307/23042796', '10.1016/j.jbusres.2020.09.068', '10.1126/science.aar6404', '10.1109/cvpr.2001.990517', '10.1109/tsc.2020.3000900', '10.22215/timreview/1282', '10.1007/bf00116900', '10.18653/v1/d18-1310', '10.1109/mci.2018.2840738', '10.1038/s41524-018-0081-z']"
True,http://arxiv.org/abs/1405.1304v1,2014-05-03T14:26:42Z,Seventh Sense Research Group Journals,arXiv,cs.CE,http://arxiv.org/pdf/1405.1304v1.pdf,10.14445/22312803/ijctt-v10p137,Application of Machine Learning Techniques in Aquaculture,"In this paper we present applications of different machine learning algorithms in aquaculture. Machine learning algorithms learn models from historical data. In aquaculture historical data are obtained from farm practices, yields, and environmental data sources. Associations between these different variables can be obtained by applying machine learning algorithms to historical data. In this paper we present applications of different machine learning algorithms in aquaculture applications.","cs.CE, cs.LG","Akhlaqur Rahman, Sumaira Tasnim",[]
True,http://arxiv.org/abs/1612.04251v1,2016-12-13T16:00:51Z,,arXiv,cs.DC,http://arxiv.org/pdf/1612.04251v1.pdf,1612.04251v1,TF.Learn: TensorFlow's High-level Module for Distributed Machine   Learning,"TF.Learn is a high-level Python module for distributed machine learning inside TensorFlow. It provides an easy-to-use Scikit-learn style interface to simplify the process of creating, configuring, training, evaluating, and experimenting a machine learning model. TF.Learn integrates a wide range of state-of-art machine learning algorithms built on top of TensorFlow's low level APIs for small to large-scale supervised and unsupervised problems. This module focuses on bringing machine learning to non-specialists using a general-purpose high-level language as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment. Emphasis is put on ease of use, performance, documentation, and API consistency.","cs.DC, cs.LG",Yuan Tang,[]
True,http://arxiv.org/abs/1910.02544v1,2019-10-06T22:53:28Z,,arXiv,cs.LG,http://arxiv.org/pdf/1910.02544v1.pdf,1910.02544v1,Using Deep Learning and Machine Learning to Detect Epileptic Seizure   with Electroencephalography (EEG) Data,"The prediction of epileptic seizure has always been extremely challenging in medical domain. However, as the development of computer technology, the application of machine learning introduced new ideas for seizure forecasting. Applying machine learning model onto the predication of epileptic seizure could help us obtain a better result and there have been plenty of scientists who have been doing such works so that there are sufficient medical data provided for researchers to do training of machine learning models.","cs.LG, eess.SP, stat.ML","Haotian Liu, Lin Xi, Ying Zhao, Zhixiang Li",[]
True,http://arxiv.org/abs/2103.11249v1,2021-03-20T21:43:24Z,,arXiv,cs.SE,http://arxiv.org/pdf/2103.11249v1.pdf,2103.11249v1,SELM: Software Engineering of Machine Learning Models,"One of the pillars of any machine learning model is its concepts. Using software engineering, we can engineer these concepts and then develop and expand them. In this article, we present a SELM framework for Software Engineering of machine Learning Models. We then evaluate this framework through a case study. Using the SELM framework, we can improve a machine learning process efficiency and provide more accuracy in learning with less processing hardware resources and a smaller training dataset. This issue highlights the importance of an interdisciplinary approach to machine learning. Therefore, in this article, we have provided interdisciplinary teams' proposals for machine learning.","cs.SE, cs.AI","Nafiseh Jafari, Mohammad Reza Besharati, Mohammad Izadi, Maryam Hourali",[]
True,http://arxiv.org/abs/2001.11489v1,2019-11-18T14:10:17Z,Academy and Industry Research Collaboration Center (AIRCC),arXiv,cs.CR,http://arxiv.org/pdf/2001.11489v1.pdf,10.5121/ijnsa.2019.11501,Machine Learning in Network Security Using KNIME Analytics,"Machine learning has more and more effect on our every day's life. This field keeps growing and expanding into new areas. Machine learning is based on the implementation of artificial intelligence that gives systems the capability to automatically learn and enhance from experiments without being explicitly programmed. Machine Learning algorithms apply mathematical equations to analyze datasets and predict values based on the dataset. In the field of cybersecurity, machine learning algorithms can be utilized to train and analyze the Intrusion Detection Systems (IDSs) on security-related datasets. In this paper, we tested different machine learning algorithms to analyze NSL-KDD dataset using KNIME analytics.",cs.CR,Munther Abualkibash,[]
True,http://arxiv.org/abs/1908.00868v2,2019-08-02T14:08:17Z,IOP Publishing,arXiv,cs.LG,http://arxiv.org/pdf/1908.00868v2.pdf,10.1088/1751-8121/ab956e,Machine Learning as Ecology,"Machine learning methods have had spectacular success on numerous problems. Here we show that a prominent class of learning algorithms - including Support Vector Machines (SVMs) -- have a natural interpretation in terms of ecological dynamics. We use these ideas to design new online SVM algorithms that exploit ecological invasions, and benchmark performance using the MNIST dataset. Our work provides a new ecological lens through which we can view statistical learning and opens the possibility of designing ecosystems for machine learning.   Supplemental code is found at https://github.com/owenhowell20/EcoSVM.","cs.LG, cond-mat.stat-mech, stat.ML","Owen Howell, Cui Wenping, Robert Marsland III, Pankaj Mehta","['10.1007/bf00994018', '10.1016/0040-5809(70)90039-0', '10.1016/0040-5809(90)90025-q', '10.1103/physreve.99.052111', '10.1086/282505', '10.1086/282505', '10.2307/1934144', '10.1023/b:mach.0000008084.60811.49', '10.1609/aaai.v33i01.33013991', '10.1017/cbo9780511804441', '10.1111/j.1600-0706.2012.20830.x', '10.1016/s0169-5347(02)02495-3', '10.1073/pnas.87.24.9610', '10.1109/nnsp.2003.1318049', '10.1109/icpr.2018.8545819', '10.1038/s41467-018-04593-y', '10.1109/5.726791', '10.1016/j.jmb.2015.10.019']"
True,http://arxiv.org/abs/2303.09491v1,2023-03-16T17:10:39Z,Springer Science and Business Media LLC,arXiv,quant-ph,http://arxiv.org/pdf/2303.09491v1.pdf,10.1038/s43588-022-00311-3,Challenges and Opportunities in Quantum Machine Learning,"At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the trainability of QML models. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with QML.","quant-ph, cs.LG, stat.ML","M. Cerezo, Guillaume Verdon, Hsin-Yuan Huang, Lukasz Cincio, Patrick J. Coles","['10.1098/rspa.2016.0822', '10.1080/00107514.2014.964942', '10.1038/nature23474', '10.1038/s41586-019-1666-5', '10.1088/1367-2630/aae94a', '10.1038/s41467-018-06847-1', '10.1038/s41567-019-0648-8', '10.1145/3313276.3316310', '10.1038/s41467-021-22539-9', '10.1103/prxquantum.2.040321', '10.1103/revmodphys.89.035002', '10.1038/nphoton.2011.35', '10.1103/physreva.80.022339', '10.1103/physrevlett.113.130503', '10.1103/physrevlett.122.040504', '10.1103/physreva.103.032430', '10.1080/00107514.2019.1667078', '10.1103/physreva.52.r2493', '10.1038/s41586-019-0980-2', '10.1038/s41567-021-01287-z', '10.1007/978-3-030-83098-4_6', '10.1038/s41586-021-03242-7', '10.22331/q-2022-05-24-720', '10.1126/science.abn7293', '10.1103/physreva.102.032420', '10.1038/s41467-022-32550-3', '10.2172/2377336', '10.22331/q-2021-11-17-582', '10.1038/s42254-021-00348-9', '10.1038/s41534-017-0032-4', '10.1038/s41467-020-14454-2', '10.1007/s11128-014-0809-8', '10.1103/physreva.98.012324', '10.1103/physrevresearch.1.033063', '10.1007/bf00994018', '10.22331/q-2020-05-11-263', '10.22331/q-2020-08-31-314', '10.22331/q-2020-05-25-269', '10.1103/physrevlett.128.070501', '10.1038/s43588-021-00084-1', '10.1038/323533a0', '10.1126/science.abn7293', '10.1109/focs52979.2021.00063', '10.1038/s41597-022-01639-1', '10.1088/2058-9565/ac7d06', '10.1103/physrevlett.127.120502', '10.1038/s41534-019-0167-6', '10.1038/s41467-018-07090-4', '10.1038/s41467-021-21728-w', '10.1088/2058-9565/abf51a', '10.22331/q-2021-10-05-558', '10.1103/prxquantum.3.010313', '10.1088/2058-9565/abd891', '10.1103/physrevlett.128.180505', '10.1103/physrevlett.126.190501', '10.1103/prxquantum.2.040316', '10.1088/1751-8121/abfac7', '10.1103/physrevresearch.3.033090', '10.1038/s41467-021-27045-6', '10.1103/prxquantum.3.030341', '10.1038/s41534-023-00710-y', '10.22331/q-2022-09-29-824', '10.1103/physrevresearch.4.043100', '10.22331/q-2023-07-13-1060', '10.1103/physrevlett.119.180509', '10.22331/q-2021-11-26-592', '10.7566/jpsj.90.032001', '10.1088/1367-2630/ab784c', '10.1103/prxquantum.2.010324', '10.1038/s41534-021-00425-y', '10.1103/physrevresearch.4.013083', '10.1038/nphys4074', '10.1038/s41534-018-0082-2', '10.1038/ncomms5213', '10.1038/s41534-019-0187-2', '10.1038/s41534-020-00302-0', '10.1103/physrevlett.126.190505', '10.1038/s41467-021-27922-0', '10.1145/3357713.3384314', '10.22331/q-2018-08-06-79', '10.1126/science.abk3333', '10.1103/physrev.136.b864', '10.1103/revmodphys.71.1253', '10.1088/2632-2153/ab9009', '10.1201/9780203881095', '10.1145/237814.237866', '10.1137/s0097539796300921', '10.1103/prxquantum.2.010103', '10.1103/revmodphys.86.153', '10.1103/physrevlett.114.090502', '10.1038/nphoton.2009.231', '10.1126/science.aat2663']"
True,http://arxiv.org/abs/2407.05520v1,2024-07-07T23:57:10Z,,arXiv,cs.LG,http://arxiv.org/pdf/2407.05520v1.pdf,2407.05520v1,A Theory of Machine Learning,"We critically review three major theories of machine learning and provide a new theory according to which machines learn a function when the machines successfully compute it. We show that this theory challenges common assumptions in the statistical and the computational learning theories, for it implies that learning true probabilities is equivalent neither to obtaining a correct calculation of the true probabilities nor to obtaining an almost-sure convergence to them. We also briefly discuss some case studies from natural language processing and macroeconomics from the perspective of the new theory.","cs.LG, stat.ML","Jinsook Kim, Jinho Kang",[]
True,http://arxiv.org/abs/1501.04309v1,2015-01-18T14:57:02Z,,arXiv,cs.IT,http://arxiv.org/pdf/1501.04309v1.pdf,1501.04309v1,Information Theory and its Relation to Machine Learning,"In this position paper, I first describe a new perspective on machine learning (ML) by four basic problems (or levels), namely, ""What to learn?"", ""How to learn?"", ""What to evaluate?"", and ""What to adjust?"". The paper stresses more on the first level of ""What to learn?"", or ""Learning Target Selection"". Towards this primary problem within the four levels, I briefly review the existing studies about the connection between information theoretical learning (ITL [1]) and machine learning. A theorem is given on the relation between the empirically-defined similarity measure and information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.","cs.IT, cs.LG, math.IT",Bao-Gang Hu,[]
True,http://arxiv.org/abs/1602.00198v1,2016-01-31T04:05:50Z,,arXiv,cs.AI,http://arxiv.org/pdf/1602.00198v1.pdf,1602.00198v1,Discussion on Mechanical Learning and Learning Machine,"Mechanical learning is a computing system that is based on a set of simple and fixed rules, and can learn from incoming data. A learning machine is a system that realizes mechanical learning. Importantly, we emphasis that it is based on a set of simple and fixed rules, contrasting to often called machine learning that is sophisticated software based on very complicated mathematical theory, and often needs human intervene for software fine tune and manual adjustments. Here, we discuss some basic facts and principles of such system, and try to lay down a framework for further study. We propose 2 directions to approach mechanical learning, just like Church-Turing pair: one is trying to realize a learning machine, another is trying to well describe the mechanical learning.",cs.AI,Chuyu Xiong,[]
True,http://arxiv.org/abs/1605.07805v2,2016-05-25T10:11:03Z,,arXiv,cs.FL,http://arxiv.org/pdf/1605.07805v2.pdf,1605.07805v2,Learning Moore Machines from Input-Output Traces,"The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper we study this problem for finite state machines with inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input-output traces into an incomplete Moore machine and then completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the fundamental identification in the limit property. We also compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers, and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.","cs.FL, cs.LG","Georgios Giantamidis, Stavros Tripakis",[]
True,http://arxiv.org/abs/1912.09630v1,2019-12-20T03:47:28Z,,arXiv,cs.LG,http://arxiv.org/pdf/1912.09630v1.pdf,1912.09630v1,Practical Solutions for Machine Learning Safety in Autonomous Vehicles,"Autonomous vehicles rely on machine learning to solve challenging tasks in perception and motion planning. However, automotive software safety standards have not fully evolved to address the challenges of machine learning safety such as interpretability, verification, and performance limitations. In this paper, we review and organize practical machine learning safety techniques that can complement engineering safety for machine learning based software in autonomous vehicles. Our organization maps safety strategies to state-of-the-art machine learning techniques in order to enhance dependability and safety of machine learning algorithms. We also discuss security limitations and user experience aspects of machine learning components in autonomous vehicles.","cs.LG, stat.ML","Sina Mohseni, Mandar Pitale, Vasu Singh, Zhangyang Wang",[]
True,http://arxiv.org/abs/2003.10146v2,2020-03-23T09:31:02Z,Elsevier BV,arXiv,cs.LG,http://arxiv.org/pdf/2003.10146v2.pdf,10.1016/j.cosrev.2020.100254,"Julia Language in Machine Learning: Algorithms, Applications, and Open   Issues","Machine learning is driving development across many fields in science and engineering. A simple and efficient programming language could accelerate applications of machine learning in various fields. Currently, the programming languages most commonly used to develop machine learning algorithms include Python, MATLAB, and C/C ++. However, none of these languages well balance both efficiency and simplicity. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing, which can well balance the efficiency and simplicity. This paper summarizes the related research work and developments in the application of the Julia language in machine learning. It first surveys the popular machine learning algorithms that are developed in the Julia language. Then, it investigates applications of the machine learning algorithms implemented with the Julia language. Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia language in machine learning.","cs.LG, stat.ML","Kaifeng Gao, Gang Mei, Francesco Piccialli, Salvatore Cuomo, Jingzhi Tu, Zenan Huo","['10.1126/science.aaa8415', '10.1161/circulationaha.115.001593', '10.1145/2347736.2347755', '10.1038/d41586-019-02307-y', '10.1038/nature14539', '10.1126/science.293.5537.2051', '10.1137/141000671', '10.1038/d41586-019-02310-3', '10.1016/j.jpdc.2020.01.003', '10.1109/tpds.2018.2872064', '10.1109/bigdata47090.2019.9005453', '10.1137/16m1081063', '10.1016/j.neuroimage.2007.01.058', '10.1371/journal.pone.0193981', '10.1186/s12859-019-2824-3', '10.18637/jss.v035.i04', '10.1016/j.jneumeth.2011.10.025', '10.1016/j.ijar.2018.11.002', '10.1088/1749-4699/8/1/014008', '10.1023/a:1010933404324', '10.3389/fgene.2019.00579', '10.1016/j.landurbplan.2017.05.010', '10.1007/s10107-010-0420-4', '10.1109/jas.2019.1911825', '10.1049/iet-its.2018.5144', '10.3389/fninf.2014.00014', '10.1016/j.neucom.2019.08.022', '10.1109/tits.2019.2892405', '10.1007/3-540-63930-6_173', '10.1016/j.compbiomed.2017.12.003', '10.3847/1538-3881/aa68a1', '10.1080/10556788.2018.1435651', '10.1016/j.ins.2018.06.056', '10.1109/tpami.2011.255', '10.1109/jiot.2019.2903191', '10.1093/nar/16.22.10881', '10.1007/bf02289588', '10.1109/2.781637', '10.1093/bioinformatics/btt626', '10.1109/lra.2019.2926677', '10.1016/j.compbiomed.2007.11.001', '10.1016/j.csda.2018.01.014', '10.1093/bioinformatics/bty056', '10.1007/s00422-001-0298-6', '10.1007/s11517-006-0051-3', '10.1016/0167-2789(90)90081-y', '10.1145/3065386', '10.1186/s12859-015-0793-8', '10.1109/tnnls.2015.2424995', '10.1109/tmc.2019.2892451', '10.1016/j.bushor.2015.03.008', '10.1016/j.future.2013.01.010', '10.1016/j.comnet.2010.05.010', '10.1109/comst.2018.2844341', '10.1016/j.dcan.2017.10.002', '10.1083/jcb.201610026', '10.1073/pnas.1619003114', '10.1016/j.gsf.2014.10.005', '10.1016/j.conbuildmat.2017.09.110', '10.1016/j.engstruct.2016.11.038', '10.1126/science.aaa8685', '10.1016/j.knosys.2016.06.009', '10.1109/mci.2018.2840738', '10.1016/j.neucom.2019.08.096', '10.1016/j.neucom.2016.12.038', '10.1007/s11042-018-6986-1', '10.1109/tits.2011.2157145', '10.1049/iet-com.2016.1290', '10.1016/j.patcog.2007.08.018']"
True,http://arxiv.org/abs/1803.10311v2,2018-03-27T20:38:05Z,,arXiv,cs.LG,http://arxiv.org/pdf/1803.10311v2.pdf,1803.10311v2,How Developers Iterate on Machine Learning Workflows -- A Survey of the   Applied Machine Learning Literature,"Machine learning workflow development is anecdotally regarded to be an iterative process of trial-and-error with humans-in-the-loop. However, we are not aware of quantitative evidence corroborating this popular belief. A quantitative characterization of iteration can serve as a benchmark for machine learning workflow development in practice, and can aid the development of human-in-the-loop machine learning systems. To this end, we conduct a small-scale survey of the applied machine learning literature from five distinct application domains. We collect and distill statistics on the role of iteration within machine learning workflow development, and report preliminary trends and insights from our investigation, as a starting point towards this benchmark. Based on our findings, we finally describe desiderata for effective and versatile human-in-the-loop machine learning systems that can cater to users in diverse domains.","cs.LG, cs.DB, cs.HC, stat.ML","Doris Xin, Litian Ma, Shuchen Song, Aditya Parameswaran",[]
True,http://arxiv.org/abs/2006.15680v1,2020-06-28T19:06:16Z,,arXiv,cs.LG,http://arxiv.org/pdf/2006.15680v1.pdf,2006.15680v1,Modeling Generalization in Machine Learning: A Methodological and   Computational Study,"As machine learning becomes more and more available to the general public, theoretical questions are turning into pressing practical issues. Possibly, one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions. In many real-world cases, it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize, i.e., to provide accurate predictions on unseen data, depending on the characteristics of the target problem. In this work, we perform a meta-analysis of 109 publicly-available classification data sets, modeling machine learning generalization as a function of a variety of data set characteristics, ranging from number of samples to intrinsic dimensionality, from class-wise feature skewness to $F1$ evaluated on test samples falling outside the convex hull of the training set. Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization, by emphasizing the difference between interpolated and extrapolated predictions. Besides several predictable correlations, we observe unexpectedly weak associations between the generalization ability of machine learning models and all metrics related to dimensionality, thus challenging the common assumption that the \textit{curse of dimensionality} might impair generalization in machine learning.","cs.LG, stat.ML","Pietro Barbiero, Giovanni Squillero, Alberto Tonda",[]
True,http://arxiv.org/abs/2105.03726v4,2021-05-08T16:05:07Z,,arXiv,cs.CR,http://arxiv.org/pdf/2105.03726v4.pdf,2105.03726v4,Mental Models of Adversarial Machine Learning,"Although machine learning is widely used in practice, little is known about practitioners' understanding of potential security challenges. In this work, we close this substantial gap and contribute a qualitative study focusing on developers' mental models of the machine learning pipeline and potentially vulnerable components. Similar studies have helped in other security fields to discover root causes or improve risk communication. Our study reveals two \facets of practitioners' mental models of machine learning security. Firstly, practitioners often confuse machine learning security with threats and defences that are not directly related to machine learning. Secondly, in contrast to most academic research, our participants perceive security of machine learning as not solely related to individual models, but rather in the context of entire workflows that consist of multiple components. Jointly with our additional findings, these two facets provide a foundation to substantiate mental models for machine learning security and have implications for the integration of adversarial machine learning into corporate workflows, \new{decreasing practitioners' reported uncertainty}, and appropriate regulatory frameworks for machine learning security.","cs.CR, cs.AI","Lukas Bieringer, Kathrin Grosse, Michael Backes, Battista Biggio, Katharina Krombholz",[]
True,http://arxiv.org/abs/2209.02057v3,2022-09-05T17:09:03Z,,arXiv,stat.ML,http://arxiv.org/pdf/2209.02057v3.pdf,2209.02057v3,Applying Machine Learning to Life Insurance: some knowledge sharing to   master it,"Machine Learning permeates many industries, which brings new source of benefits for companies. However within the life insurance industry, Machine Learning is not widely used in practice as over the past years statistical models have shown their efficiency for risk assessment. Thus insurers may face difficulties to assess the value of the artificial intelligence. Focusing on the modification of the life insurance industry over time highlights the stake of using Machine Learning for insurers and benefits that it can bring by unleashing data value. This paper reviews traditional actuarial methodologies for survival modeling and extends them with Machine Learning techniques. It points out differences with regular machine learning models and emphasizes importance of specific implementations to face censored data with machine learning models family. In complement to this article, a Python library has been developed. Different open-source Machine Learning algorithms have been adjusted to adapt the specificities of life insurance data, namely censoring and truncation. Such models can be easily applied from this SCOR library to accurately model life insurance risks.","stat.ML, cs.CY, cs.LG, stat.AP","Antoine Chancel, Laura Bradier, Antoine Ly, Razvan Ionescu, Laurene Martin, Marguerite Sauce",[]
True,http://arxiv.org/abs/2312.14050v1,2023-12-21T17:19:27Z,,arXiv,math.NA,http://arxiv.org/pdf/2312.14050v1.pdf,2312.14050v1,Machine learning and domain decomposition methods -- a survey,"Hybrid algorithms, which combine black-box machine learning methods with experience from traditional numerical methods and domain expertise from diverse application areas, are progressively gaining importance in scientific machine learning and various industrial domains, especially in computational science and engineering. In the present survey, several promising avenues of research will be examined which focus on the combination of machine learning (ML) and domain decomposition methods (DDMs). The aim of this survey is to provide an overview of existing work within this field and to structure it into domain decomposition for machine learning and machine learning-enhanced domain decomposition, including: domain decomposition for classical machine learning, domain decomposition to accelerate the training of physics-aware neural networks, machine learning to enhance the convergence properties or computational efficiency of DDMs, and machine learning as a discretization method in a DDM for the solution of PDEs. In each of these fields, we summarize existing work and key advances within a common framework and, finally, disuss ongoing challenges and opportunities for future research.","math.NA, cs.LG, cs.NA, 65F10, 65N22, 65N55, 68T05, 68T07","Axel Klawonn, Martin Lanser, Janine Weber",[]
True,http://arxiv.org/abs/2409.03632v1,2024-09-05T15:47:04Z,Springer Science and Business Media LLC,arXiv,cs.LG,http://arxiv.org/pdf/2409.03632v1.pdf,10.1007/s00146-024-02056-1,Beyond Model Interpretability: Socio-Structural Explanations in Machine   Learning,"What is it to interpret the outputs of an opaque machine learning model. One approach is to develop interpretable machine learning techniques. These techniques aim to show how machine learning models function by providing either model centric local or global explanations, which can be based on mechanistic interpretations revealing the inner working mechanisms of models or nonmechanistic approximations showing input feature output data relationships. In this paper, we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively salient domains could require appealing to a third type of explanation that we call sociostructural explanation. The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures. Sociostructural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models. We demonstrate the importance of sociostructural explanations by examining a racially biased healthcare allocation algorithm. Our proposal highlights the need for transparency beyond model interpretability, understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself.",cs.LG,"Andrew Smart, Atoosa Kasirzadeh","['10.1111/rssb.12377', '10.1145/3351095.3375624', '10.1097/acm.0000000000001294', '10.23915/distill.00015', '10.1145/3511299', '10.1086/709729', '10.1145/3561048', '10.1093/acprof:oso/9780199381104.001.0001', '10.1214/aos/1013203451', '10.1609/aaai.v33i01.33013681', '10.1007/s10618-022-00831-6', '10.1145/3351095.3372826', '10.1093/acprof:oso/9780199892631.001.0001', '10.1007/s11098-014-0434-5', '10.1111/josp.12373', '10.1080/1369118x.2019.1573912', '10.1007/978-3-031-04083-2_2', '10.1016/j.artint.2021.103571', '10.1145/3442188.3445866', '10.1145/3514094.3534188', '10.1145/3461702.3462606', '10.1145/3442188.3445886', '10.1038/d41586-019-03228-6', '10.2139/ssrn.3837493', '10.5206/fpq/2022.3/4.14191', '10.1145/3236386.3241340', '10.1007/978-3-030-64949-4_1', '10.1145/3287560.3287574', '10.1007/978-3-031-04083-2_4', '10.1145/3359992.3366639', '10.1145/3351095.3372850', '10.1126/science.aax2342', '10.1056/nejmp1811499', '10.1145/3351095.3372873', '10.1109/satml54575.2023.00039', '10.1007/978-3-030-56286-1_3', '10.1145/2939672.2939778', '10.1093/biostatistics/kxz040', '10.1038/s42256-019-0048-x', '10.1016/j.knosys.2023.110273', '10.2307/j.ctv2g591sq', '10.1007/s11229-022-03485-5', '10.1017/s0265052506060043']"
True,http://arxiv.org/abs/2103.03122v1,2021-03-03T10:31:44Z,,arXiv,stat.CO,http://arxiv.org/pdf/2103.03122v1.pdf,2103.03122v1,Machine Learning using Stata/Python,"We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting popular Machine Learning (ML) methods both in regression and classification settings. Using the recent Stata/Python integration platform (sfi) of Stata 16, these commands provide hyper-parameters' optimal tuning via K-fold cross-validation using greed search. More specifically, they make use of the Python Scikit-learn API to carry out both cross-validation and outcome/label prediction.","stat.CO, cs.LG, cs.MS",Giovanni Cerulli,[]
True,http://arxiv.org/abs/1907.03010v1,2019-07-03T15:10:23Z,,arXiv,q-fin.ST,http://arxiv.org/pdf/1907.03010v1.pdf,1907.03010v1,Financial Time Series Data Processing for Machine Learning,"This article studies the financial time series data processing for machine learning. It introduces the most frequent scaling methods, then compares the resulting stationarity and preservation of useful information for trend forecasting. It proposes an empirical test based on the capability to learn simple data relationship with simple models. It also speaks about the data split method specific to time series, avoiding unwanted overfitting and proposes various labelling for classification and regression.","q-fin.ST, cs.LG, stat.ML",Fabrice Daniel,[]
True,http://arxiv.org/abs/1902.04622v1,2019-02-12T20:28:09Z,,arXiv,cs.LG,http://arxiv.org/pdf/1902.04622v1.pdf,1902.04622v1,Learning Theory and Support Vector Machines - a primer,"The main goal of statistical learning theory is to provide a fundamental framework for the problem of decision making and model construction based on sets of data. Here, we present a brief introduction to the fundamentals of statistical learning theory, in particular the difference between empirical and structural risk minimization, including one of its most prominent implementations, i.e. the Support Vector Machine.","cs.LG, stat.ML",Michael Banf,[]
True,http://arxiv.org/abs/2206.13446v1,2022-06-27T16:53:18Z,,arXiv,cs.LG,http://arxiv.org/pdf/2206.13446v1.pdf,2206.13446v1,Pen and Paper Exercises in Machine Learning,"This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.","cs.LG, stat.ML",Michael U. Gutmann,[]
True,http://arxiv.org/abs/2310.11470v1,2023-05-24T13:38:38Z,,arXiv,cs.LG,http://arxiv.org/pdf/2310.11470v1.pdf,2310.11470v1,Classic machine learning methods,"In this chapter, we present the main classic machine learning methods. A large part of the chapter is devoted to supervised learning techniques for classification and regression, including nearest-neighbor methods, linear and logistic regressions, support vector machines and tree-based algorithms. We also describe the problem of overfitting as well as strategies to overcome it. We finally provide a brief overview of unsupervised learning methods, namely for clustering and dimensionality reduction.","cs.LG, cs.AI","Johann Faouzi, Olivier Colliot",[]
True,http://arxiv.org/abs/2011.03733v1,2020-11-07T09:32:49Z,,arXiv,cs.LG,http://arxiv.org/pdf/2011.03733v1.pdf,2011.03733v1,Human-Like Active Learning: Machines Simulating the Human Learning   Process,"Although the use of active learning to increase learners' engagement has recently been introduced in a variety of methods, empirical experiments are lacking. In this study, we attempted to align two experiments in order to (1) make a hypothesis for machine and (2) empirically confirm the effect of active learning on learning. In Experiment 1, we compared the effect of a passive form of learning to active form of learning. The results showed that active learning had a greater learning outcomes than passive learning. In the machine experiment based on the human result, we imitated the human active learning as a form of knowledge distillation. The active learning framework performed better than the passive learning framework. In the end, we showed not only that we can make build better machine training framework through the human experiment result, but also empirically confirm the result of human experiment through imitated machine experiments; human-like active learning have crucial effect on learning performance.",cs.LG,"Jaeseo Lim, Hwiyeol Jo, Byoung-Tak Zhang, Jooyong Park",[]
True,http://arxiv.org/abs/1908.01262v1,2019-08-04T02:51:53Z,Public Library of Science (PLoS),arXiv,cs.CR,http://arxiv.org/pdf/1908.01262v1.pdf,10.1371/journal.pone.0237749,A systematic review of fuzzing based on machine learning techniques,"Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzzing techniques have many challenges, such as how to mutate input seed files, how to increase code coverage, and how to effectively bypass verification. Machine learning technology has been introduced as a new method into fuzzing test to alleviate these challenges. This paper reviews the research progress of using machine learning technology for fuzzing test in recent years, analyzes how machine learning improve the fuzz process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies six different stages in which machine learning have been used. Then this paper systematically study the machine learning based fuzzing models from selection of machine learning algorithm, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Next, this paper assesses the performance of the machine learning models based on the frequently used evaluation metrics. The results of the evaluation prove that machine learning technology has an acceptable capability of categorize predictive for fuzzing. Finally, the comparison on capability of discovering vulnerabilities between traditional fuzzing tools and machine learning based fuzzing tools is analyzed. The results depict that the introduction of machine learning technology can improve the performance of fuzzing. However, there are still some limitations, such as unbalanced training samples and difficult to extract the characteristics related to vulnerabilities.","cs.CR, cs.LG","Yan Wang, Peng Jia, Luping Liu, Jiayong Liu","['10.1145/96267.96279', '10.1109/msp.2005.55', '10.1109/acsac.2007.27', '10.1145/3140587.3062349', '10.1145/2970276.2970321', '10.1007/s10664-015-9422-4', '10.1186/s42400-018-0002-y', '10.1145/3180445.3180453', '10.1145/2857705.2857720', '10.1109/compcomm.2017.8322752', '10.1109/ase.2017.8115618', '10.1109/sp.2019.00052', '10.1109/sp.2017.23', '10.1145/1064978.1065034', '10.1145/2499370.2462173', '10.1049/sej.1995.0010', '10.1145/1180405.1180445', '10.1145/1961295.1950396', '10.1145/1379022.1375607', '10.1145/2699026.2699098', '10.1145/360248.360252', '10.1109/dsn.2009.5270315', '10.1145/2560217.2560219', '10.1109/sp.2012.31', '10.14722/ndss.2015.23294', '10.1145/2927924', '10.1145/1966445.1966463', '10.1109/icnidc.2014.7000307', '10.1007/978-3-642-13986-4_7', '10.1007/978-3-319-72389-1_24', '10.1109/pac.2017.10', '10.1109/apsec.2017.30', '10.1007/978-3-319-89500-0_53', '10.1145/3203217.3203241', '10.1145/3270101.3270108', '10.1145/3213846.3213848', '10.1109/compsac.2018.00098', '10.1109/spw.2018.00026', '10.1007/978-3-030-12146-4_22', '10.5220/0006836604720481', '10.1007/978-3-319-98989-1_2', '10.1109/tencon.2018.8650188', '10.1109/icstw.2019.00065', '10.1109/tcyb.2020.3013675', '10.1109/access.2019.2911121', '10.1007/978-3-030-29959-0_13', '10.1609/aaai.v33i01.33011044', '10.1109/access.2019.2903291', '10.1109/icst.2019.00016', '10.14722/ndss.2019.23525', '10.1145/3319535.3363230', '10.1109/ukrcon.2019.8879997', '10.1109/iccsnt47585.2019.8962499', '10.1109/icse-companion.2019.00096', '10.1109/ase.2019.00093', '10.1007/978-3-030-42921-8_8', '10.1109/icst.2019.00029', '10.3390/s20072040', '10.1561/2000000039', '10.1038/nature14539', '10.1109/tnn.1998.712192', '10.1109/72.788640', '10.1198/tech.2007.s518', '10.1145/319382.319388', '10.1007/978-3-319-40667-1_20', '10.1016/j.asoc.2019.105598', '10.4108/eai.3-12-2015.2262516', '10.1145/1299015.1299021', '10.1145/1242572.1242660', '10.1109/sere.2013.26', '10.1109/3pgcic.2014.100', '10.1098/rsta.1894.0003', '10.1198/tas.2009.0033', '10.1145/383952.383995', '10.1109/sp.2016.15', '10.1016/s0375-9601(00)00725-8', '10.1145/1993498.1993532', '10.14722/ndss.2019.23371', '10.14722/ndss.2019.23504', '10.1109/sp.2018.00046', '10.1109/access.2019.2894178', '10.1109/sp.2018.00056', '10.1109/access.2018.2851237', '10.1016/j.infsof.2011.09.002', '10.1016/j.eswa.2011.03.041']"
True,http://arxiv.org/abs/1610.08251v1,2016-10-26T09:35:11Z,American Physical Society (APS),arXiv,quant-ph,http://arxiv.org/pdf/1610.08251v1.pdf,10.1103/physrevlett.117.130501,Quantum-enhanced machine learning,"The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.","quant-ph, cs.AI, cs.LG","Vedran Dunjko, Jacob M. Taylor, Hans J. Briegel","['10.1038/521435a', '10.1126/science.349.6245.248', '10.1038/nature16961', '10.1007/s10994-012-5316-5', '10.1103/physrevlett.113.130503', '10.1109/tsmcb.2008.925743', '10.1103/physrevx.4.031002', '10.1103/physreva.90.032310', '10.1103/physrevlett.110.220501', '10.1038/srep12874', '10.1103/physrevlett.116.090405', '10.1007/s11128-014-0809-8', '10.1137/040605072', '10.1038/srep00400', '10.1109/access.2016.2556579', '10.1002/(sici)1521-3978(199806)46:4/5&lt;493::aid-prop493&gt;3.0.co;2-p', '10.1103/physrevlett.113.210501', '10.1103/physrevlett.100.160501']"
True,http://arxiv.org/abs/2304.01316v1,2023-04-03T19:32:30Z,,arXiv,stat.ME,http://arxiv.org/pdf/2304.01316v1.pdf,2304.01316v1,Matched Machine Learning: A Generalized Framework for Treatment Effect   Inference With Learned Metrics,"We introduce Matched Machine Learning, a framework that combines the flexibility of machine learning black boxes with the interpretability of matching, a longstanding tool in observational causal inference. Interpretability is paramount in many high-stakes application of causal inference. Current tools for nonparametric estimation of both average and individualized treatment effects are black-boxes that do not allow for human auditing of estimates. Our framework uses machine learning to learn an optimal metric for matching units and estimating outcomes, thus achieving the performance of machine learning black-boxes, while being interpretable. Our general framework encompasses several published works as special cases. We provide asymptotic inference theory for our proposed framework, enabling users to construct approximate confidence intervals around estimates of both individualized and average treatment effects. We show empirically that instances of Matched Machine Learning perform on par with black-box machine learning methods and better than existing matching methods for similar problems. Finally, in our application we show how Matched Machine Learning can be used to perform causal inference even when covariate data are highly complex: we study an image dataset, and produce high quality matches and estimates of treatment effects.","stat.ME, stat.ML","Marco Morucci, Cynthia Rudin, Alexander Volfovsky",[]
True,http://arxiv.org/abs/1302.0406v1,2013-02-02T17:20:47Z,,arXiv,cs.LG,http://arxiv.org/pdf/1302.0406v1.pdf,1302.0406v1,Generalization Guarantees for a Binary Classification Framework for   Two-Stage Multiple Kernel Learning,We present generalization bounds for the TS-MKL framework for two stage multiple kernel learning. We also present bounds for sparse kernel learning formulations within the TS-MKL framework.,"cs.LG, stat.ML",Purushottam Kar,[]
True,http://arxiv.org/abs/1708.07826v1,2017-08-24T20:19:20Z,,arXiv,stat.ML,http://arxiv.org/pdf/1708.07826v1.pdf,1708.07826v1,Logistic Regression as Soft Perceptron Learning,"We comment on the fact that gradient ascent for logistic regression has a connection with the perceptron learning algorithm. Logistic learning is the ""soft"" variant of perceptron learning.","stat.ML, 62M45, 68Q32, K.3.2; I.5.1",Raul Rojas,[]
True,http://arxiv.org/abs/2009.06410v2,2020-09-09T19:14:38Z,,arXiv,cs.AI,http://arxiv.org/pdf/2009.06410v2.pdf,2009.06410v2,Beneficial and Harmful Explanatory Machine Learning,"Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie's definition of Ultra-Strong Machine Learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine's involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.","cs.AI, cs.LG","Lun Ai, Stephen H. Muggleton, Céline Hocquette, Mark Gromowski, Ute Schmid",[]
True,http://arxiv.org/abs/2106.03015v1,2021-06-06T03:03:04Z,,arXiv,cs.LG,http://arxiv.org/pdf/2106.03015v1.pdf,2106.03015v1,Learning proofs for the classification of nilpotent semigroups,"Machine learning is applied to find proofs, with smaller or smallest numbers of nodes, for the classification of 4-nilpotent semigroups.","cs.LG, math.LO, math.RA, 68T15 (Primary) 20M10, 03F07, 03B35 (Secondary)",Carlos Simpson,[]
True,http://arxiv.org/abs/2106.09756v1,2021-06-17T18:35:37Z,,arXiv,cs.LG,http://arxiv.org/pdf/2106.09756v1.pdf,2106.09756v1,PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python,"Machine learning is a general-purpose technology holding promises for many interdisciplinary research problems. However, significant barriers exist in crossing disciplinary boundaries when most machine learning tools are developed in different areas separately. We present Pykale - a Python library for knowledge-aware machine learning on graphs, images, texts, and videos to enable and accelerate interdisciplinary research. We formulate new green machine learning guidelines based on standard software engineering practices and propose a novel pipeline-based application programming interface (API). PyKale focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction, thus supporting multimodal learning and transfer learning (particularly domain adaptation) with latest deep learning and dimensionality reduction models. We build PyKale on PyTorch and leverage the rich PyTorch ecosystem. Our pipeline-based API design enforces standardization and minimalism, embracing green machine learning concepts via reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. We demonstrate its interdisciplinary nature via examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging.","cs.LG, cs.AI, cs.CV, stat.ML","Haiping Lu, Xianyuan Liu, Robert Turner, Peizhen Bai, Raivo E Koot, Shuo Zhou, Mustafa Chasmai, Lawrence Schobs",[]
True,http://arxiv.org/abs/1904.03259v1,2019-04-05T20:05:46Z,,arXiv,cs.LG,http://arxiv.org/pdf/1904.03259v1.pdf,1904.03259v1,Is 'Unsupervised Learning' a Misconceived Term?,"Is all of machine learning supervised to some degree? The field of machine learning has traditionally been categorized pedagogically into $supervised~vs~unsupervised~learning$; where supervised learning has typically referred to learning from labeled data, while unsupervised learning has typically referred to learning from unlabeled data. In this paper, we assert that all machine learning is in fact supervised to some degree, and that the scope of supervision is necessarily commensurate to the scope of learning potential. In particular, we argue that clustering algorithms such as k-means, and dimensionality reduction algorithms such as principal component analysis, variational autoencoders, and deep belief networks are each internally supervised by the data itself to learn their respective representations of its features. Furthermore, these algorithms are not capable of external inference until their respective outputs (clusters, principal components, or representation codes) have been identified and externally labeled in effect. As such, they do not suffice as examples of unsupervised learning. We propose that the categorization `supervised vs unsupervised learning' be dispensed with, and instead, learning algorithms be categorized as either $internally~or~externally~supervised$ (or both). We believe this change in perspective will yield new fundamental insights into the structure and character of data and of learning algorithms.","cs.LG, cs.AI, cs.CV, stat.ML",Stephen G. Odaibo,[]
True,http://arxiv.org/abs/2403.02432v1,2024-03-04T19:26:39Z,,arXiv,stat.ML,http://arxiv.org/pdf/2403.02432v1.pdf,2403.02432v1,On the impact of measure pre-conditionings on general parametric ML   models and transfer learning via domain adaptation,We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.,"stat.ML, cs.LG, math.OC",Joaquín Sánchez García,[]
True,http://arxiv.org/abs/2202.13608v2,2022-02-28T08:30:24Z,,arXiv,stat.ML,http://arxiv.org/pdf/2202.13608v2.pdf,2202.13608v2,Semi-supervised Learning on Large Graphs: is Poisson Learning a   Game-Changer?,"We explain Poisson learning on graph-based semi-supervised learning to see if it could avoid the problem of global information loss problem as Laplace-based learning methods on large graphs. From our analysis, Poisson learning is simply Laplace regularization with thresholding, cannot overcome the problem.","stat.ML, cs.LG",Canh Hao Nguyen,[]
True,http://arxiv.org/abs/1612.07640v1,2016-12-16T04:56:30Z,,arXiv,cs.LG,http://arxiv.org/pdf/1612.07640v1.pdf,1612.07640v1,Deep Learning and Its Applications to Machine Health Monitoring: A   Survey,"Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). Finally, some new trends of DL-based machine health monitoring methods are discussed.","cs.LG, stat.ML","Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, Robert X. Gao",[]
True,http://arxiv.org/abs/1807.01477v2,2018-07-04T08:25:17Z,Institute of Electrical and Electronics Engineers (IEEE),arXiv,cs.CV,http://arxiv.org/pdf/1807.01477v2.pdf,10.1109/access.2019.2917620,Diversity in Machine Learning,"Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a total good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though the diversity plays an important role in machine learning process, there is no systematical analysis of the diversification in machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process, respectively. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed, including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work.",cs.CV,"Zhiqiang Gong, Ping Zhong, Weidong Hu","['10.1561/2200000044', '10.1109/tpami.2009.167', '10.1109/cvpr.2013.251', '10.1145/1143844.1143980', '10.1109/tgrs.2017.2730583', '10.1109/iccv.2011.6126552', '10.1007/978-3-642-33715-4_1', '10.1109/tgrs.2017.2675902', '10.1109/tgrs.2017.2748120', '10.1145/2783258.2783264', '10.1023/b:visi.0000013087.49260.fb', '10.1109/taslp.2016.2536478', '10.1109/tgrs.2019.2893180', '10.1109/cvpr.2018.00831', '10.3390/rs11010076', '10.1145/1963405.1963415', '10.1007/978-3-319-46466-4_20', '10.1007/978-3-030-01219-9_32', '10.1016/j.patcog.2018.03.003', '10.1016/j.sigpro.2014.11.010', '10.1109/tpami.2014.2353628', '10.1109/tpami.2010.231', '10.1145/2124295.2124329', '10.1145/2396761.2396857', '10.1145/2020408.2020479', '10.1109/tpami.2016.2562626', '10.1109/iccv.2015.312', '10.1109/cvpr.2017.779', '10.1145/1390334.1390446', '10.1016/j.inffus.2018.01.006', '10.1109/icip.2016.7533164', '10.1007/978-3-030-01252-6_2', '10.1145/2020408.2020573', '10.1145/1060745.1060754', '10.1016/j.inffus.2018.07.011', '10.24963/ijcai.2017/296', '10.1023/a:1022859003006', '10.1109/icmla.2016.0021', '10.1145/2339530.2339617', '10.3389/fncom.2015.00104', '10.1016/j.neucom.2016.08.060', '10.1109/ssp.2016.7551844', '10.1109/tpami.2009.144', '10.1007/978-3-319-23528-8_38', '10.1109/icccyb.2008.4721376', '10.1023/a:1007607513941', '10.1016/s0262-8856(01)00045-2', '10.1109/tcsvt.2013.2276704', '10.1109/34.709601', '10.1016/j.patcog.2016.07.038', '10.1109/iccv.2017.59', '10.1109/igarss.2018.8517748', '10.1016/j.ins.2013.12.016', '10.1007/s10489-012-0342-3', '10.1109/tip.2011.2169274', '10.1109/tkde.2015.2433262', '10.1016/j.patcog.2016.08.026', '10.1145/1869790.1869829', '10.1007/978-3-319-11179-7_53', '10.1109/ijcnn.2016.7727538', '10.1111/rssb.12096', '10.1109/tgrs.2018.2886022', '10.24963/ijcai.2017/241', '10.1109/tcsvt.2015.2400779', '10.1007/s10994-006-9449-2', '10.1109/tnnls.2017.2785403', '10.1109/tpami.2015.2400461', '10.1016/j.inffus.2004.04.005', '10.1016/j.inffus.2004.04.004', '10.1109/cvpr.2017.36', '10.1016/j.patcog.2017.01.012', '10.1007/s10044-002-0173-7', '10.1103/physrevlett.110.018701', '10.1109/iccv.2015.211', '10.1007/bf01588971', '10.1007/978-3-319-20248-8_18', '10.1145/956750.956769', '10.1007/s11263-011-0439-x', '10.1109/tpami.2012.79', '10.1145/1498759.1498766', '10.1109/tkde.2012.170', '10.1145/1860702.1860709', '10.1109/tip.2014.2327805', '10.1007/s11263-014-0781-x', '10.1126/science.287.5456.1273', '10.1080/095400996116820', '10.1038/381607a0', '10.1371/journal.pcbi.1002250', '10.1016/j.inffus.2013.11.003', '10.1109/cvpr.2018.00564', '10.1109/mgrs.2013.2244672', '10.1007/s10618-011-0243-9', '10.1016/s0004-3702(02)00190-x', '10.1109/ijcnn.2017.7966354', '10.1109/cvpr.2013.377', '10.1109/cvpr.2014.146']"
True,http://arxiv.org/abs/1904.00001v2,2019-04-01T13:57:27Z,,arXiv,cs.SE,http://arxiv.org/pdf/1904.00001v2.pdf,1904.00001v2,Engineering problems in machine learning systems,"Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems --- that is, in terms of requirement, design, and verification of machine learning models and systems --- as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuARE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.","cs.SE, cs.LG","Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae",[]
True,http://arxiv.org/abs/2201.01288v2,2022-01-04T18:31:31Z,,arXiv,cs.LG,http://arxiv.org/pdf/2201.01288v2.pdf,2201.01288v2,"Automated Graph Machine Learning: Approaches, Libraries, Benchmarks and   Directions","Graph machine learning has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To tackle the challenge, automated graph machine learning, which aims at discovering the best hyper-parameter and neural architecture configuration for different graph tasks/data without manual design, is gaining an increasing number of attentions from the research community. In this paper, we extensively discuss automated graph machine learning approaches, covering hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We briefly overview existing libraries designed for either graph machine learning or automated machine learning respectively, and further in depth introduce AutoGL, our dedicated and the world's first open-source library for automated graph machine learning. Also, we describe a tailored benchmark that supports unified, reproducible, and efficient evaluations. Last but not least, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive discussion of approaches, libraries as well as directions for automated graph machine learning.","cs.LG, cs.AI","Xin Wang, Ziwei Zhang, Haoyang Li, Wenwu Zhu",[]
True,http://arxiv.org/abs/1711.01431v1,2017-11-04T12:54:35Z,,arXiv,cs.AI,http://arxiv.org/pdf/1711.01431v1.pdf,1711.01431v1,The Case for Meta-Cognitive Machine Learning: On Model Entropy and   Concept Formation in Deep Learning,"Machine learning is usually defined in behaviourist terms, where external validation is the primary mechanism of learning. In this paper, I argue for a more holistic interpretation in which finding more probable, efficient and abstract representations is as central to learning as performance. In other words, machine learning should be extended with strategies to reason over its own learning process, leading to so-called meta-cognitive machine learning. As such, the de facto definition of machine learning should be reformulated in these intrinsically multi-objective terms, taking into account not only the task performance but also internal learning objectives. To this end, we suggest a ""model entropy function"" to be defined that quantifies the efficiency of the internal learning processes. It is conjured that the minimization of this model entropy leads to concept formation. Besides philosophical aspects, some initial illustrations are included to support the claims.","cs.AI, cs.LG, stat.ML",Johan Loeckx,[]
True,http://arxiv.org/abs/1907.07543v1,2019-07-17T14:23:15Z,,arXiv,cs.LG,http://arxiv.org/pdf/1907.07543v1.pdf,1907.07543v1,Low-Shot Classification: A Comparison of Classical and Deep Transfer   Machine Learning Approaches,"Despite the recent success of deep transfer learning approaches in NLP, there is a lack of quantitative studies demonstrating the gains these models offer in low-shot text classification tasks over existing paradigms. Deep transfer learning approaches such as BERT and ULMFiT demonstrate that they can beat state-of-the-art results on larger datasets, however when one has only 100-1000 labelled examples per class, the choice of approach is less clear, with classical machine learning and deep transfer learning representing valid options. This paper compares the current best transfer learning approach with top classical machine learning approaches on a trinary sentiment classification task to assess the best paradigm. We find that BERT, representing the best of deep transfer learning, is the best performing approach, outperforming top classical machine learning algorithms by 9.7% on average when trained with 100 examples per class, narrowing to 1.8% at 1000 labels per class. We also show the robustness of deep transfer learning in moving across domains, where the maximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross domain, compared to classical machine learning which loses up to 20.6%.","cs.LG, stat.ML","Peter Usherwood, Steven Smit",[]
True,http://arxiv.org/abs/2107.11277v3,2021-07-23T14:43:56Z,,arXiv,cs.LG,http://arxiv.org/pdf/2107.11277v3.pdf,2107.11277v3,Machine Learning with a Reject Option: A survey,"Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.   This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.","cs.LG, cs.AI, 68T02, I.2.6","Kilian Hendrickx, Lorenzo Perini, Dries Van der Plas, Wannes Meert, Jesse Davis",[]
True,http://arxiv.org/abs/2205.14136v1,2022-05-27T17:55:54Z,,arXiv,cs.LG,http://arxiv.org/pdf/2205.14136v1.pdf,2205.14136v1,PSL is Dead. Long Live PSL,"Property Specification Language (PSL) is a form of temporal logic that has been mainly used in discrete domains (e.g. formal hardware verification). In this paper, we show that by merging machine learning techniques with PSL monitors, we can extend PSL to work on continuous domains. We apply this technique in machine learning-based anomaly detection to analyze scenarios of real-time streaming events from continuous variables in order to detect abnormal behaviors of a system. By using machine learning with formal models, we leverage the strengths of both machine learning methods and formal semantics of time. On one hand, machine learning techniques can produce distributions on continuous variables, where abnormalities can be captured as deviations from the distributions. On the other hand, formal methods can characterize discrete temporal behaviors and relations that cannot be easily learned by machine learning techniques. Interestingly, the anomalies detected by machine learning and the underlying time representation used are discrete events. We implemented a temporal monitoring package (TEF) that operates in conjunction with normal data science packages for anomaly detection machine learning systems, and we show that TEF can be used to perform accurate interpretation of temporal correlation between events.","cs.LG, cs.FL","Kevin Smith, Hai Lin, Praveen Tiwari, Marjorie Sayer, Claudionor Coelho",[]
True,http://arxiv.org/abs/2405.16159v1,2024-05-25T09:58:33Z,,arXiv,cs.LG,http://arxiv.org/pdf/2405.16159v1.pdf,2405.16159v1,A Declarative Query Language for Scientific Machine Learning,"The popularity of data science as a discipline and its importance in the emerging economy and industrial progress dictate that machine learning be democratized for the masses. This also means that the current practice of workforce training using machine learning tools, which requires low-level statistical and algorithmic details, is a barrier that needs to be addressed. Similar to data management languages such as SQL, machine learning needs to be practiced at a conceptual level to help make it a staple tool for general users. In particular, the technical sophistication demanded by existing machine learning frameworks is prohibitive for many scientists who are not computationally savvy or well versed in machine learning techniques. The learning curve to use the needed machine learning tools is also too high for them to take advantage of these powerful platforms to rapidly advance science. In this paper, we introduce a new declarative machine learning query language, called {\em MQL}, for naive users. We discuss its merit and possible ways of implementing it over a traditional relational database system. We discuss two materials science experiments implemented using MQL on a materials science workflow system called MatFlow.","cs.LG, cs.DB",Hasan M Jamil,[]
True,http://arxiv.org/abs/2409.04923v1,2024-09-07T22:13:26Z,,arXiv,physics.flu-dyn,http://arxiv.org/pdf/2409.04923v1.pdf,2409.04923v1,Single-snapshot machine learning for turbulence super resolution,"Modern machine-learning techniques are generally considered data-hungry. However, this may not be the case for turbulence as each of its snapshots can hold more information than a single data file in general machine-learning applications. This study asks the question of whether nonlinear machine-learning techniques can effectively extract physical insights even from as little as a single snapshot of a turbulent vortical flow. As an example, we consider machine-learning-based super-resolution analysis that reconstructs a high-resolution field from low-resolution data for two-dimensional decaying turbulence. We reveal that a carefully designed machine-learning model trained with flow tiles sampled from only a single snapshot can reconstruct vortical structures across a range of Reynolds numbers. Successful flow reconstruction indicates that nonlinear machine-learning techniques can leverage scale-invariance properties to learn turbulent flows. We further show that training data of turbulent flows can be cleverly collected from a single snapshot by considering characteristics of rotation and shear tensors. The present findings suggest that embedding prior knowledge in designing a model and collecting data is important for a range of data-driven analyses for turbulent flows. More broadly, this work hopes to stop machine-learning practitioners from being wasteful with turbulent flow data.","physics.flu-dyn, cs.LG, physics.comp-ph","Kai Fukami, Kunihiko Taira",[]
True,http://arxiv.org/abs/1303.2104v1,2013-03-08T20:46:27Z,,arXiv,cs.LG,http://arxiv.org/pdf/1303.2104v1.pdf,1303.2104v1,Transfer Learning for Voice Activity Detection: A Denoising Deep Neural   Network Perspective,"Mismatching problem between the source and target noisy corpora severely hinder the practical use of the machine-learning-based voice activity detection (VAD). In this paper, we try to address this problem in the transfer learning prospective. Transfer learning tries to find a common learning machine or a common feature subspace that is shared by both the source corpus and the target corpus. The denoising deep neural network is used as the learning machine. Three transfer techniques, which aim to learn common feature representations, are used for analysis. Experimental results demonstrate the effectiveness of the transfer learning schemes on the mismatch problem.",cs.LG,"Xiao-Lei Zhang, Ji Wu",[]
True,http://arxiv.org/abs/1807.10681v1,2018-07-27T15:21:43Z,,arXiv,cs.LG,http://arxiv.org/pdf/1807.10681v1.pdf,1807.10681v1,Learnable: Theory vs Applications,"Two different views on machine learning problem: Applied learning (machine learning with business applications) and Agnostic PAC learning are formalized and compared here. I show that, under some conditions, the theory of PAC Learnable provides a way to solve the Applied learning problem. However, the theory requires to have the training sets so large, that it would make the learning practically useless. I suggest shedding some theoretical misconceptions about learning to make the theory more aligned with the needs and experience of practitioners.","cs.LG, stat.ML",Marina Sapir,[]
True,http://arxiv.org/abs/1905.07822v2,2019-05-19T22:46:23Z,,arXiv,cs.LG,http://arxiv.org/pdf/1905.07822v2.pdf,1905.07822v2,Minimal Achievable Sufficient Statistic Learning,"We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a training method for machine learning models that attempts to produce minimal sufficient statistics with respect to a class of functions (e.g. deep networks) being optimized over. In deriving MASS Learning, we also introduce Conserved Differential Information (CDI), an information-theoretic quantity that - unlike standard mutual information - can be usefully applied to deterministically-dependent continuous random variables like the input and output of a deep network. In a series of experiments, we show that deep networks trained with MASS Learning achieve competitive performance on supervised learning and uncertainty quantification benchmarks.","cs.LG, stat.ML","Milan Cvitkovic, Günther Koliander",[]
True,http://arxiv.org/abs/2102.11274v1,2021-02-22T18:58:47Z,,arXiv,cs.LG,http://arxiv.org/pdf/2102.11274v1.pdf,2102.11274v1,Sustainable Federated Learning,"Potential environmental impact of machine learning by large-scale wireless networks is a major challenge for the sustainability of future smart ecosystems. In this paper, we introduce sustainable machine learning in federated learning settings, using rechargeable devices that can collect energy from the ambient environment. We propose a practical federated learning framework that leverages intermittent energy arrivals for training, with provable convergence guarantees. Our framework can be applied to a wide range of machine learning settings in networked environments, including distributed and federated learning in wireless and edge networks. Our experiments demonstrate that the proposed framework can provide significant performance improvement over the benchmark energy-agnostic federated learning settings.","cs.LG, cs.IT, math.IT","Basak Guler, Aylin Yener",[]
True,http://arxiv.org/abs/2305.00520v1,2023-04-30T16:36:57Z,,arXiv,stat.ML,http://arxiv.org/pdf/2305.00520v1.pdf,2305.00520v1,The ART of Transfer Learning: An Adaptive and Robust Pipeline,"Transfer learning is an essential tool for improving the performance of primary tasks by leveraging information from auxiliary data resources. In this work, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline of performing transfer learning with generic machine learning algorithms. We establish the non-asymptotic learning theory of ART, providing a provable theoretical guarantee for achieving adaptive transfer while preventing negative transfer. Additionally, we introduce an ART-integrated-aggregating machine that produces a single final model when multiple candidate algorithms are considered. We demonstrate the promising performance of ART through extensive empirical studies on regression, classification, and sparse learning. We further present a real-data analysis for a mortality study.","stat.ML, cs.LG","Boxiang Wang, Yunan Wu, Chenglong Ye",[]
True,http://arxiv.org/abs/1404.6674v1,2014-04-26T19:24:24Z,,arXiv,cs.LG,http://arxiv.org/pdf/1404.6674v1.pdf,1404.6674v1,A Comparison of First-order Algorithms for Machine Learning,"Using an optimization algorithm to solve a machine learning problem is one of mainstreams in the field of science. In this work, we demonstrate a comprehensive comparison of some state-of-the-art first-order optimization algorithms for convex optimization problems in machine learning. We concentrate on several smooth and non-smooth machine learning problems with a loss function plus a regularizer. The overall experimental results show the superiority of primal-dual algorithms in solving a machine learning problem from the perspectives of the ease to construct, running time and accuracy.",cs.LG,"Yu Wei, Pock Thomas",[]
True,http://arxiv.org/abs/1607.00279v1,2016-07-01T15:07:52Z,,arXiv,stat.ML,http://arxiv.org/pdf/1607.00279v1.pdf,1607.00279v1,Meaningful Models: Utilizing Conceptual Structure to Improve Machine   Learning Interpretability,"The last decade has seen huge progress in the development of advanced machine learning models; however, those models are powerless unless human users can interpret them. Here we show how the mind's construction of concepts and meaning can be used to create more interpretable machine learning models. By proposing a novel method of classifying concepts, in terms of 'form' and 'function', we elucidate the nature of meaning and offer proposals to improve model understandability. As machine learning begins to permeate daily life, interpretable models may serve as a bridge between domain-expert authors and non-expert users.","stat.ML, cs.AI",Nick Condry,[]
True,http://arxiv.org/abs/1805.07072v1,2018-05-18T07:04:37Z,,arXiv,stat.ML,http://arxiv.org/pdf/1805.07072v1.pdf,1805.07072v1,Optimizing for Generalization in Machine Learning with Cross-Validation   Gradients,"Cross-validation is the workhorse of modern applied statistics and machine learning, as it provides a principled framework for selecting the model that maximizes generalization performance. In this paper, we show that the cross-validation risk is differentiable with respect to the hyperparameters and training data for many common machine learning algorithms, including logistic regression, elastic-net regression, and support vector machines. Leveraging this property of differentiability, we propose a cross-validation gradient method (CVGM) for hyperparameter optimization. Our method enables efficient optimization in high-dimensional hyperparameter spaces of the cross-validation risk, the best surrogate of the true generalization ability of our learning algorithm.","stat.ML, cs.LG","Shane Barratt, Rishi Sharma",[]
True,http://arxiv.org/abs/1805.11959v2,2018-05-26T20:54:13Z,,arXiv,cs.LG,http://arxiv.org/pdf/1805.11959v2.pdf,1805.11959v2,Algebraic Expression of Subjective Spatial and Temporal Patterns,"Universal learning machine is a theory trying to study machine learning from mathematical point of view. The outside world is reflected inside an universal learning machine according to pattern of incoming data. This is subjective pattern of learning machine. In [2,4], we discussed subjective spatial pattern, and established a powerful tool -- X-form, which is an algebraic expression for subjective spatial pattern. However, as the initial stage of study, there we only discussed spatial pattern. Here, we will discuss spatial and temporal patterns, and algebraic expression for them.","cs.LG, stat.ML",Chuyu Xiong,[]
True,http://arxiv.org/abs/1903.00092v2,2019-02-28T22:34:46Z,,arXiv,cs.LG,http://arxiv.org/pdf/1903.00092v2.pdf,1903.00092v2,Optimal Algorithms for Ski Rental with Soft Machine-Learned Predictions,"We consider a variant of the classic Ski Rental online algorithm with applications to machine learning. In our variant, we allow the skier access to a black-box machine-learning algorithm that provides an estimate of the probability that there will be at most a threshold number of ski-days. We derive a class of optimal randomized algorithms to determine the strategy that minimizes the worst-case expected competitive ratio for the skier given a prediction from the machine learning algorithm,and analyze the performance and robustness of these algorithms.","cs.LG, cs.DS, stat.ML",Rohan Kodialam,[]
True,http://arxiv.org/abs/1911.07679v1,2019-11-18T15:04:31Z,,arXiv,cs.LG,http://arxiv.org/pdf/1911.07679v1.pdf,1911.07679v1,Towards Quantification of Bias in Machine Learning for Healthcare: A   Case Study of Renal Failure Prediction,"As machine learning (ML) models, trained on real-world datasets, become common practice, it is critical to measure and quantify their potential biases. In this paper, we focus on renal failure and compare a commonly used traditional risk score, Tangri, with a more powerful machine learning model, which has access to a larger variable set and trained on 1.6 million patients' EHR data. We will compare and discuss the generalization and applicability of these two models, in an attempt to quantify biases of status quo clinical practice, compared to ML-driven models.","cs.LG, stat.AP, stat.ML","Josie Williams, Narges Razavian",[]
True,http://arxiv.org/abs/1911.07749v1,2019-11-15T08:14:26Z,,arXiv,cs.LG,http://arxiv.org/pdf/1911.07749v1.pdf,1911.07749v1,On the computation of counterfactual explanations -- A survey,Due to the increasing use of machine learning in practice it becomes more and more important to be able to explain the prediction and behavior of machine learning models. An instance of explanations are counterfactual explanations which provide an intuitive and useful explanations of machine learning models. In this survey we review model-specific methods for efficiently computing counterfactual explanations of many different machine learning models and propose methods for models that have not been considered in literature so far.,"cs.LG, cs.AI, stat.ML","André Artelt, Barbara Hammer",[]
True,http://arxiv.org/abs/1911.12593v1,2019-11-28T08:43:01Z,,arXiv,cs.CY,http://arxiv.org/pdf/1911.12593v1.pdf,1911.12593v1,"Computer Systems Have 99 Problems, Let's Not Make Machine Learning   Another One","Machine learning techniques are finding many applications in computer systems, including many tasks that require decision making: network optimization, quality of service assurance, and security. We believe machine learning systems are here to stay, and to materialize on their potential we advocate a fresh look at various key issues that need further attention, including security as a requirement and system complexity, and how machine learning systems affect them. We also discuss reproducibility as a key requirement for sustainable machine learning systems, and leads to pursuing it.","cs.CY, cs.CR, cs.LG","David Mohaisen, Songqing Chen",[]
True,http://arxiv.org/abs/2008.07758v1,2020-08-18T06:17:31Z,,arXiv,cs.CR,http://arxiv.org/pdf/2008.07758v1.pdf,2008.07758v1,Efficient Private Machine Learning by Differentiable Random   Transformations,"With the increasing demands for privacy protection, many privacy-preserving machine learning systems were proposed in recent years. However, most of them cannot be put into production due to their slow training and inference speed caused by the heavy cost of homomorphic encryption and secure multiparty computation(MPC) methods. To circumvent this, I proposed a privacy definition which is suitable for large amount of data in machine learning tasks. Based on that, I showed that random transformations like linear transformation and random permutation can well protect privacy. Merging random transformations and arithmetic sharing together, I designed a framework for private machine learning with high efficiency and low computation cost.","cs.CR, cs.LG, stat.ML",Fei Zheng,[]
True,http://arxiv.org/abs/1801.04016v1,2018-01-11T23:37:48Z,,arXiv,cs.LG,http://arxiv.org/pdf/1801.04016v1.pdf,1801.04016v1,Theoretical Impediments to Machine Learning With Seven Sparks from the   Causal Revolution,"Current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.","cs.LG, cs.AI, stat.ML",Judea Pearl,[]
True,http://arxiv.org/abs/2101.04025v2,2021-01-11T16:58:30Z,ACM,arXiv,cs.DC,http://arxiv.org/pdf/2101.04025v2.pdf,10.1145/3447545.3451181,Distributed Double Machine Learning with a Serverless Architecture,"This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation \texttt{DoubleML-Serverless} for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs.","cs.DC, cs.LG, stat.ML",Malte S. Kurz,"['10.1109/infocom41043.2020.9155363', '10.32614/cran.package.doubleml', '10.32614/cran.package.doubleml', '10.1214/17-aos1671', '10.1093/restud/rdt044', '10.1093/biomet/asu056', '10.1145/3357223.3362711', '10.1093/ectj/utaa001', '10.1111/ectj.12097', '10.1016/j.jeconom.2020.09.003', '10.1111/jofi.12883', '10.1109/ic2e.2018.00052', '10.1145/3127479.3128601', '10.1145/3341105.3373948', '10.1109/ic2e48712.2020.00023', '10.2307/1912705', '10.1073/pnas.2015577118', '10.18653/v1/n18-5002', '10.1145/3185768.3186308', '10.1145/3154847.3154848', '10.1109/infocom.2019.8737391']"
True,http://arxiv.org/abs/2111.04439v1,2021-10-25T03:40:25Z,,arXiv,cs.CY,http://arxiv.org/pdf/2111.04439v1.pdf,2111.04439v1,Addressing Privacy Threats from Machine Learning,"Every year at NeurIPS, machine learning researchers gather and discuss exciting applications of machine learning in areas such as public health, disaster response, climate change, education, and more. However, many of these same researchers are expressing growing concern about applications of machine learning for surveillance (Nanayakkara et al., 2021). This paper presents a brief overview of strategies for resisting these surveillance technologies and calls for greater collaboration between machine learning and human-computer interaction researchers to address the threats that these technologies pose.","cs.CY, cs.CR, cs.LG",Mary Anne Smart,[]
True,http://arxiv.org/abs/1812.10422v1,2018-12-13T11:02:01Z,,arXiv,cs.CY,http://arxiv.org/pdf/1812.10422v1.pdf,1812.10422v1,Machine Learning in Official Statistics,"In the first half of 2018, the Federal Statistical Office of Germany (Destatis) carried out a ""Proof of Concept Machine Learning"" as part of its Digital Agenda. A major component of this was surveys on the use of machine learning methods in official statistics, which were conducted at selected national and international statistical institutions and among the divisions of Destatis. It was of particular interest to find out in which statistical areas and for which tasks machine learning is used and which methods are applied. This paper is intended to make the results of the surveys publicly accessible.","cs.CY, cs.LG, stat.ML","Martin Beck, Florian Dumpert, Joerg Feuerhake",[]
True,http://arxiv.org/abs/1902.06789v2,2019-02-18T20:38:14Z,,arXiv,cs.LG,http://arxiv.org/pdf/1902.06789v2.pdf,1902.06789v2,Seven Myths in Machine Learning Research,"We present seven myths commonly believed to be true in machine learning research, circa Feb 2019. This is an archival copy of the blog post at https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/   Myth 1: TensorFlow is a Tensor manipulation library   Myth 2: Image datasets are representative of real images found in the wild   Myth 3: Machine Learning researchers do not use the test set for validation   Myth 4: Every datapoint is used in training a neural network   Myth 5: We need (batch) normalization to train very deep residual networks   Myth 6: Attention $>$ Convolution   Myth 7: Saliency maps are robust ways to interpret neural networks","cs.LG, stat.ML","Oscar Chang, Hod Lipson",[]
True,http://arxiv.org/abs/2006.00700v1,2020-06-01T03:56:47Z,,arXiv,q-bio.MN,http://arxiv.org/pdf/2006.00700v1.pdf,2006.00700v1,When Machine Learning Meets Multiscale Modeling in Chemical Reactions,"Due to the intrinsic complexity and nonlinearity of chemical reactions, direct applications of traditional machine learning algorithms may face with many difficulties. In this study, through two concrete examples with biological background, we illustrate how the key ideas of multiscale modeling can help to reduce the computational cost of machine learning a lot, as well as how machine learning algorithms perform model reduction automatically in a time-scale separated system. Our study highlights the necessity and effectiveness of an integration of machine learning algorithms and multiscale modeling during the study of chemical reactions.","q-bio.MN, cs.LG","Wuyue Yang, Liangrong Peng, Yi Zhu, Liu Hong",[]
True,http://arxiv.org/abs/2203.06430v2,2022-03-12T13:03:30Z,,arXiv,cs.LG,http://arxiv.org/pdf/2203.06430v2.pdf,2203.06430v2,Categories of Differentiable Polynomial Circuits for Machine Learning,"Reverse derivative categories (RDCs) have recently been shown to be a suitable semantic framework for studying machine learning algorithms. Whereas emphasis has been put on training methodologies, less attention has been devoted to particular \emph{model classes}: the concrete categories whose morphisms represent machine learning models. In this paper we study presentations by generators and equations of classes of RDCs. In particular, we propose \emph{polynomial circuits} as a suitable machine learning model. We give an axiomatisation for these circuits and prove a functional completeness result. Finally, we discuss the use of polynomial circuits over specific semirings to perform machine learning with discrete values.","cs.LG, math.CT","Paul Wilson, Fabio Zanasi",[]
True,http://arxiv.org/abs/2203.16797v1,2022-03-31T04:58:27Z,,arXiv,cs.LG,http://arxiv.org/pdf/2203.16797v1.pdf,2203.16797v1,When Physics Meets Machine Learning: A Survey of Physics-Informed   Machine Learning,"Physics-informed machine learning (PIML), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in PIML and summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of physics knowledge integration in PIML. We also discuss current challenges and corresponding research opportunities in PIML.","cs.LG, stat.ML","Chuizheng Meng, Sungyong Seo, Defu Cao, Sam Griesemer, Yan Liu",[]
True,http://arxiv.org/abs/2208.10896v2,2022-08-23T12:03:04Z,,arXiv,econ.EM,http://arxiv.org/pdf/2208.10896v2.pdf,2208.10896v2,pystacked: Stacking generalization and machine learning in Stata,"pystacked implements stacked generalization (Wolpert, 1992) for regression and binary classification via Python's scikit-learn. Stacking combines multiple supervised machine learners -- the ""base"" or ""level-0"" learners -- into a single learner. The currently supported base learners include regularized regression, random forest, gradient boosted trees, support vector machines, and feed-forward neural nets (multi-layer perceptron). pystacked can also be used with as a `regular' machine learning program to fit a single base learner and, thus, provides an easy-to-use API for scikit-learn's machine learning algorithms.","econ.EM, stat.ML","Achim Ahrens, Christian B. Hansen, Mark E. Schaffer",[]
True,http://arxiv.org/abs/2305.10472v2,2023-05-17T15:41:08Z,,arXiv,q-bio.PE,http://arxiv.org/pdf/2305.10472v2.pdf,2305.10472v2,Nine tips for ecologists using machine learning,"Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. Here we provide a series of tips to help ecologists in implementing machine learning models. We focus on classification problems as many ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Each of the nine tips identifies a common error, trap or challenge in developing machine learning models and provides recommendations to facilitate their use in ecological studies.","q-bio.PE, cs.LG","Marine Desprez, Vincent Miele, Olivier Gimenez",[]
True,http://arxiv.org/abs/2307.02071v1,2023-07-05T07:26:27Z,,arXiv,cs.LG,http://arxiv.org/pdf/2307.02071v1.pdf,2307.02071v1,A Comparison of Machine Learning Methods for Data with High-Cardinality   Categorical Variables,"High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level. Machine learning methods can have difficulties with high-cardinality variables. In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.","cs.LG, cs.AI, stat.ML",Fabio Sigrist,[]
True,http://arxiv.org/abs/2311.00196v1,2023-11-01T00:02:09Z,Wiley,arXiv,physics.chem-ph,http://arxiv.org/pdf/2311.00196v1.pdf,10.1002/jcc.27366,Machine learning for accuracy in density functional approximations,"Machine learning techniques have found their way into computational chemistry as indispensable tools to accelerate atomistic simulations and materials design. In addition, machine learning approaches hold the potential to boost the predictive power of computationally efficient electronic structure methods, such as density functional theory, to chemical accuracy and to correct for fundamental errors in density functional approaches. Here, recent progress in applying machine learning to improve the accuracy of density functional and related approximations is reviewed. Promises and challenges in devising machine learning models transferable between different chemistries and materials classes are discussed with the help of examples applying promising models to systems far outside their training sets.","physics.chem-ph, cs.LG",Johannes Voss,"['10.1021/acs.chemrev.1c00107', '10.1038/s41524-019-0221-0', '10.1038/s41467-020-18556-9', '10.1103/physrevlett.98.146401', '10.1063/1.5019779', '10.1039/c6sc05720a', '10.1103/physrevb.96.014112', '10.1103/physrevlett.104.136403', '10.1126/sciadv.1603015', '10.1021/acs.chemrev.0c01111', '10.1038/ncomms15679', '10.1103/physrevlett.120.145301', '10.1021/cm100795d', '10.1038/npjcompumats.2016.28', '10.1038/s41524-018-0128-1', '10.1016/j.matt.2019.08.017', '10.1103/physrev.136.b864', '10.1021/jp960669l', '10.1063/1.4704546', '10.1021/ct800531s', '10.1021/cr200107z', '10.1063/1.4869598', '10.1016/j.trechm.2020.02.005', '10.1002/wcms.1631', '10.1103/physrevb.45.13244', '10.1103/physrevb.23.5048', '10.1103/physrevlett.45.566', '10.1103/physrevlett.100.136406', '10.1103/physrevb.73.235116', '10.1103/physrevlett.80.890', '10.1103/physrevb.59.7413', '10.1103/physrevlett.91.146401', '10.1103/physrevlett.115.036402', '10.1073/pnas.1705670114', '10.1021/acs.jctc.8b00288', '10.1002/jcc.26732', '10.1103/physrevmaterials.6.083803', '10.1063/1.464304', '10.1063/1.1383587', '10.1016/s0009-2614(97)00758-6', '10.1063/1.1564060', '10.1103/physrevb.100.035439', '10.1063/1.1794633', '10.1021/acs.jpclett.5b00733', '10.1063/1.440939', '10.1002/qua.560520414', '10.1103/physrev.82.625', '10.1063/1.3317437', '10.1103/physrevlett.49.1691', '10.1063/1.2403848', '10.1103/physrevb.44.943', '10.1103/physrevb.49.14211', '10.1103/physrevb.52.r5467', '10.1103/physrevb.71.035105', '10.1103/physrevb.73.195107', '10.1103/physrevlett.100.146401', '10.1103/physrevb.82.115106', '10.1103/physrevlett.76.102', '10.1103/physrevlett.92.246401', '10.1063/1.3521275', '10.1002/jcc.20495', '10.1063/1.3382344', '10.1063/1.5090222', '10.1103/physrevlett.102.073005', '10.1063/1.3269802', '10.1063/1.3545985', '10.1063/1.477422', '10.1063/1.2436888', '10.1063/1.481336', '10.1063/1.2039080', '10.1038/sdata.2014.22', '10.1063/1.2770701', '10.1021/ci300415d', '10.1063/1.2348881', '10.1063/1.2755751', '10.1016/j.cplett.2011.05.007', '10.1002/jcc.24854', '10.1021/ct2002946', '10.1021/ct200523a', '10.1063/1.1727484', '10.1039/c6cp00688d', '10.1021/ct600281g', '10.1021/ct800568m', '10.1021/jp049908s', '10.1021/jp045141s', '10.1039/c7cp04913g', '10.1080/00268976.2017.1333644', '10.1021/acs.jctc.9b00239', '10.1103/physrevb.84.045115', '10.1063/1.4812323', '10.1038/npjcompumats.2015.10', '10.1103/physrevb.93.235162', '10.1088/1367-2630/aac7f0', '10.1063/1.1626543', '10.1063/1.1321305', '10.1103/physrevb.63.224115', '10.1103/physrevb.85.014111', '10.1103/physrevlett.77.3865', '10.1002/jcc.26872', '10.1016/j.susc.2015.03.023', '10.1021/acs.jpcc.7b05677', '10.1021/acs.jctc.2c00824', '10.1126/science.aah5975', '10.1038/nature14539', '10.1063/1.464913', '10.1063/1.460205', '10.1103/physrevb.37.785', '10.1021/j100096a001', '10.1021/acs.jpca.0c01375', '10.1063/1.475007', '10.1039/c3cp54374a', '10.1038/s43588-022-00371-5', '10.1063/1.477267', '10.1063/1.480732', '10.1063/1.1342776', '10.1063/1.1599338', '10.1063/1.2126975', '10.1063/1.2370993', '10.1007/s00214-007-0310-x', '10.1021/jz201525m', '10.1021/acs.jctc.5b01082', '10.1073/pnas.1913699117', '10.1021/ct0502763', '10.1063/1.4870763', '10.1146/annurev-physchem-062422-013259', '10.1021/acs.jpclett.2c00643', '10.1063/1.4870397', '10.1103/physrevb.85.235149', '10.1080/00401706.1970.10488634', '10.1063/5.0098787', '10.1007/s00706-018-2335-3', '10.1007/bf00175355', '10.1126/sciadv.abq0279', '10.1609/aaai.v33i01.33014780', '10.1021/acs.jctc.3c00848', '10.1038/323533a0', '10.1103/physrevresearch.4.013106', '10.1038/s41524-020-0310-0', '10.1063/1.1535422', '10.1088/2632-2153/ac3149', '10.1063/5.0148438', '10.1021/acs.jctc.0c00872', '10.1103/physrevlett.126.036401', '10.1103/physrevlett.69.2863', '10.1021/acs.jpclett.2c00371', '10.1103/physrevlett.127.126403', '10.1063/5.0076202', '10.1063/1.472753', '10.1103/physreva.50.2138', '10.1162/089976699300016106', '10.1214/aos/1013203451', '10.1021/acs.jpca.1c10491', '10.1145/2939672.2939785', '10.1038/s41467-020-19093-1', '10.1038/s41467-020-20471-y', '10.1021/acs.jpca.0c03886', '10.1063/1.5088393', '10.1088/1367-2630/15/9/095003', '10.1021/ja902302h', '10.1021/acs.jctc.3c00518', '10.1021/acs.jctc.0c00898', '10.1103/physrev.46.618', '10.1021/acs.jctc.5b00099', '10.1021/acs.jctc.9b00627', '10.1063/1.5114618', '10.1021/ct400863t', '10.1021/ct500079y', '10.1038/s41467-020-17265-7', '10.1103/physrevlett.108.253002', '10.1002/qua.25040', '10.1063/5.0138429', '10.1038/s41467-017-00839-3', '10.1021/acscentsci.8b00551', '10.1039/c9sc02696g', '10.1038/s41524-019-0162-7', '10.1016/j.jcp.2021.110523', '10.1038/s41524-023-01115-3', '10.1038/s41524-023-01053-0', '10.1038/s41467-023-41953-9', '10.1063/5.0122761', '10.1103/physreva.96.022502', '10.1103/physrevmaterials.3.063801', '10.1038/s41524-020-00446-9', '10.1021/acs.jctc.1c00904', '10.1103/physrevb.13.4274', '10.1103/physrevlett.55.1665', '10.1063/5.0091198', '10.1126/science.aal3442', '10.1103/physrev.139.a796', '10.1103/physrevb.34.5390', '10.1103/physrevb.98.115161', '10.1021/acs.jpclett.0c02405', '10.1063/1.2204597', '10.1007/10135124_11', '10.1103/physrevlett.59.819', '10.1103/physrevb.100.041113', '10.1063/1.2065267', '10.1002/jcc.21759', '10.1038/s41524-019-0242-8', '10.1063/5.0004860', '10.1103/physrevlett.130.036401', '10.1063/5.0121187', '10.1038/s41592-019-0686-2', '10.1103/physrevb.97.085130', '10.1088/0953-8984/21/39/395502', '10.1103/physrevb.88.085117', '10.1016/j.cpc.2015.05.011', '10.1103/physrevb.54.11169', '10.1103/physrevb.50.17953', '10.1103/physrevb.59.1758', '10.1103/physrevb.34.4405', '10.1088/1361-648x/ab51ff']"
,http://arxiv.org/abs/2408.12655v1,2024-08-22T18:01:21Z,,arXiv,cs.LG,http://arxiv.org/pdf/2408.12655v1.pdf,2408.12655v1,Improving Radiography Machine Learning Workflows via Metadata Management   for Training Data Selection,"Most machine learning models require many iterations of hyper-parameter tuning, feature engineering, and debugging to produce effective results. As machine learning models become more complicated, this pipeline becomes more difficult to manage effectively. In the physical sciences, there is an ever-increasing pool of metadata that is generated by the scientific research cycle. Tracking this metadata can reduce redundant work, improve reproducibility, and aid in the feature and training dataset engineering process. In this case study, we present a tool for machine learning metadata management in dynamic radiography. We evaluate the efficacy of this tool against the initial research workflow and discuss extensions to general machine learning pipelines in the physical sciences.","cs.LG, cs.HC","Mirabel Reid, Christine Sweeney, Oleg Korobkin",[]
,http://arxiv.org/abs/2409.03669v1,2024-09-05T16:23:07Z,,arXiv,stat.ML,http://arxiv.org/pdf/2409.03669v1.pdf,2409.03669v1,A method to benchmark high-dimensional process drift detection,"Process curves are multi-variate finite time series data coming from manufacturing processes. This paper studies machine learning methods for drifts of process curves. A theoretic framework to synthetically generate process curves in a controlled way is introduced in order to benchmark machine learning algorithms for process drift detection. A evaluation score, called the temporal area under the curve, is introduced, which allows to quantify how well machine learning models unveil curves belonging to drift segments. Finally, a benchmark study comparing popular machine learning approaches on synthetic data generated with the introduced framework shown.","stat.ML, cs.AI, cs.LG","Edgar Wolf, Tobias Windisch",[]
,http://arxiv.org/abs/2410.10523v1,2024-10-14T14:01:35Z,,arXiv,stat.ML,http://arxiv.org/pdf/2410.10523v1.pdf,2410.10523v1,Inverse Problems and Data Assimilation: A Machine Learning Approach,"The aim of these notes is to demonstrate the potential for ideas in machine learning to impact on the fields of inverse problems and data assimilation. The perspective is one that is primarily aimed at researchers from inverse problems and/or data assimilation who wish to see a mathematical presentation of machine learning as it pertains to their fields. As a by-product, we include a succinct mathematical treatment of various topics in machine learning.","stat.ML, cs.LG, math.OC","Eviatar Bach, Ricardo Baptista, Daniel Sanz-Alonso, Andrew Stuart",[]
,http://arxiv.org/abs/1611.09139v1,2016-11-28T14:34:15Z,,arXiv,stat.ML,http://arxiv.org/pdf/1611.09139v1.pdf,1611.09139v1,Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for   Complex Systems,"This is the Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for Complex Systems, held in Barcelona, Spain on December 9, 2016",stat.ML,"Andrew Gordon Wilson, Been Kim, William Herlands",[]
,http://arxiv.org/abs/1703.01977v1,2017-02-26T10:41:26Z,,arXiv,stat.AP,http://arxiv.org/pdf/1703.01977v1.pdf,1703.01977v1,"Linear, Machine Learning and Probabilistic Approaches for Time Series   Analysis","In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.","stat.AP, cs.LG, stat.ME",B. M. Pavlyshenko,[]
,http://arxiv.org/abs/1711.09522v2,2017-11-27T03:27:17Z,,arXiv,stat.ML,http://arxiv.org/pdf/1711.09522v2.pdf,1711.09522v2,Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing   World,"This is the Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing World, held in Long Beach, California, USA on December 8, 2017",stat.ML,"Maria De-Arteaga, William Herlands",[]
,http://arxiv.org/abs/1802.00382v1,2018-02-01T16:46:00Z,,arXiv,cs.LG,http://arxiv.org/pdf/1802.00382v1.pdf,1802.00382v1,Classifying medical notes into standard disease codes using Machine   Learning,"We investigate the automatic classification of patient discharge notes into standard disease labels. We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement.","cs.LG, cs.CL, stat.AP, stat.ML",Amitabha Karmakar,[]
,http://arxiv.org/abs/2001.07278v1,2020-01-20T23:09:32Z,,arXiv,cs.LG,http://arxiv.org/pdf/2001.07278v1.pdf,2001.07278v1,Mixed integer programming formulation of unsupervised learning,"A novel formulation and training procedure for full Boltzmann machines in terms of a mixed binary quadratic feasibility problem is given. As a proof of concept, the theory is analytically and numerically tested on XOR patterns.","cs.LG, cond-mat.dis-nn, stat.ML",Arturo Berrones-Santos,[]
,http://arxiv.org/abs/2201.04703v1,2022-01-12T21:08:38Z,,arXiv,eess.IV,http://arxiv.org/pdf/2201.04703v1.pdf,2201.04703v1,Detection of brain tumors using machine learning algorithms,An algorithm capable of processing NMR images was developed for analysis using machine learning techniques to detect the presence of brain tumors.,"eess.IV, cs.LG","Horacio Corral, Javier Melchor, Balam Sotelo, Jorge Vera",[]
,http://arxiv.org/abs/2211.14401v2,2022-11-25T23:37:24Z,,arXiv,astro-ph.IM,http://arxiv.org/pdf/2211.14401v2.pdf,2211.14401v2,Elements of effective machine learning datasets in astronomy,"In this work, we identify elements of effective machine learning datasets in astronomy and present suggestions for their design and creation. Machine learning has become an increasingly important tool for analyzing and understanding the large-scale flood of data in astronomy. To take advantage of these tools, datasets are required for training and testing. However, building machine learning datasets for astronomy can be challenging. Astronomical data is collected from instruments built to explore science questions in a traditional fashion rather than to conduct machine learning. Thus, it is often the case that raw data, or even downstream processed data is not in a form amenable to machine learning. We explore the construction of machine learning datasets and we ask: what elements define effective machine learning datasets? We define effective machine learning datasets in astronomy to be formed with well-defined data points, structure, and metadata. We discuss why these elements are important for astronomical applications and ways to put them in practice. We posit that these qualities not only make the data suitable for machine learning, they also help to foster usable, reusable, and replicable science practices.","astro-ph.IM, cs.LG","Bernie Boscoe, Tuan Do, Evan Jones, Yunqi Li, Kevin Alfaro, Christy Ma",[]
,http://arxiv.org/abs/1807.06689v1,2018-07-17T22:18:19Z,,arXiv,cs.LG,http://arxiv.org/pdf/1807.06689v1.pdf,1807.06689v1,Efficient Deep Learning on Multi-Source Private Data,"Machine learning models benefit from large and diverse datasets. Using such datasets, however, often requires trusting a centralized data aggregator. For sensitive applications like healthcare and finance this is undesirable as it could compromise patient privacy or divulge trade secrets. Recent advances in secure and privacy-preserving computation, including trusted hardware enclaves and differential privacy, offer a way for mutually distrusting parties to efficiently train a machine learning model without revealing the training data. In this work, we introduce Myelin, a deep learning framework which combines these privacy-preservation primitives, and use it to establish a baseline level of performance for fully private machine learning.","cs.LG, stat.ML","Nick Hynes, Raymond Cheng, Dawn Song",[]
,http://arxiv.org/abs/1811.00542v1,2018-10-31T22:54:12Z,,arXiv,stat.ML,http://arxiv.org/pdf/1811.00542v1.pdf,1811.00542v1,Pymc-learn: Practical Probabilistic Machine Learning in Python,"$\textit{Pymc-learn}$ is a Python package providing a variety of state-of-the-art probabilistic models for supervised and unsupervised machine learning. It is inspired by $\textit{scikit-learn}$ and focuses on bringing probabilistic machine learning to non-specialists. It uses a general-purpose high-level language that mimics $\textit{scikit-learn}$. Emphasis is put on ease of use, productivity, flexibility, performance, documentation, and an API consistent with $\textit{scikit-learn}$. It depends on $\textit{scikit-learn}$ and $\textit{pymc3}$ and is distributed under the new BSD-3 license, encouraging its use in both academia and industry. Source code, binaries, and documentation are available on http://github.com/pymc-learn/pymc-learn.","stat.ML, cs.LG",Daniel Emaasit,[]
,http://arxiv.org/abs/2310.10368v1,2023-10-16T13:05:47Z,,arXiv,cs.LG,http://arxiv.org/pdf/2310.10368v1.pdf,2310.10368v1,Machine learning in physics: a short guide,"Machine learning is a rapidly growing field with the potential to revolutionize many areas of science, including physics. This review provides a brief overview of machine learning in physics, covering the main concepts of supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning. We present some of the principal applications of machine learning in physics and discuss the associated challenges and perspectives.","cs.LG, cond-mat.stat-mech, physics.app-ph",Francisco A. Rodrigues,[]
,http://arxiv.org/abs/2005.09428v2,2020-05-15T10:20:35Z,,arXiv,cs.LG,http://arxiv.org/pdf/2005.09428v2.pdf,2005.09428v2,Quantum-Classical Machine learning by Hybrid Tensor Networks,"Tensor networks (TN) have found a wide use in machine learning, and in particular, TN and deep learning bear striking similarities. In this work, we propose the quantum-classical hybrid tensor networks (HTN) which combine tensor networks with classical neural networks in a uniform deep learning framework to overcome the limitations of regular tensor networks in machine learning. We first analyze the limitations of regular tensor networks in the applications of machine learning involving the representation power and architecture scalability. We conclude that in fact the regular tensor networks are not competent to be the basic building blocks of deep learning. Then, we discuss the performance of HTN which overcome all the deficiency of regular tensor networks for machine learning. In this sense, we are able to train HTN in the deep learning way which is the standard combination of algorithms such as Back Propagation and Stochastic Gradient Descent. We finally provide two applicable cases to show the potential applications of HTN, including quantum states classification and quantum-classical autoencoder. These cases also demonstrate the great potentiality to design various HTN in deep learning way.","cs.LG, quant-ph, stat.ML","Ding Liu, Jiaqi Yao, Zekun Yao, Quan Zhang",[]
,http://arxiv.org/abs/1706.05749v1,2017-06-19T00:16:24Z,,arXiv,stat.ML,http://arxiv.org/pdf/1706.05749v1.pdf,1706.05749v1,Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning,"This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.","stat.ML, cs.AI, cs.LG","Nick Erickson, Qi Zhao",[]
,http://arxiv.org/abs/1504.03874v1,2015-04-15T12:04:58Z,,arXiv,cs.AI,http://arxiv.org/pdf/1504.03874v1.pdf,1504.03874v1,Bridging belief function theory to modern machine learning,"Machine learning is a quickly evolving field which now looks really different from what it was 15 years ago, when classification and clustering were major issues. This document proposes several trends to explore the new questions of modern machine learning, with the strong afterthought that the belief function framework has a major role to play.","cs.AI, cs.LG",Thomas Burger,[]
,http://arxiv.org/abs/1606.02767v2,2016-06-08T21:46:20Z,,arXiv,cs.AI,http://arxiv.org/pdf/1606.02767v2.pdf,1606.02767v2,Theoretical Robopsychology: Samu Has Learned Turing Machines,"From the point of view of a programmer, the robopsychology is a synonym for the activity is done by developers to implement their machine learning applications. This robopsychological approach raises some fundamental theoretical questions of machine learning. Our discussion of these questions is constrained to Turing machines. Alan Turing had given an algorithm (aka the Turing Machine) to describe algorithms. If it has been applied to describe itself then this brings us to Turing's notion of the universal machine. In the present paper, we investigate algorithms to write algorithms. From a pedagogy point of view, this way of writing programs can be considered as a combination of learning by listening and learning by doing due to it is based on applying agent technology and machine learning. As the main result we introduce the problem of learning and then we show that it cannot easily be handled in reality therefore it is reasonable to use machine learning algorithm for learning Turing machines.","cs.AI, 68T05, I.2.6",Norbert Bátfai,[]
,http://arxiv.org/abs/1606.05386v1,2016-06-16T23:39:41Z,,arXiv,stat.ML,http://arxiv.org/pdf/1606.05386v1.pdf,1606.05386v1,Model-Agnostic Interpretability of Machine Learning,"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","stat.ML, cs.LG","Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",[]
,http://arxiv.org/abs/1607.02531v2,2016-07-08T21:07:54Z,,arXiv,stat.ML,http://arxiv.org/pdf/1607.02531v2.pdf,1607.02531v2,Proceedings of the 2016 ICML Workshop on Human Interpretability in   Machine Learning (WHI 2016),"This is the Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.   Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang, and Hanna Wallach.","stat.ML, cs.LG","Been Kim, Dmitry M. Malioutov, Kush R. Varshney",[]
,http://arxiv.org/abs/1703.10121v1,2017-03-29T16:29:04Z,,arXiv,cs.LG,http://arxiv.org/pdf/1703.10121v1.pdf,1703.10121v1,The Top 10 Topics in Machine Learning Revisited: A Quantitative   Meta-Study,"Which topics of machine learning are most commonly addressed in research? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers. In our study, we revisit this question from a quantitative perspective. Concretely, we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences. We then use machine learning in order to determine the top 10 topics in machine learning. We not only include models, but provide a holistic view across optimization, data, features, etc. This quantitative approach allows reducing the bias of surveys. It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are. This allows researchers to identify popular topics as well as new and rising topics for their research.","cs.LG, cs.AI, stat.ML","Patrick Glauner, Manxing Du, Victor Paraschiv, Andrey Boytsov, Isabel Lopez Andrade, Jorge Meira, Petko Valtchev, Radu State",[]
,http://arxiv.org/abs/1706.08519v1,2017-06-26T17:41:20Z,,arXiv,stat.ML,http://arxiv.org/pdf/1706.08519v1.pdf,1706.08519v1,On conditional parity as a notion of non-discrimination in machine   learning,"We identify conditional parity as a general notion of non-discrimination in machine learning. In fact, several recently proposed notions of non-discrimination, including a few counterfactual notions, are instances of conditional parity. We show that conditional parity is amenable to statistical analysis by studying randomization as a general mechanism for achieving conditional parity and a kernel-based test of conditional parity.","stat.ML, cs.CY, cs.LG","Ya'acov Ritov, Yuekai Sun, Ruofei Zhao",[]
,http://arxiv.org/abs/1708.02666v1,2017-08-08T22:21:11Z,,arXiv,stat.ML,http://arxiv.org/pdf/1708.02666v1.pdf,1708.02666v1,Proceedings of the 2017 ICML Workshop on Human Interpretability in   Machine Learning (WHI 2017),"This is the Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10, 2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.","stat.ML, cs.LG","Been Kim, Dmitry M. Malioutov, Kush R. Varshney, Adrian Weller",[]
,http://arxiv.org/abs/1807.01308v1,2018-07-03T17:49:14Z,,arXiv,stat.ML,http://arxiv.org/pdf/1807.01308v1.pdf,1807.01308v1,Proceedings of the 2018 ICML Workshop on Human Interpretability in   Machine Learning (WHI 2018),"This is the Proceedings of the 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14, 2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda Vi\'egas, and Martin Wattenberg.","stat.ML, cs.LG","Been Kim, Kush R. Varshney, Adrian Weller",[]
,http://arxiv.org/abs/1807.04162v3,2018-07-11T14:39:17Z,,arXiv,cs.LG,http://arxiv.org/pdf/1807.04162v3.pdf,1807.04162v3,TherML: Thermodynamics of Machine Learning,In this work we offer a framework for reasoning about a wide class of existing objectives in machine learning. We develop a formal correspondence between this work and thermodynamics and discuss its implications.,"cs.LG, cond-mat.stat-mech, stat.ML","Alexander A. Alemi, Ian Fischer",[]
,http://arxiv.org/abs/1807.05351v1,2018-07-14T08:07:31Z,,arXiv,cs.LG,http://arxiv.org/pdf/1807.05351v1.pdf,1807.05351v1,ML-Schema: Exposing the Semantics of Machine Learning with Schemas and   Ontologies,"The ML-Schema, proposed by the W3C Machine Learning Schema Community Group, is a top-level ontology that provides a set of classes, properties, and restrictions for representing and interchanging information on machine learning algorithms, datasets, and experiments. It can be easily extended and specialized and it is also mapped to other more domain-specific ontologies developed in the area of machine learning and data mining. In this paper we overview existing state-of-the-art machine learning interchange formats and present the first release of ML-Schema, a canonical format resulted of more than seven years of experience among different research institutions. We argue that exposing semantics of machine learning algorithms, models, and experiments through a canonical format may pave the way to better interpretability and to realistically achieve the full interoperability of experiments regardless of platform or adopted workflow solution.","cs.LG, cs.DB, cs.IR, stat.ML","Gustavo Correa Publio, Diego Esteves, Agnieszka Ławrynowicz, Panče Panov, Larisa Soldatova, Tommaso Soru, Joaquin Vanschoren, Hamid Zafar",[]
,http://arxiv.org/abs/1805.04272v2,2018-05-11T08:28:55Z,,arXiv,cs.LG,http://arxiv.org/pdf/1805.04272v2.pdf,1805.04272v2,An $O(N)$ Sorting Algorithm: Machine Learning Sort,"We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method, which shows a huge potential sorting big data. This sorting algorithm can be applied to parallel sorting and is suitable for GPU or TPU acceleration. Furthermore, we discuss the application of this algorithm to sparse hash table.","cs.LG, cs.DS, stat.ML","Hanqing Zhao, Yuehan Luo",[]
,http://arxiv.org/abs/1903.11726v1,2019-03-27T23:11:15Z,,arXiv,eess.IV,http://arxiv.org/pdf/1903.11726v1.pdf,10.1016/j.compbiomed.2019.02.017,"Radiological images and machine learning: trends, perspectives, and   prospects","The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently.","eess.IV, cs.LG","Zhenwei Zhang, Ervin Sejdic",[]
,http://arxiv.org/abs/1911.09052v1,2019-11-08T13:58:31Z,,arXiv,cs.GT,http://arxiv.org/pdf/1911.09052v1.pdf,1911.09052v1,Collaborative Machine Learning Markets with Data-Replication-Robust   Payments,"We study the problem of collaborative machine learning markets where multiple parties can achieve improved performance on their machine learning tasks by combining their training data. We discuss desired properties for these machine learning markets in terms of fair revenue distribution and potential threats, including data replication. We then instantiate a collaborative market for cases where parties share a common machine learning task and where parties' tasks are different. Our marketplace incentivizes parties to submit high quality training and true validation data. To this end, we introduce a novel payment division function that is robust-to-replication and customized output models that perform well only on requested machine learning tasks. In experiments, we validate the assumptions underlying our theoretical analysis and show that these are approximately satisfied for commonly used machine learning models.","cs.GT, cs.LG, stat.ML","Olga Ohrimenko, Shruti Tople, Sebastian Tschiatschek",[]
,http://arxiv.org/abs/1912.07323v1,2019-12-16T12:40:26Z,,arXiv,cs.SE,http://arxiv.org/pdf/1912.07323v1.pdf,1912.07323v1,Analysis of Software Engineering for Agile Machine Learning Projects,"The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.","cs.SE, cs.LG, D.2","Kushal Singla, Joy Bose, Chetan Naik",[]
,http://arxiv.org/abs/2103.00366v2,2021-02-28T01:10:09Z,,arXiv,q-fin.ST,http://arxiv.org/pdf/2103.00366v2.pdf,2103.00366v2,Confronting Machine Learning With Financial Research,"This study aims to examine the challenges and applications of machine learning for financial research. Machine learning algorithms have been developed for certain data environments which substantially differ from the one we encounter in finance. Not only do difficulties arise due to some of the idiosyncrasies of financial markets, there is a fundamental tension between the underlying paradigm of machine learning and the research philosophy in financial economics. Given the peculiar features of financial markets and the empirical framework within social science, various adjustments have to be made to the conventional machine learning methodology. We discuss some of the main challenges of machine learning in finance and examine how these could be accounted for. Despite some of the challenges, we argue that machine learning could be unified with financial research to become a robust complement to the econometrician's toolbox. Moreover, we discuss the various applications of machine learning in the research process such as estimation, empirical discovery, testing, causal inference and prediction.","q-fin.ST, cs.LG, econ.EM","Kristof Lommers, Ouns El Harzli, Jack Kim",[]
,http://arxiv.org/abs/1505.06614v1,2015-05-25T13:02:32Z,,arXiv,stat.ML,http://arxiv.org/pdf/1505.06614v1.pdf,1505.06614v1,Electre Tri-Machine Learning Approach to the Record Linkage Problem,"In this short paper, the Electre Tri-Machine Learning Method, generally used to solve ordinal classification problems, is proposed for solving the Record Linkage problem. Preliminary experimental results show that, using the Electre Tri method, high accuracy can be achieved and more than 99% of the matches and nonmatches were correctly identified by the procedure.","stat.ML, cs.LG","Renato De Leone, Valentina Minnetti",[]
,http://arxiv.org/abs/1804.11238v1,2018-03-27T15:10:31Z,,arXiv,cs.CR,http://arxiv.org/pdf/1804.11238v1.pdf,1804.11238v1,Privacy Preserving Machine Learning: Threats and Solutions,"For privacy concerns to be addressed adequately in current machine learning systems, the knowledge gap between the machine learning and privacy communities must be bridged. This article aims to provide an introduction to the intersection of both fields with special emphasis on the techniques used to protect the data.","cs.CR, cs.LG","Mohammad Al-Rubaie, J. Morris Chang",[]
,http://arxiv.org/abs/2101.12097v1,2021-01-28T16:34:04Z,,arXiv,cs.LG,http://arxiv.org/pdf/2101.12097v1.pdf,2101.12097v1,Adversarial Machine Learning Attacks on Condition-Based Maintenance   Capabilities,"Condition-based maintenance (CBM) strategies exploit machine learning models to assess the health status of systems based on the collected data from the physical environment, while machine learning models are vulnerable to adversarial attacks. A malicious adversary can manipulate the collected data to deceive the machine learning model and affect the CBM system's performance. Adversarial machine learning techniques introduced in the computer vision domain can be used to make stealthy attacks on CBM systems by adding perturbation to data to confuse trained models. The stealthy nature causes difficulty and delay in detection of the attacks. In this paper, adversarial machine learning in the domain of CBM is introduced. A case study shows how adversarial machine learning can be used to attack CBM capabilities. Adversarial samples are crafted using the Fast Gradient Sign method, and the performance of a CBM system under attack is investigated. The obtained results reveal that CBM systems are vulnerable to adversarial machine learning attacks and defense strategies need to be considered.","cs.LG, cs.AI, cs.CR",Hamidreza Habibollahi Najaf Abadi,[]
,http://arxiv.org/abs/2108.09664v1,2021-08-22T08:23:30Z,,arXiv,quant-ph,http://arxiv.org/pdf/2108.09664v1.pdf,10.1209/0295-5075/132/60004,New Trends in Quantum Machine Learning,"Here we will give a perspective on new possible interplays between Machine Learning and Quantum Physics, including also practical cases and applications. We will explore the ways in which machine learning could benefit from new quantum technologies and algorithms to find new ways to speed up their computations by breakthroughs in physical hardware, as well as to improve existing models or devise new learning schemes in the quantum domain. Moreover, there are lots of experiments in quantum physics that do generate incredible amounts of data and machine learning would be a great tool to analyze those and make predictions, or even control the experiment itself. On top of that, data visualization techniques and other schemes borrowed from machine learning can be of great use to theoreticians to have better intuition on the structure of complex manifolds or to make predictions on theoretical models. This new research field, named as Quantum Machine Learning, is very rapidly growing since it is expected to provide huge advantages over its classical counterpart and deeper investigations are timely needed since they can be already tested on the already commercially available quantum machines.","quant-ph, cond-mat.dis-nn, cs.LG, stat.ML","Lorenzo Buffoni, Filippo Caruso",[]
,http://arxiv.org/abs/1811.04871v1,2018-11-12T17:32:24Z,,arXiv,cs.LG,http://arxiv.org/pdf/1811.04871v1.pdf,1811.04871v1,Characterizing machine learning process: A maturity framework,"Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises. For example, existing machine learning processes cannot address how to define business use cases for an AI application, how to convert business requirements from offering managers into data requirements for data scientists, and how to continuously improve AI applications in term of accuracy and fairness, and how to customize general purpose machine learning models with industry, domain, and use case specific data to make them more accurate for specific situations etc. Making AI work for enterprises requires special considerations, tools, methods and processes. In this paper we present a maturity framework for machine learning model lifecycle management for enterprises. Our framework is a re-interpretation of the software Capability Maturity Model (CMM) for machine learning model development process. We present a set of best practices from our personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point.","cs.LG, cs.SE","Rama Akkiraju, Vibha Sinha, Anbang Xu, Jalal Mahmud, Pritam Gundecha, Zhe Liu, Xiaotong Liu, John Schumacher",[]
,http://arxiv.org/abs/1811.11669v1,2018-11-28T16:49:37Z,,arXiv,cs.LG,http://arxiv.org/pdf/1811.11669v1.pdf,1811.11669v1,Towards Identifying and Managing Sources of Uncertainty in AI and   Machine Learning Models - An Overview,Quantifying and managing uncertainties that occur when data-driven models such as those provided by AI and machine learning methods are applied is crucial. This whitepaper provides a brief motivation and first overview of the state of the art in identifying and quantifying sources of uncertainty for data-driven components as well as means for analyzing their impact.,"cs.LG, stat.ML, 68T01",Michael Kläs,[]
,http://arxiv.org/abs/1812.10398v2,2018-12-21T03:29:09Z,,arXiv,cs.CY,http://arxiv.org/pdf/1812.10398v2.pdf,1812.10398v2,Proceedings of NeurIPS 2018 Workshop on Machine Learning for the   Developing World: Achieving Sustainable Impact,"This is the Proceedings of NeurIPS 2018 Workshop on Machine Learning for the Developing World: Achieving Sustainable Impact, held in Montreal, Canada on December 8, 2018","cs.CY, cs.AI, cs.LG, stat.ML","Maria De-Arteaga, Amanda Coston, William Herlands",[]
,http://arxiv.org/abs/1904.01957v1,2019-04-02T09:37:44Z,,arXiv,cs.AI,http://arxiv.org/pdf/1904.01957v1.pdf,1904.01957v1,A Game of Dice: Machine Learning and the Question Concerning Art,"We review some practical and philosophical questions raised by the use of machine learning in creative practice. Beyond the obvious problems regarding plagiarism and authorship, we argue that the novelty in AI Art relies mostly on a narrow machine learning contribution : manifold approximation. Nevertheless, this contribution creates a radical shift in the way we have to consider this movement. Is this omnipotent tool a blessing or a curse for the artists?","cs.AI, cs.LG",Paul Todorov,[]
,http://arxiv.org/abs/2005.00478v3,2020-05-01T16:40:25Z,,arXiv,cs.LG,http://arxiv.org/pdf/2005.00478v3.pdf,2005.00478v3,DriveML: An R Package for Driverless Machine Learning,"In recent years, the concept of automated machine learning has become very popular. Automated Machine Learning (AutoML) mainly refers to the automated methods for model selection and hyper-parameter optimization of various algorithms such as random forests, gradient boosting, neural networks, etc. In this paper, we introduce a new package i.e. DriveML for automated machine learning. DriveML helps in implementing some of the pillars of an automated machine learning pipeline such as automated data preparation, feature engineering, model building and model explanation by running the function instead of writing lengthy R codes. The DriveML package is available in CRAN. We compare the DriveML package with other relevant packages in CRAN/Github and find that DriveML performs the best across different parameters. We also provide an illustration by applying the DriveML package with default configuration on a real world dataset. Overall, the main benefits of DriveML are in development time savings, reduce developer's errors, optimal tuning of machine learning models and reproducibility.","cs.LG, stat.ML","Sayan Putatunda, Dayananda Ubrangala, Kiran Rama, Ravi Kondapalli",[]
,http://arxiv.org/abs/2006.01387v2,2020-06-02T04:36:50Z,,arXiv,stat.ML,http://arxiv.org/pdf/2006.01387v2.pdf,2006.01387v2,A combinatorial conjecture from PAC-Bayesian machine learning,We present a proof of a combinatorial conjecture from the second author's Ph.D. thesis. The proof relies on binomial and multinomial sums identities. We also discuss the relevance of the conjecture in the context of PAC-Bayesian machine learning.,"stat.ML, cs.LG, math.CO","M. Younsi, A. Lacasse",[]
,http://arxiv.org/abs/2006.02619v1,2020-06-04T02:35:10Z,,arXiv,physics.comp-ph,http://arxiv.org/pdf/2006.02619v1.pdf,2006.02619v1,Integrating Machine Learning with Physics-Based Modeling,"Machine learning is poised as a very powerful tool that can drastically improve our ability to carry out scientific research. However, many issues need to be addressed before this becomes a reality. This article focuses on one particular issue of broad interest: How can we integrate machine learning with physics-based modeling to develop new interpretable and truly reliable physical models? After introducing the general guidelines, we discuss the two most important issues for developing machine learning-based physical models: Imposing physical constraints and obtaining optimal datasets. We also provide a simple and intuitive explanation for the fundamental reasons behind the success of modern machine learning, as well as an introduction to the concurrent machine learning framework needed for integrating machine learning with physics-based modeling. Molecular dynamics and moment closure of kinetic equations are used as examples to illustrate the main issues discussed. We end with a general discussion on where this integration will lead us to, and where the new frontier will be after machine learning is successfully integrated into scientific modeling.","physics.comp-ph, cs.LG, cs.NA, math.NA","Weinan E, Jiequn Han, Linfeng Zhang",[]
,http://arxiv.org/abs/2006.07237v1,2020-06-12T14:40:46Z,,arXiv,cs.LG,http://arxiv.org/pdf/2006.07237v1.pdf,2006.07237v1,Power Consumption Variation over Activation Functions,"The power that machine learning models consume when making predictions can be affected by a model's architecture. This paper presents various estimates of power consumption for a range of different activation functions, a core factor in neural network model architecture design. Substantial differences in hardware performance exist between activation functions. This difference informs how power consumption in machine learning models can be reduced.","cs.LG, cs.NE, stat.ML",Leon Derczynski,[]
,http://arxiv.org/abs/2006.12270v1,2020-06-22T14:05:31Z,,arXiv,quant-ph,http://arxiv.org/pdf/2006.12270v1.pdf,2006.12270v1,Classification with Quantum Machine Learning: A Survey,"Due to the superiority and noteworthy progress of Quantum Computing (QC) in a lot of applications such as cryptography, chemistry, Big data, machine learning, optimization, Internet of Things (IoT), Blockchain, communication, and many more. Fully towards to combine classical machine learning (ML) with Quantum Information Processing (QIP) to build a new field in the quantum world is called Quantum Machine Learning (QML) to solve and improve problems that displayed in classical machine learning (e.g. time and energy consumption, kernel estimation). The aim of this paper presents and summarizes a comprehensive survey of the state-of-the-art advances in Quantum Machine Learning (QML). Especially, recent QML classification works. Also, we cover about 30 publications that are published lately in Quantum Machine Learning (QML). we propose a classification scheme in the quantum world and discuss encoding methods for mapping classical data to quantum data. Then, we provide quantum subroutines and some methods of Quantum Computing (QC) in improving performance and speed up of classical Machine Learning (ML). And also some of QML applications in various fields, challenges, and future vision will be presented.","quant-ph, cs.LG","Zainab Abohashima, Mohamed Elhosen, Essam H. Houssein, Waleed M. Mohamed",[]
,http://arxiv.org/abs/2009.14596v1,2020-09-23T23:16:46Z,,arXiv,math.NA,http://arxiv.org/pdf/2009.14596v1.pdf,10.4208/cicp.oa-2020-0185,Machine Learning and Computational Mathematics,"Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of ""black box"" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, {can} impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.","math.NA, cs.LG, cs.NA, stat.ML, 68T07, 46E15, 26B35, 26B40",Weinan E,[]
,http://arxiv.org/abs/2011.04328v1,2020-11-09T10:50:50Z,,arXiv,cs.LG,http://arxiv.org/pdf/2011.04328v1.pdf,2011.04328v1,Risk Assessment for Machine Learning Models,"In this paper we propose a framework for assessing the risk associated with deploying a machine learning model in a specified environment. For that we carry over the risk definition from decision theory to machine learning. We develop and implement a method that allows to define deployment scenarios, test the machine learning model under the conditions specified in each scenario, and estimate the damage associated with the output of the machine learning model under test. Using the likelihood of each scenario together with the estimated damage we define \emph{key risk indicators} of a machine learning model.   The definition of scenarios and weighting by their likelihood allows for standardized risk assessment in machine learning throughout multiple domains of application. In particular, in our framework, the robustness of a machine learning model to random input corruptions, distributional shifts caused by a changing environment, and adversarial perturbations can be assessed.","cs.LG, cs.AI","Paul Schwerdtner, Florens Greßner, Nikhil Kapoor, Felix Assion, René Sass, Wiebke Günther, Fabian Hüger, Peter Schlicht",[]
,http://arxiv.org/abs/2201.12428v1,2022-01-28T21:33:31Z,,arXiv,cs.LG,http://arxiv.org/pdf/2201.12428v1.pdf,2201.12428v1,Systematic Training and Testing for Machine Learning Using Combinatorial   Interaction Testing,"This paper demonstrates the systematic use of combinatorial coverage for selecting and characterizing test and training sets for machine learning models. The presented work adapts combinatorial interaction testing, which has been successfully leveraged in identifying faults in software testing, to characterize data used in machine learning. The MNIST hand-written digits data is used to demonstrate that combinatorial coverage can be used to select test sets that stress machine learning model performance, to select training sets that lead to robust model performance, and to select data for fine-tuning models to new domains. Thus, the results posit combinatorial coverage as a holistic approach to training and testing for machine learning. In contrast to prior work which has focused on the use of coverage in regard to the internal of neural networks, this paper considers coverage over simple features derived from inputs and outputs. Thus, this paper addresses the case where the supplier of test and training sets for machine learning models does not have intellectual property rights to the models themselves. Finally, the paper addresses prior criticism of combinatorial coverage and provides a rebuttal which advocates the use of coverage metrics in machine learning applications.","cs.LG, cs.SE, stat.ML","Tyler Cody, Erin Lanus, Daniel D. Doyle, Laura Freeman",[]
,http://arxiv.org/abs/2205.00210v1,2022-04-30T08:47:10Z,,arXiv,cs.SE,http://arxiv.org/pdf/2205.00210v1.pdf,10.1609/aaai.v34i09.7084,Software Testing for Machine Learning,"Machine learning has become prevalent across a wide variety of applications. Unfortunately, machine learning has also shown to be susceptible to deception, leading to errors, and even fatal failures. This circumstance calls into question the widespread use of machine learning, especially in safety-critical applications, unless we are able to assure its correctness and trustworthiness properties. Software verification and testing are established technique for assuring such properties, for example by detecting errors. However, software testing challenges for machine learning are vast and profuse - yet critical to address. This summary talk discusses the current state-of-the-art of software testing for machine learning. More specifically, it discusses six key challenge areas for software testing of machine learning systems, examines current approaches to these challenges and highlights their limitations. The paper provides a research agenda with elaborated directions for making progress toward advancing the state-of-the-art on testing of machine learning.","cs.SE, cs.AI, cs.LG","Dusica Marijan, Arnaud Gotlieb",[]
,http://arxiv.org/abs/2205.09488v1,2022-05-02T02:42:16Z,,arXiv,cs.SE,http://arxiv.org/pdf/2205.09488v1.pdf,2205.09488v1,PSI Draft Specification,"This document presents the draft specification for delivering machine learning services over HTTP, developed as part of the Protocols and Structures for Inference project, which concluded in 2013. It presents the motivation for providing machine learning as a service, followed by a description of the essential and optional components of such a service.","cs.SE, cs.LG, cs.NI","Mark Reid, James Montgomery, Barry Drake, Avraham Ruderman",[]
,http://arxiv.org/abs/2207.05548v1,2022-07-12T14:17:58Z,,arXiv,cs.CR,http://arxiv.org/pdf/2207.05548v1.pdf,10.1109/msec.2022.3182356,Practical Attacks on Machine Learning: A Case Study on Adversarial   Windows Malware,"While machine learning is vulnerable to adversarial examples, it still lacks systematic procedures and tools for evaluating its security in different application contexts. In this article, we discuss how to develop automated and scalable security evaluations of machine learning using practical attacks, reporting a use case on Windows malware detection.","cs.CR, cs.LG","Luca Demetrio, Battista Biggio, Fabio Roli",[]
,http://arxiv.org/abs/2207.13596v2,2022-07-27T15:55:05Z,,arXiv,cs.LG,http://arxiv.org/pdf/2207.13596v2.pdf,2207.13596v2,Fairness and Randomness in Machine Learning: Statistical Independence   and Relativization,"Fair Machine Learning endeavors to prevent unfairness arising in the context of machine learning applications embedded in society. Despite the variety of definitions of fairness and proposed ""fair algorithms"", there remain unresolved conceptual problems regarding fairness. In this paper, we dissect the role of statistical independence in fairness and randomness notions regularly used in machine learning. Thereby, we are led to a suprising hypothesis: randomness and fairness can be considered equivalent concepts in machine learning.   In particular, we obtain a relativized notion of randomness expressed as statistical independence by appealing to Von Mises' century-old foundations for probability. This notion turns out to be ""orthogonal"" in an abstract sense to the commonly used i.i.d.-randomness. Using standard fairness notions in machine learning, which are defined via statistical independence, we then link the ex ante randomness assumptions about the data to the ex post requirements for fair predictions. This connection proves fruitful: we use it to argue that randomness and fairness are essentially relative and that both concepts should reflect their nature as modeling assumptions in machine learning.","cs.LG, cs.CY","Rabanus Derr, Robert C. Williamson",[]
,http://arxiv.org/abs/2208.10463v1,2022-08-22T17:33:59Z,,arXiv,cs.LG,http://arxiv.org/pdf/2208.10463v1.pdf,2208.10463v1,Survey of Machine Learning Techniques To Predict Heartbeat Arrhythmias,"Many works in biomedical computer science research use machine learning techniques to give accurate results. However, these techniques may not be feasible for real-time analysis of data pulled from live hospital feeds. In this project, different machine learning techniques are compared from various sources to find one that provides not only high accuracy but also low latency and memory overhead to be used in real-world health care systems.","cs.LG, eess.SP",Samuel Armstrong,[]
,http://arxiv.org/abs/2209.06529v1,2022-09-14T10:07:44Z,,arXiv,cs.LG,http://arxiv.org/pdf/2209.06529v1.pdf,10.1109/msec.2022.3178187,Data Privacy and Trustworthy Machine Learning,"The privacy risks of machine learning models is a major concern when training them on sensitive and personal data. We discuss the tradeoffs between data privacy and the remaining goals of trustworthy machine learning (notably, fairness, robustness, and explainability).","cs.LG, cs.CR","Martin Strobel, Reza Shokri",[]
,http://arxiv.org/abs/2212.13988v1,2022-12-12T02:40:51Z,,arXiv,cs.CR,http://arxiv.org/pdf/2212.13988v1.pdf,2212.13988v1,Machine Learning for Detecting Malware in PE Files,"The increasing number of sophisticated malware poses a major cybersecurity threat. Portable executable (PE) files are a common vector for such malware. In this work we review and evaluate machine learning-based PE malware detection techniques. Using a large benchmark dataset, we evaluate features of PE files using the most common machine learning techniques to detect malware.","cs.CR, cs.LG","Collin Connors, Dilip Sarkar",[]
,http://arxiv.org/abs/2303.13735v1,2023-03-24T01:39:56Z,,arXiv,cs.SE,http://arxiv.org/pdf/2303.13735v1.pdf,2303.13735v1,An investigation of licensing of datasets for machine learning based on   the GQM model,"Dataset licensing is currently an issue in the development of machine learning systems. And in the development of machine learning systems, the most widely used are publicly available datasets. However, since the images in the publicly available dataset are mainly obtained from the Internet, some images are not commercially available. Furthermore, developers of machine learning systems do not often care about the license of the dataset when training machine learning models with it. In summary, the licensing of datasets for machine learning systems is in a state of incompleteness in all aspects at this stage.   Our investigation of two collection datasets revealed that most of the current datasets lacked licenses, and the lack of licenses made it impossible to determine the commercial availability of the datasets. Therefore, we decided to take a more scientific and systematic approach to investigate the licensing of datasets and the licensing of machine learning systems that use the dataset to make it easier and more compliant for future developers of machine learning systems.","cs.SE, cs.CY, cs.LG","Junyu Chen, Norihiro Yoshida, Hiroaki Takada",[]
,http://arxiv.org/abs/2403.02467v1,2024-03-04T20:28:28Z,,arXiv,econ.EM,http://arxiv.org/pdf/2403.02467v1.pdf,2403.02467v1,Applied Causal Inference Powered by ML and AI,"An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.","econ.EM, cs.LG, stat.ME, stat.ML","Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, Vasilis Syrgkanis",[]
,http://arxiv.org/abs/2009.08497v1,2020-09-17T18:47:06Z,,arXiv,cs.LG,http://arxiv.org/pdf/2009.08497v1.pdf,2009.08497v1,The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons   from Infant Learning,"After a surge in popularity of supervised Deep Learning, the desire to reduce the dependence on curated, labelled data sets and to leverage the vast quantities of unlabelled data available recently triggered renewed interest in unsupervised learning algorithms. Despite a significantly improved performance due to approaches such as the identification of disentangled latent representations, contrastive learning, and clustering optimisations, the performance of unsupervised machine learning still falls short of its hypothesised potential. Machine learning has previously taken inspiration from neuroscience and cognitive science with great success. However, this has mostly been based on adult learners with access to labels and a vast amount of prior knowledge. In order to push unsupervised machine learning forward, we argue that developmental science of infant cognition might hold the key to unlocking the next generation of unsupervised learning approaches. Conceptually, human infant learning is the closest biological parallel to artificial unsupervised learning, as infants too must learn useful representations from unlabelled data. In contrast to machine learning, these new representations are learned rapidly and from relatively few examples. Moreover, infants learn robust representations that can be used flexibly and efficiently in a number of different tasks and contexts. We identify five crucial factors enabling infants' quality and speed of learning, assess the extent to which these have already been exploited in machine learning, and propose how further adoption of these factors can give rise to previously unseen performance levels in unsupervised learning.","cs.LG, cs.AI, cs.CV, cs.NE","Lorijn Zaadnoordijk, Tarek R. Besold, Rhodri Cusack",[]
,http://arxiv.org/abs/2004.11149v7,2020-04-17T03:11:08Z,,arXiv,cs.LG,http://arxiv.org/pdf/2004.11149v7.pdf,2004.11149v7,A Comprehensive Overview and Survey of Recent Advances in Meta-Learning,"This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly introduce meta-learning methodologies in the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.","cs.LG, stat.ML",Huimin Peng,[]
,http://arxiv.org/abs/1606.08531v1,2016-06-28T01:43:38Z,,arXiv,cs.AI,http://arxiv.org/pdf/1606.08531v1.pdf,1606.08531v1,A Learning Algorithm for Relational Logistic Regression: Preliminary   Results,"Relational logistic regression (RLR) is a representation of conditional probability in terms of weighted formulae for modelling multi-relational data. In this paper, we develop a learning algorithm for RLR models. Learning an RLR model from data consists of two steps: 1- learning the set of formulae to be used in the model (a.k.a. structure learning) and learning the weight of each formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt and Murphy's hierarchical assumption: first we learn a model with simple formulae, then more complex formulae are added iteratively only if all their sub-formulae have proven effective in previous learned models. For parameter learning, we convert the problem into a non-relational learning problem and use an off-the-shelf logistic regression learning algorithm from Weka, an open-source machine learning tool, to learn the weights. We also indicate how hidden features about the individuals can be incorporated into RLR to boost the learning performance. We compare our learning algorithm to other structure and parameter learning algorithms in the literature, and compare the performance of RLR models to standard logistic regression and RDN-Boost on a modified version of the MovieLens data-set.","cs.AI, cs.LG, stat.ML","Bahare Fatemi, Seyed Mehran Kazemi, David Poole",[]
,http://arxiv.org/abs/1705.10201v2,2017-05-29T14:07:33Z,,arXiv,cs.AI,http://arxiv.org/pdf/1705.10201v2.pdf,1705.10201v2,Machine Learned Learning Machines,"There are two common approaches for optimizing the performance of a machine: genetic algorithms and machine learning. A genetic algorithm is applied over many generations whereas machine learning works by applying feedback until the system meets a performance threshold. Though these are methods that typically operate separately, we combine evolutionary adaptation and machine learning into one approach. Our focus is on machines that can learn during their lifetime, but instead of equipping them with a machine learning algorithm we aim to let them evolve their ability to learn by themselves. We use evolvable networks of probabilistic and deterministic logic gates, known as Markov Brains, as our computational model organism. The ability of Markov Brains to learn is augmented by a novel adaptive component that can change its computational behavior based on feedback. We show that Markov Brains can indeed evolve to incorporate these feedback gates to improve their adaptability to variable environments. By combining these two methods, we now also implemented a computational model that can be used to study the evolution of learning.",cs.AI,"Leigh Sheneman, Arend Hintze",[]
,http://arxiv.org/abs/2004.12076v2,2020-04-25T07:45:20Z,,arXiv,quant-ph,http://arxiv.org/pdf/2004.12076v2.pdf,10.1088/2632-2153/ab9803,Quantum machine learning and quantum biomimetics: A perspective,"Quantum machine learning has emerged as an exciting and promising paradigm inside quantum technologies. It may permit, on the one hand, to carry out more efficient machine learning calculations by means of quantum devices, while, on the other hand, to employ machine learning techniques to better control quantum systems. Inside quantum machine learning, quantum reinforcement learning aims at developing ""intelligent"" quantum agents that may interact with the outer world and adapt to it, with the strategy of achieving some final goal. Another paradigm inside quantum machine learning is that of quantum autoencoders, which may allow one for employing fewer resources in a quantum device via a training process. Moreover, the field of quantum biomimetics aims at establishing analogies between biological and quantum systems, to look for previously inadvertent connections that may enable useful applications. Two recent examples are the concepts of quantum artificial life, as well as of quantum memristors. In this Perspective, we give an overview of these topics, describing the related research carried out by the scientific community.","quant-ph, cond-mat.mes-hall, cs.LG",Lucas Lamata,[]
,http://arxiv.org/abs/1912.05796v1,2019-12-12T06:52:12Z,,arXiv,cs.LG,http://arxiv.org/pdf/1912.05796v1.pdf,1912.05796v1,Automatic Layout Generation with Applications in Machine Learning Engine   Evaluation,"Machine learning-based lithography hotspot detection has been deeply studied recently, from varies feature extraction techniques to efficient learning models. It has been observed that such machine learning-based frameworks are providing satisfactory metal layer hotspot prediction results on known public metal layer benchmarks. In this work, we seek to evaluate how these machine learning-based hotspot detectors generalize to complicated patterns. We first introduce a automatic layout generation tool that can synthesize varies layout patterns given a set of design rules. The tool currently supports both metal layer and via layer generation. As a case study, we conduct hotspot detection on the generated via layer layouts with representative machine learning-based hotspot detectors, which shows that continuous study on model robustness and generality is necessary to prototype and integrate the learning engines in DFM flows. The source code of the layout generation tool will be available at https://github. com/phdyang007/layout-generation.","cs.LG, cs.AI, stat.ML","Haoyu Yang, Wen Chen, Piyush Pathak, Frank Gennari, Ya-Chieh Lai, Bei Yu",[]
,http://arxiv.org/abs/2306.16156v2,2023-06-28T12:37:23Z,,arXiv,cs.LG,http://arxiv.org/pdf/2306.16156v2.pdf,2306.16156v2,Recent Advances in Optimal Transport for Machine Learning,"Recently, Optimal Transport has been proposed as a probabilistic framework in Machine Learning for comparing and manipulating probability distributions. This is rooted in its rich history and theory, and has offered new solutions to different problems in machine learning, such as generative modeling and transfer learning. In this survey we explore contributions of Optimal Transport for Machine Learning over the period 2012 -- 2023, focusing on four sub-fields of Machine Learning: supervised, unsupervised, transfer and reinforcement learning. We further highlight the recent development in computational Optimal Transport and its extensions, such as partial, unbalanced, Gromov and Neural Optimal Transport, and its interplay with Machine Learning practice.","cs.LG, math.PR, stat.ML","Eduardo Fernandes Montesuma, Fred Ngolè Mboula, Antoine Souloumiac",[]
,http://arxiv.org/abs/1911.11374v1,2019-11-26T07:21:34Z,,arXiv,stat.ML,http://arxiv.org/pdf/1911.11374v1.pdf,1911.11374v1,Representation Learning: A Statistical Perspective,"Learning representations of data is an important problem in statistics and machine learning. While the origin of learning representations can be traced back to factor analysis and multidimensional scaling in statistics, it has become a central theme in deep learning with important applications in computer vision and computational neuroscience. In this article, we review recent advances in learning representations from a statistical perspective. In particular, we review the following two themes: (a) unsupervised learning of vector representations and (b) learning of both vector and matrix representations.","stat.ML, cs.LG","Jianwen Xie, Ruiqi Gao, Erik Nijkamp, Song-Chun Zhu, Ying Nian Wu",[]
,http://arxiv.org/abs/2002.03123v1,2020-02-08T09:04:21Z,,arXiv,cs.LG,http://arxiv.org/pdf/2002.03123v1.pdf,2002.03123v1,Towards a combinatorial characterization of bounded memory learning,"Combinatorial dimensions play an important role in the theory of machine learning. For example, VC dimension characterizes PAC learning, SQ dimension characterizes weak learning with statistical queries, and Littlestone dimension characterizes online learning.   In this paper we aim to develop combinatorial dimensions that characterize bounded memory learning. We propose a candidate solution for the case of realizable strong learning under a known distribution, based on the SQ dimension of neighboring distributions. We prove both upper and lower bounds for our candidate solution, that match in some regime of parameters. In this parameter regime there is an equivalence between bounded memory and SQ learning. We conjecture that our characterization holds in a much wider regime of parameters.","cs.LG, stat.ML","Alon Gonen, Shachar Lovett, Michal Moshkovitz",[]
,http://arxiv.org/abs/1908.09788v1,2019-08-26T16:42:33Z,,arXiv,cs.LG,http://arxiv.org/pdf/1908.09788v1.pdf,1908.09788v1,"An Introduction to Advanced Machine Learning : Meta Learning Algorithms,   Applications and Promises","In [1, 2], we have explored the theoretical aspects of feature extraction optimization processes for solving largescale problems and overcoming machine learning limitations. Majority of optimization algorithms that have been introduced in [1, 2] guarantee the optimal performance of supervised learning, given offline and discrete data, to deal with curse of dimensionality (CoD) problem. These algorithms, however, are not tailored for solving emerging learning problems. One of the important issues caused by online data is lack of sufficient samples per class. Further, traditional machine learning algorithms cannot achieve accurate training based on limited distributed data, as data has proliferated and dispersed significantly. Machine learning employs a strict model or embedded engine to train and predict which still fails to learn unseen classes and sufficiently use online data. In this chapter, we introduce these challenges elaborately. We further investigate Meta-Learning (MTL) algorithm, and their application and promises to solve the emerging problems by answering how autonomous agents can learn to learn?.","cs.LG, stat.ML","Farid Ghareh Mohammadi, M. Hadi Amini, Hamid R. Arabnia",[]
,http://arxiv.org/abs/2211.02263v1,2022-11-04T04:56:35Z,,arXiv,cs.LG,http://arxiv.org/pdf/2211.02263v1.pdf,2211.02263v1,Impact Learning: A Learning Method from Features Impact and Competition,"Machine learning is the study of computer algorithms that can automatically improve based on data and experience. Machine learning algorithms build a model from sample data, called training data, to make predictions or judgments without being explicitly programmed to do so. A variety of wellknown machine learning algorithms have been developed for use in the field of computer science to analyze data. This paper introduced a new machine learning algorithm called impact learning. Impact learning is a supervised learning algorithm that can be consolidated in both classification and regression problems. It can furthermore manifest its superiority in analyzing competitive data. This algorithm is remarkable for learning from the competitive situation and the competition comes from the effects of autonomous features. It is prepared by the impacts of the highlights from the intrinsic rate of natural increase (RNI). We, moreover, manifest the prevalence of the impact learning over the conventional machine learning algorithm.","cs.LG, cs.AI","Nusrat Jahan Prottasha, Saydul Akbar Murad, Abu Jafar Md Muzahid, Masud Rana, Md Kowsher, Apurba Adhikary, Sujit Biswas, Anupam Kumar Bairagi",[]
,http://arxiv.org/abs/2205.15104v1,2022-05-30T13:45:56Z,,arXiv,cs.LG,http://arxiv.org/pdf/2205.15104v1.pdf,2205.15104v1,FLICU: A Federated Learning Workflow for Intensive Care Unit Mortality   Prediction,"Although Machine Learning (ML) can be seen as a promising tool to improve clinical decision-making for supporting the improvement of medication plans, clinical procedures, diagnoses, or medication prescriptions, it remains limited by access to healthcare data. Healthcare data is sensitive, requiring strict privacy practices, and typically stored in data silos, making traditional machine learning challenging. Federated learning can counteract those limitations by training machine learning models over data silos while keeping the sensitive data localized. This study proposes a federated learning workflow for ICU mortality prediction. Hereby, the applicability of federated learning as an alternative to centralized machine learning and local machine learning is investigated by introducing federated learning to the binary classification problem of predicting ICU mortality. We extract multivariate time series data from the MIMIC-III database (lab values and vital signs), and benchmark the predictive performance of four deep sequential classifiers (FRNN, LSTM, GRU, and 1DCNN) varying the patient history window lengths (8h, 16h, 24h, 48h) and the number of FL clients (2, 4, 8). The experiments demonstrate that both centralized machine learning and federated learning are comparable in terms of AUPRC and F1-score. Furthermore, the federated approach shows superior performance over local machine learning. Thus, the federated approach can be seen as a valid and privacy-preserving alternative to centralized machine learning for classifying ICU mortality when sharing sensitive patient data between hospitals is not possible.",cs.LG,"Lena Mondrejevski, Ioanna Miliou, Annaclaudia Montanino, David Pitts, Jaakko Hollmén, Panagiotis Papapetrou",[]
,http://arxiv.org/abs/1506.01709v1,2015-06-04T19:58:56Z,,arXiv,stat.ML,http://arxiv.org/pdf/1506.01709v1.pdf,1506.01709v1,The Preference Learning Toolbox,"Preference learning (PL) is a core area of machine learning that handles datasets with ordinal relations. As the number of generated data of ordinal nature is increasing, the importance and role of the PL field becomes central within machine learning research and practice. This paper introduces an open source, scalable, efficient and accessible preference learning toolbox that supports the key phases of the data training process incorporating various popular data preprocessing, feature selection and preference learning methods.","stat.ML, cs.IR, cs.LG","Vincent E. Farrugia, Héctor P. Martínez, Georgios N. Yannakakis",[]
,http://arxiv.org/abs/1703.10444v1,2017-03-30T12:46:46Z,,arXiv,cs.LG,http://arxiv.org/pdf/1703.10444v1.pdf,1703.10444v1,On Fundamental Limits of Robust Learning,"We consider the problems of robust PAC learning from distributed and streaming data, which may contain malicious errors and outliers, and analyze their fundamental complexity questions. In particular, we establish lower bounds on the communication complexity for distributed robust learning performed on multiple machines, and on the space complexity for robust learning from streaming data on a single machine. These results demonstrate that gaining robustness of learning algorithms is usually at the expense of increased complexities. As far as we know, this work gives the first complexity results for distributed and online robust PAC learning.","cs.LG, stat.ML",Jiashi Feng,[]
,http://arxiv.org/abs/2004.13598v1,2020-04-28T15:35:20Z,,arXiv,cs.LG,http://arxiv.org/pdf/2004.13598v1.pdf,2004.13598v1,Private Dataset Generation Using Privacy Preserving Collaborative   Learning,"With increasing usage of deep learning algorithms in many application, new research questions related to privacy and adversarial attacks are emerging. However, the deep learning algorithm improvement needs more and more data to be shared within research community. Methodologies like federated learning, differential privacy, additive secret sharing provides a way to train machine learning models on edge without moving the data from the edge. However, it is very computationally intensive and prone to adversarial attacks. Therefore, this work introduces a privacy preserving FedCollabNN framework for training machine learning models at edge, which is computationally efficient and robust against adversarial attacks. The simulation results using MNIST dataset indicates the effectiveness of the framework.","cs.LG, cs.CR, stat.ML",Amit Chaulwar,[]
,http://arxiv.org/abs/1803.08118v3,2018-03-21T20:30:34Z,,arXiv,stat.ML,http://arxiv.org/pdf/1803.08118v3.pdf,1803.08118v3,Seglearn: A Python Package for Learning Sequences and Time Series,"Seglearn is an open-source python package for machine learning time series or sequences using a sliding window segmentation approach. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. Seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage.","stat.ML, cs.LG, I.2.5","David M. Burns, Cari M. Whyne",[]
,http://arxiv.org/abs/2009.12999v1,2020-09-28T01:09:23Z,,arXiv,cs.LG,http://arxiv.org/pdf/2009.12999v1.pdf,2009.12999v1,Loosely Coupled Federated Learning Over Generative Models,"Federated learning (FL) was proposed to achieve collaborative machine learning among various clients without uploading private data. However, due to model aggregation strategies, existing frameworks require strict model homogeneity, limiting the application in more complicated scenarios. Besides, the communication cost of FL's model and gradient transmission is extremely high. This paper proposes Loosely Coupled Federated Learning (LC-FL), a framework using generative models as transmission media to achieve low communication cost and heterogeneous federated learning. LC-FL can be applied on scenarios where clients possess different kinds of machine learning models. Experiments on real-world datasets covering different multiparty scenarios demonstrate the effectiveness of our proposal.","cs.LG, cs.DC, stat.ML","Shaoming Song, Yunfeng Shao, Jian Li",[]
,http://arxiv.org/abs/2012.00152v1,2020-11-30T23:02:47Z,,arXiv,cs.LG,http://arxiv.org/pdf/2012.00152v1.pdf,2012.00152v1,Every Model Learned by Gradient Descent Is Approximately a Kernel   Machine,"Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.","cs.LG, cs.NE, stat.ML, I.2.6; I.5.1",Pedro Domingos,[]
,http://arxiv.org/abs/2012.15505v1,2020-12-31T08:49:43Z,,arXiv,cs.LG,http://arxiv.org/pdf/2012.15505v1.pdf,2012.15505v1,Flexible model composition in machine learning and its implementation in   MLJ,"A graph-based protocol called `learning networks' which combine assorted machine learning models into meta-models is described. Learning networks are shown to overcome several limitations of model composition as implemented in the dominant machine learning platforms. After illustrating the protocol in simple examples, a concise syntax for specifying a learning network, implemented in the MLJ framework, is presented. Using the syntax, it is shown that learning networks are are sufficiently flexible to include Wolpert's model stacking, with out-of-sample predictions for the base learners.","cs.LG, I.2.6","Anthony D. Blaom, Sebastian J. Vollmer",[]
,http://arxiv.org/abs/2208.04707v1,2022-07-17T12:51:52Z,,arXiv,q-bio.NC,http://arxiv.org/pdf/2208.04707v1.pdf,2208.04707v1,Context sequence theory: a common explanation for multiple types of   learning,"Although principles of neuroscience like reinforcement learning, visual perception and attention have been applied in machine learning models, there is a huge gap between machine learning and mammalian learning. Based on the advances in neuroscience, we propose the context sequence theory to give a common explanation for multiple types of learning in mammals and hope that can provide a new insight into the construct of machine learning models.","q-bio.NC, cs.AI, cs.LG","Yu Mingcan, Wang Junying",[]
,http://arxiv.org/abs/2303.03181v1,2023-03-06T14:48:30Z,,arXiv,cs.LG,http://arxiv.org/pdf/2303.03181v1.pdf,2303.03181v1,MetaPhysiCa: OOD Robustness in Physics-informed Machine Learning,"A fundamental challenge in physics-informed machine learning (PIML) is the design of robust PIML methods for out-of-distribution (OOD) forecasting tasks. These OOD tasks require learning-to-learn from observations of the same (ODE) dynamical system with different unknown ODE parameters, and demand accurate forecasts even under out-of-support initial conditions and out-of-support ODE parameters. In this work we propose a solution for such tasks, which we define as a meta-learning procedure for causal structure discovery (including invariant risk minimization). Using three different OOD tasks, we empirically observe that the proposed approach significantly outperforms existing state-of-the-art PIML and deep learning methods.","cs.LG, stat.ML","S Chandra Mouli, Muhammad Ashraful Alam, Bruno Ribeiro",[]
,http://arxiv.org/abs/2104.02466v2,2021-04-06T12:48:17Z,,arXiv,cs.PL,http://arxiv.org/pdf/2104.02466v2.pdf,2104.02466v2,A Review of Formal Methods applied to Machine Learning,"We review state-of-the-art formal methods applied to the emerging field of the verification of machine learning systems. Formal methods can provide rigorous correctness guarantees on hardware and software systems. Thanks to the availability of mature tools, their use is well established in the industry, and in particular to check safety-critical applications as they undergo a stringent certification process. As machine learning is becoming more popular, machine-learned components are now considered for inclusion in critical systems. This raises the question of their safety and their verification. Yet, established formal methods are limited to classic, i.e. non machine-learned software. Applying formal methods to verify systems that include machine learning has only been considered recently and poses novel challenges in soundness, precision, and scalability.   We first recall established formal methods and their current use in an exemplar safety-critical field, avionic software, with a focus on abstract interpretation based techniques as they provide a high level of scalability. This provides a golden standard and sets high expectations for machine learning verification. We then provide a comprehensive and detailed review of the formal methods developed so far for machine learning, highlighting their strengths and limitations. The large majority of them verify trained neural networks and employ either SMT, optimization, or abstract interpretation techniques. We also discuss methods for support vector machines and decision tree ensembles, as well as methods targeting training and data preparation, which are critical but often neglected aspects of machine learning. Finally, we offer perspectives for future research directions towards the formal verification of machine learning systems.","cs.PL, cs.LG, cs.LO","Caterina Urban, Antoine Miné",[]
,http://arxiv.org/abs/1606.01487v1,2016-06-05T09:59:32Z,,arXiv,stat.ML,http://arxiv.org/pdf/1606.01487v1.pdf,1606.01487v1,Bounds for Vector-Valued Function Estimation,"We present a framework to derive risk bounds for vector-valued learning with a broad class of feature maps and loss functions. Multi-task learning and one-vs-all multi-category learning are treated as examples. We discuss in detail vector-valued functions with one hidden layer, and demonstrate that the conditions under which shared representations are beneficial for multi- task learning are equally applicable to multi-category learning.","stat.ML, cs.LG","Andreas Maurer, Massimiliano Pontil",[]
,http://arxiv.org/abs/2007.09982v1,2020-07-20T10:10:13Z,,arXiv,cs.LG,http://arxiv.org/pdf/2007.09982v1.pdf,2007.09982v1,MKLpy: a python-based framework for Multiple Kernel Learning,"Multiple Kernel Learning is a recent and powerful paradigm to learn the kernel function from data. In this paper, we introduce MKLpy, a python-based framework for Multiple Kernel Learning. The library provides Multiple Kernel Learning algorithms for classification tasks, mechanisms to compute kernel functions for different data types, and evaluation strategies. The library is meant to maximize the usability and to simplify the development of novel solutions.","cs.LG, stat.ML","Ivano Lauriola, Fabio Aiolli",[]
,http://arxiv.org/abs/1811.06622v1,2018-11-15T23:13:26Z,,arXiv,cs.LG,http://arxiv.org/pdf/1811.06622v1.pdf,1811.06622v1,Concept-Oriented Deep Learning: Generative Concept Representations,"Generative concept representations have three major advantages over discriminative ones: they can represent uncertainty, they support integration of learning and reasoning, and they are good for unsupervised and semi-supervised learning. We discuss probabilistic and generative deep learning, which generative concept representations are based on, and the use of variational autoencoders and generative adversarial networks for learning generative concept representations, particularly for concepts whose data are sequences, structured data or graphs.","cs.LG, cs.AI, stat.ML",Daniel T. Chang,[]
,http://arxiv.org/abs/2010.07744v1,2020-10-15T13:45:02Z,,arXiv,stat.ML,http://arxiv.org/pdf/2010.07744v1.pdf,2010.07744v1,A Theory of Hyperbolic Prototype Learning,"We introduce Hyperbolic Prototype Learning, a type of supervised learning, where class labels are represented by ideal points (points at infinity) in hyperbolic space. Learning is achieved by minimizing the 'penalized Busemann loss', a new loss function based on the Busemann function of hyperbolic geometry. We discuss several theoretical features of this setup. In particular, Hyperbolic Prototype Learning becomes equivalent to logistic regression in the one-dimensional case.","stat.ML, cs.LG, 68T07, 62J02, G.3; I.5",Martin Keller-Ressel,[]
,http://arxiv.org/abs/2112.12181v2,2021-12-22T19:16:24Z,,arXiv,cs.LG,http://arxiv.org/pdf/2112.12181v2.pdf,2112.12181v2,Simple and near-optimal algorithms for hidden stratification and   multi-group learning,"Multi-group agnostic learning is a formal learning criterion that is concerned with the conditional risks of predictors within subgroups of a population. The criterion addresses recent practical concerns such as subgroup fairness and hidden stratification. This paper studies the structure of solutions to the multi-group learning problem, and provides simple and near-optimal algorithms for the learning problem.","cs.LG, stat.ML","Christopher Tosh, Daniel Hsu",[]
,http://arxiv.org/abs/2005.13299v1,2020-05-27T11:56:56Z,,arXiv,cs.SE,http://arxiv.org/pdf/2005.13299v1.pdf,10.1109/access.2021.3119746,Machine Learning for Software Engineering: A Systematic Mapping,"Context: The software development industry is rapidly adopting machine learning for transitioning modern day software systems towards highly intelligent and self-learning systems. However, the full potential of machine learning for improving the software engineering life cycle itself is yet to be discovered, i.e., up to what extent machine learning can help reducing the effort/complexity of software engineering and improving the quality of resulting software systems. To date, no comprehensive study exists that explores the current state-of-the-art on the adoption of machine learning across software engineering life cycle stages. Objective: This article addresses the aforementioned problem and aims to present a state-of-the-art on the growing number of uses of machine learning in software engineering. Method: We conduct a systematic mapping study on applications of machine learning to software engineering following the standard guidelines and principles of empirical software engineering. Results: This study introduces a machine learning for software engineering (MLSE) taxonomy classifying the state-of-the-art machine learning techniques according to their applicability to various software engineering life cycle stages. Overall, 227 articles were rigorously selected and analyzed as a result of this study. Conclusion: From the selected articles, we explore a variety of aspects that should be helpful to academics and practitioners alike in understanding the potential of adopting machine learning techniques during software engineering projects.","cs.SE, cs.LG","Saad Shafiq, Atif Mashkoor, Christoph Mayr-Dorn, Alexander Egyed",[]
,http://arxiv.org/abs/2204.13291v3,2022-04-28T05:07:26Z,,arXiv,cs.LG,http://arxiv.org/pdf/2204.13291v3.pdf,2204.13291v3,Decision Models for Selecting Federated Learning Architecture Patterns,"Federated machine learning is growing fast in academia and industries as a solution to solve data hungriness and privacy issues in machine learning. Being a widely distributed system, federated machine learning requires various system design thinking. To better design a federated machine learning system, researchers have introduced multiple patterns and tactics that cover various system design aspects. However, the multitude of patterns leaves the designers confused about when and which pattern to adopt. In this paper, we present a set of decision models for the selection of patterns for federated machine learning architecture design based on a systematic literature review on federated machine learning, to assist designers and architects who have limited knowledge of federated machine learning. Each decision model maps functional and non-functional requirements of federated machine learning systems to a set of patterns. We also clarify the drawbacks of the patterns. We evaluated the decision models by mapping the decision patterns to concrete federated machine learning architectures by big tech firms to assess the models' correctness and usefulness. The evaluation results indicate that the proposed decision models are able to bring structure to the federated machine learning architecture design process and help explicitly articulate the design rationale.","cs.LG, cs.SE","Sin Kit Lo, Qinghua Lu, Hye-Young Paik, Liming Zhu",[]
,http://arxiv.org/abs/1611.00379v1,2016-11-01T20:35:46Z,,arXiv,cs.HC,http://arxiv.org/pdf/1611.00379v1.pdf,1611.00379v1,The Machine Learning Algorithm as Creative Musical Tool,"Machine learning is the capacity of a computational system to learn structures from datasets in order to make predictions on newly seen data. Such an approach offers a significant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system's capacity in unexpected ways. In this chapter we draw on music, machine learning, and human-computer interaction to elucidate an understanding of machine learning algorithms as creative tools for music and the sonic arts. We motivate a new understanding of learning algorithms as human-computer interfaces. We show that, like other interfaces, learning algorithms can be characterised by the ways their affordances intersect with goals of human users. We also argue that the nature of interaction between users and algorithms impacts the usability and usefulness of those algorithms in profound ways. This human-centred view of machine learning motivates our concluding discussion of what it means to employ machine learning as a creative tool.","cs.HC, cs.LG","Rebecca Fiebrink, Baptiste Caramiaux",[]
,http://arxiv.org/abs/1901.03678v1,2019-01-11T18:06:05Z,,arXiv,cs.LG,http://arxiv.org/pdf/1901.03678v1.pdf,1901.03678v1,Machine Learning Automation Toolbox (MLaut),"In this paper we present MLaut (Machine Learning AUtomation Toolbox) for the python data science ecosystem. MLaut automates large-scale evaluation and benchmarking of machine learning algorithms on a large number of datasets. MLaut provides a high-level workflow interface to machine algorithm algorithms, implements a local back-end to a database of dataset collections, trained algorithms, and experimental results, and provides easy-to-use interfaces to the scikit-learn and keras modelling libraries. Experiments are easy to set up with default settings in a few lines of code, while remaining fully customizable to the level of hyper-parameter tuning, pipeline composition, or deep learning architecture.   As a principal test case for MLaut, we conducted a large-scale supervised classification study in order to benchmark the performance of a number of machine learning algorithms - to our knowledge also the first larger-scale study on standard supervised learning data sets to include deep learning algorithms. While corroborating a number of previous findings in literature, we found (within the limitations of our study) that deep neural networks do not perform well on basic supervised learning, i.e., outside the more specialized, image-, audio-, or text-based tasks.","cs.LG, cs.AI, stat.ML","Viktor Kazakov, Franz J. Király",[]
,http://arxiv.org/abs/2006.05604v1,2020-06-10T01:47:34Z,,arXiv,cs.LG,http://arxiv.org/pdf/2006.05604v1.pdf,2006.05604v1,Machine Learning and Control Theory,"We survey in this article the connections between Machine Learning and Control Theory. Control Theory provide useful concepts and tools for Machine Learning. Conversely Machine Learning can be used to solve large control problems. In the first part of the paper, we develop the connections between reinforcement learning and Markov Decision Processes, which are discrete time control problems. In the second part, we review the concept of supervised learning and the relation with static optimization. Deep learning which extends supervised learning, can be viewed as a control problem. In the third part, we present the links between stochastic gradient descent and mean-field theory. Conversely, in the fourth and fifth parts, we review machine learning approaches to stochastic control problems, and focus on the deterministic case, to explain, more easily, the numerical algorithms.","cs.LG, math.OC, stat.ML","Alain Bensoussan, Yiqun Li, Dinh Phan Cao Nguyen, Minh-Binh Tran, Sheung Chi Phillip Yam, Xiang Zhou",[]
,http://arxiv.org/abs/2011.08450v2,2020-11-17T06:12:23Z,,arXiv,cs.LG,http://arxiv.org/pdf/2011.08450v2.pdf,2011.08450v2,A Quantitative Perspective on Values of Domain Knowledge for Machine   Learning,"With the exploding popularity of machine learning, domain knowledge in various forms has been playing a crucial role in improving the learning performance, especially when training data is limited. Nonetheless, there is little understanding of to what extent domain knowledge can affect a machine learning task from a quantitative perspective. To increase the transparency and rigorously explain the role of domain knowledge in machine learning, we study the problem of quantifying the values of domain knowledge in terms of its contribution to the learning performance in the context of informed machine learning. We propose a quantification method based on Shapley value that fairly attributes the overall learning performance improvement to different domain knowledge. We also present Monte-Carlo sampling to approximate the fair value of domain knowledge with a polynomial time complexity. We run experiments of injecting symbolic domain knowledge into semi-supervised learning tasks on both MNIST and CIFAR10 datasets, providing quantitative values of different symbolic knowledge and rigorously explaining how it affects the machine learning performance in terms of test accuracy.",cs.LG,"Jianyi Yang, Shaolei Ren",[]
,http://arxiv.org/abs/2103.07802v1,2021-03-13T22:01:47Z,,arXiv,cs.LG,http://arxiv.org/pdf/2103.07802v1.pdf,2103.07802v1,Hybrid computer approach to train a machine learning system,"This book chapter describes a novel approach to training machine learning systems by means of a hybrid computer setup i.e. a digital computer tightly coupled with an analog computer. As an example a reinforcement learning system is trained to balance an inverted pendulum which is simulated on an analog computer, thus demonstrating a solution to the major challenge of adequately simulating the environment for reinforcement learning.","cs.LG, cs.AI, cs.ET, 68T05, I.2.6; B.m","Mirko Holzer, Bernd Ulmann",[]
,http://arxiv.org/abs/1803.02388v1,2018-03-06T19:16:08Z,,arXiv,cs.LG,http://arxiv.org/pdf/1803.02388v1.pdf,1803.02388v1,Learning SMaLL Predictors,"We present a new machine learning technique for training small resource-constrained predictors. Our algorithm, the Sparse Multiprototype Linear Learner (SMaLL), is inspired by the classic machine learning problem of learning $k$-DNF Boolean formulae. We present a formal derivation of our algorithm and demonstrate the benefits of our approach with a detailed empirical study.",cs.LG,"Vikas K. Garg, Ofer Dekel, Lin Xiao",[]
,http://arxiv.org/abs/1901.05353v3,2019-01-16T15:47:51Z,,arXiv,stat.ML,http://arxiv.org/pdf/1901.05353v3.pdf,1901.05353v3,A Primer on PAC-Bayesian Learning,"Generalised Bayesian learning algorithms are increasingly popular in machine learning, due to their PAC generalisation properties and flexibility. The present paper aims at providing a self-contained survey on the resulting PAC-Bayes framework and some of its main theoretical and algorithmic developments.","stat.ML, cs.LG",Benjamin Guedj,[]
,http://arxiv.org/abs/2009.06093v1,2020-09-13T21:47:36Z,,arXiv,quant-ph,http://arxiv.org/pdf/2009.06093v1.pdf,2009.06093v1,Simultaneous Quantum Machine Learning Training and Architecture   Discovery,"With the onset of gated quantum machine learning, the architecture for such a system is an open question. Many architectures are created either ad hoc or are directly analogous from known classical architectures. Presented here is a novel algorithm which learns a gated quantum machine learning architecture while simultaneously learning its parameters. This proof of concept and some of its variations are explored and discussed.",quant-ph,Dominic Pasquali,[]
,http://arxiv.org/abs/2310.03751v1,2023-09-22T00:01:02Z,,arXiv,eess.SP,http://arxiv.org/pdf/2310.03751v1.pdf,10.1016/j.rinam.2023.100409,A Simple Illustration of Interleaved Learning using Kalman Filter for   Linear Least Squares,"Interleaved learning in machine learning algorithms is a biologically inspired training method with promising results. In this short note, we illustrate the interleaving mechanism via a simple statistical and optimization framework based on Kalman Filter for Linear Least Squares.","eess.SP, cs.LG, q-bio.NC, stat.AP, stat.ML","Majnu John, Yihren Wu",[]
,http://arxiv.org/abs/1103.3095v1,2011-03-16T04:54:58Z,,arXiv,cs.LG,http://arxiv.org/pdf/1103.3095v1.pdf,1103.3095v1,A note on active learning for smooth problems,"We show that the disagreement coefficient of certain smooth hypothesis classes is $O(m)$, where $m$ is the dimension of the hypothesis space, thereby answering a question posed in \cite{friedman09}.","cs.LG, stat.ML, 68Q32: Computational learning theory",Satyaki Mahalanabis,[]
,http://arxiv.org/abs/1204.2477v1,2012-04-11T15:35:43Z,,arXiv,stat.ME,http://arxiv.org/pdf/1204.2477v1.pdf,1204.2477v1,A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov   Models,"A simple linear algebraic explanation of the algorithm in ""A Spectral Algorithm for Learning Hidden Markov Models"" (COLT 2009). Most of the content is in Figure 2; the text just makes everything precise in four nearly-trivial claims.","stat.ME, cs.LG, stat.ML",Matthew James Johnson,[]
,http://arxiv.org/abs/1212.3900v2,2012-12-17T06:49:14Z,,arXiv,stat.ML,http://arxiv.org/pdf/1212.3900v2.pdf,1212.3900v2,A Tutorial on Probabilistic Latent Semantic Analysis,"In this tutorial, I will discuss the details about how Probabilistic Latent Semantic Analysis (PLSA) is formalized and how different learning algorithms are proposed to learn the model.","stat.ML, cs.LG",Liangjie Hong,[]
,http://arxiv.org/abs/1408.6618v1,2014-08-28T03:29:06Z,,arXiv,cs.LG,http://arxiv.org/pdf/1408.6618v1.pdf,1408.6618v1,Falsifiable implies Learnable,The paper demonstrates that falsifiability is fundamental to learning. We prove the following theorem for statistical learning and sequential prediction: If a theory is falsifiable then it is learnable -- i.e. admits a strategy that predicts optimally. An analogous result is shown for universal induction.,"cs.LG, math.ST, stat.ML, stat.TH",David Balduzzi,[]
,http://arxiv.org/abs/1502.02704v1,2015-02-09T22:05:25Z,,arXiv,cs.LG,http://arxiv.org/pdf/1502.02704v1.pdf,1502.02704v1,Learning Reductions that Really Work,"We provide a summary of the mathematical and computational techniques that have enabled learning reductions to effectively address a wide class of problems, and show that this approach to solving machine learning problems can be broadly useful.",cs.LG,"Alina Beygelzimer, Hal Daumé III, John Langford, Paul Mineiro",[]
,http://arxiv.org/abs/1809.07904v2,2018-09-21T01:02:48Z,,arXiv,cs.LG,http://arxiv.org/pdf/1809.07904v2.pdf,1809.07904v2,Automatic Rule Learning for Autonomous Driving Using Semantic Memory,This paper presents a novel approach for automatic rule learning applicable to an autonomous driving system using real driving data.,"cs.LG, stat.ML","Dmitriy Korchev, Aruna Jammalamadaka, Rajan Bhattacharyya",[]
,http://arxiv.org/abs/1803.06586v1,2018-03-17T23:39:57Z,,arXiv,cs.LG,http://arxiv.org/pdf/1803.06586v1.pdf,1803.06586v1,Structural query-by-committee,"In this work, we describe a framework that unifies many different interactive learning tasks. We present a generalization of the {\it query-by-committee} active learning algorithm for this setting, and we study its consistency and rate of convergence, both theoretically and empirically, with and without noise.","cs.LG, stat.ML","Christopher Tosh, Sanjoy Dasgupta",[]
,http://arxiv.org/abs/2012.03130v1,2020-12-05T22:10:25Z,,arXiv,stat.ML,http://arxiv.org/pdf/2012.03130v1.pdf,2012.03130v1,Rejoinder: New Objectives for Policy Learning,"I provide a rejoinder for discussion of ""More Efficient Policy Learning via Optimal Retargeting"" to appear in the Journal of the American Statistical Association with discussion by Oliver Dukes and Stijn Vansteelandt; Sijia Li, Xiudi Li, and Alex Luedtkeand; and Muxuan Liang and Yingqi Zhao.","stat.ML, cs.LG, math.OC",Nathan Kallus,[]
,http://arxiv.org/abs/1805.07938v1,2018-05-21T08:15:09Z,,arXiv,stat.ML,http://arxiv.org/pdf/1805.07938v1.pdf,1805.07938v1,Transductive Boltzmann Machines,"We present transductive Boltzmann machines (TBMs), which firstly achieve transductive learning of the Gibbs distribution. While exact learning of the Gibbs distribution is impossible by the family of existing Boltzmann machines due to combinatorial explosion of the sample space, TBMs overcome the problem by adaptively constructing the minimum required sample space from data to avoid unnecessary generalization. We theoretically provide bias-variance decomposition of the KL divergence in TBMs to analyze its learnability, and empirically demonstrate that TBMs are superior to the fully visible Boltzmann machines and popularly used restricted Boltzmann machines in terms of efficiency and effectiveness.","stat.ML, cs.LG","Mahito Sugiyama, Koji Tsuda, Hiroyuki Nakahara",[]
,http://arxiv.org/abs/1610.06072v1,2016-10-19T15:46:30Z,,arXiv,cs.LG,http://arxiv.org/pdf/1610.06072v1.pdf,1610.06072v1,Learning to Learn Neural Networks,"Meta-learning consists in learning learning algorithms. We use a Long Short Term Memory (LSTM) based network to learn to compute on-line updates of the parameters of another neural network. These parameters are stored in the cell state of the LSTM. Our framework allows to compare learned algorithms to hand-made algorithms within the traditional train and test methodology. In an experiment, we learn a learning algorithm for a one-hidden layer Multi-Layer Perceptron (MLP) on non-linearly separable datasets. The learned algorithm is able to update parameters of both layers and generalise well on similar datasets.","cs.LG, stat.ML",Tom Bosc,[]
,http://arxiv.org/abs/2008.01171v1,2020-07-31T10:06:02Z,,arXiv,cs.LG,http://arxiv.org/pdf/2008.01171v1.pdf,2008.01171v1,Deep Reinforcement Learning using Cyclical Learning Rates,"Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning of hyperparameters to successfully resolve problems. One of the most influential parameters in optimization procedures based on stochastic gradient descent (SGD) is the learning rate. We investigate cyclical learning and propose a method for defining a general cyclical learning rate for various DRL problems. In this paper we present a method for cyclical learning applied to complex DRL problems. Our experiments show that, utilizing cyclical learning achieves similar or even better results than highly tuned fixed learning rates. This paper presents the first application of cyclical learning rates in DRL settings and is a step towards overcoming manual hyperparameter tuning.","cs.LG, stat.ML","Ralf Gulde, Marc Tuscher, Akos Csiszar, Oliver Riedel, Alexander Verl",[]
,http://arxiv.org/abs/2008.07739v1,2020-08-18T04:45:59Z,,arXiv,cs.LG,http://arxiv.org/pdf/2008.07739v1.pdf,2008.07739v1,Positive semidefinite support vector regression metric learning,"Most existing metric learning methods focus on learning a similarity or distance measure relying on similar and dissimilar relations between sample pairs. However, pairs of samples cannot be simply identified as similar or dissimilar in many real-world applications, e.g., multi-label learning, label distribution learning. To this end, relation alignment metric learning (RAML) framework is proposed to handle the metric learning problem in those scenarios. But RAML framework uses SVR solvers for optimization. It can't learn positive semidefinite distance metric which is necessary in metric learning. In this paper, we propose two methds to overcame the weakness. Further, We carry out several experiments on the single-label classification, multi-label classification, label distribution learning to demonstrate the new methods achieves favorable performance against RAML framework.","cs.LG, stat.ML",Lifeng Gu,[]
,http://arxiv.org/abs/1706.00066v1,2017-05-31T19:42:41Z,,arXiv,cs.AI,http://arxiv.org/pdf/1706.00066v1.pdf,1706.00066v1,Descriptions of Objectives and Processes of Mechanical Learning,"In [1], we introduced mechanical learning and proposed 2 approaches to mechanical learning. Here, we follow one such approach to well describe the objects and the processes of learning. We discuss 2 kinds of patterns: objective and subjective pattern. Subjective pattern is crucial for learning machine. We prove that for any objective pattern we can find a proper subjective pattern based upon least base patterns to express the objective pattern well. X-form is algebraic expression for subjective pattern. Collection of X-forms form internal representation space, which is center of learning machine. We discuss learning by teaching and without teaching. We define data sufficiency by X-form. We then discussed some learning strategies. We show, in each strategy, with sufficient data, and with certain capabilities, learning machine indeed can learn any pattern (universal learning machine). In appendix, with knowledge of learning machine, we try to view deep learning from a different angle, i.e. its internal representation space and its learning dynamics.",cs.AI,Chuyu Xiong,[]
,http://arxiv.org/abs/1904.09644v2,2019-04-21T19:00:43Z,,arXiv,cs.LG,http://arxiv.org/pdf/1904.09644v2.pdf,1904.09644v2,Intermittent Learning: On-Device Machine Learning on Intermittently   Powered System,"This paper introduces intermittent learning - the goal of which is to enable energy harvested computing platforms capable of executing certain classes of machine learning tasks effectively and efficiently. We identify unique challenges to intermittent learning relating to the data and application semantics of machine learning tasks, and to address these challenges, we devise 1) an algorithm that determines a sequence of actions to achieve the desired learning objective under tight energy constraints, and 2) propose three heuristics that help an intermittent learner decide whether to learn or discard training examples at run-time which increases the energy efficiency of the system. We implement and evaluate three intermittent learning applications that learn the 1) air quality, 2) human presence, and 3) vibration using solar, RF, and kinetic energy harvesters, respectively. We demonstrate that the proposed framework improves the energy efficiency of a learner by up to 100% and cuts down the number of learning examples by up to 50% when compared to state-of-the-art intermittent computing systems that do not implement the proposed intermittent learning framework.","cs.LG, stat.ML","Seulki Lee, Bashima Islam, Yubo Luo, Shahriar Nirjon",[]
,http://arxiv.org/abs/1203.3783v1,2012-03-16T19:01:10Z,,arXiv,stat.ML,http://arxiv.org/pdf/1203.3783v1.pdf,10.1007/978-3-642-35289-8_33,Learning Feature Hierarchies with Centered Deep Boltzmann Machines,"Deep Boltzmann machines are in principle powerful models for extracting the hierarchical structure of data. Unfortunately, attempts to train layers jointly (without greedy layer-wise pretraining) have been largely unsuccessful. We propose a modification of the learning algorithm that initially recenters the output of the activation functions to zero. This modification leads to a better conditioned Hessian and thus makes learning easier. We test the algorithm on real data and demonstrate that our suggestion, the centered deep Boltzmann machine, learns a hierarchy of increasingly abstract representations and a better generative model of data.","stat.ML, cs.AI, cs.LG","Grégoire Montavon, Klaus-Robert Müller",[]
,http://arxiv.org/abs/1502.02127v2,2015-02-07T11:46:22Z,,arXiv,cs.LG,http://arxiv.org/pdf/1502.02127v2.pdf,1502.02127v2,Hyperparameter Search in Machine Learning,"We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.","cs.LG, stat.ML, G.1.6; I.2.6; I.2.8; I.5","Marc Claesen, Bart De Moor",[]
,http://arxiv.org/abs/1511.03198v1,2015-11-10T17:41:48Z,,arXiv,cs.LG,http://arxiv.org/pdf/1511.03198v1.pdf,1511.03198v1,Sliced Wasserstein Kernels for Probability Distributions,"Optimal transport distances, otherwise known as Wasserstein distances, have recently drawn ample attention in computer vision and machine learning as a powerful discrepancy measure for probability distributions. The recent developments on alternative formulations of the optimal transport have allowed for faster solutions to the problem and has revamped its practical applications in machine learning. In this paper, we exploit the widely used kernel methods and provide a family of provably positive definite kernels based on the Sliced Wasserstein distance and demonstrate the benefits of these kernels in a variety of learning tasks. Our work provides a new perspective on the application of optimal transport flavored distances through kernel methods in machine learning tasks.","cs.LG, stat.ML","Soheil Kolouri, Yang Zou, Gustavo K. Rohde",[]
,http://arxiv.org/abs/1511.03643v3,2015-11-11T20:27:54Z,,arXiv,stat.ML,http://arxiv.org/pdf/1511.03643v3.pdf,1511.03643v3,Unifying distillation and privileged information,"Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper unifies these two techniques into generalized distillation, a framework to learn from multiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efficacy on a variety of numerical simulations on both synthetic and real-world data.","stat.ML, cs.LG","David Lopez-Paz, Léon Bottou, Bernhard Schölkopf, Vladimir Vapnik",[]
,http://arxiv.org/abs/1510.00772v1,2015-10-03T02:57:47Z,,arXiv,cs.LG,http://arxiv.org/pdf/1510.00772v1.pdf,1510.00772v1,Machine Learning for Machine Data from a CATI Network,"This is a machine learning application paper involving big data. We present high-accuracy prediction methods of rare events in semi-structured machine log files, which are produced at high velocity and high volume by NORC's computer-assisted telephone interviewing (CATI) network for conducting surveys. We judiciously apply natural language processing (NLP) techniques and data-mining strategies to train effective learning and prediction models for classifying uncommon error messages in the log---without access to source code, updated documentation or dictionaries. In particular, our simple but effective approach of features preallocation for learning from imbalanced data coupled with naive Bayes classifiers can be conceivably generalized to supervised or semi-supervised learning and prediction methods for other critical events such as cyberattack detection.",cs.LG,Sou-Cheng T. Choi,[]
,http://arxiv.org/abs/2106.05466v2,2021-06-10T02:56:35Z,,arXiv,q-bio.QM,http://arxiv.org/pdf/2106.05466v2.pdf,2106.05466v2,Adaptive machine learning for protein engineering,"Machine-learning models that learn from data to predict how protein sequence encodes function are emerging as a useful protein engineering tool. However, when using these models to suggest new protein designs, one must deal with the vast combinatorial complexity of protein sequences. Here, we review how to use a sequence-to-function machine-learning surrogate model to select sequences for experimental measurement. First, we discuss how to select sequences through a single round of machine-learning optimization. Then, we discuss sequential optimization, where the goal is to discover optimized sequences and improve the model across multiple rounds of training, optimization, and experimental measurement.","q-bio.QM, cs.LG, q-bio.BM","Brian L. Hie, Kevin K. Yang",[]
,http://arxiv.org/abs/1909.13316v1,2019-09-29T16:44:12Z,,arXiv,stat.ML,http://arxiv.org/pdf/1909.13316v1.pdf,1909.13316v1,Machine Learning vs Statistical Methods for Time Series Forecasting:   Size Matters,"Time series forecasting is one of the most active research topics. Machine learning methods have been increasingly adopted to solve these predictive tasks. However, in a recent work, these were shown to systematically present a lower predictive performance relative to simple statistical methods. In this work, we counter these results. We show that these are only valid under an extremely low sample size. Using a learning curve method, our results suggest that machine learning methods improve their relative predictive performance as the sample size grows. The code to reproduce the experiments is available at https://github.com/vcerqueira/MLforForecasting.","stat.ML, cs.LG","Vitor Cerqueira, Luis Torgo, Carlos Soares",[]
,http://arxiv.org/abs/2001.04601v1,2020-01-14T03:06:53Z,,arXiv,stat.ML,http://arxiv.org/pdf/2001.04601v1.pdf,2001.04601v1,For2For: Learning to forecast from forecasts,"This paper presents a time series forecasting framework which combines standard forecasting methods and a machine learning model. The inputs to the machine learning model are not lagged values or regular time series features, but instead forecasts produced by standard methods. The machine learning model can be either a convolutional neural network model or a recurrent neural network model. The intuition behind this approach is that forecasts of a time series are themselves good features characterizing the series, especially when the modelling purpose is forecasting. It can also be viewed as a weighted ensemble method. Tested on the M4 competition dataset, this approach outperforms all submissions for quarterly series, and is more accurate than all but the winning algorithm for monthly series.","stat.ML, cs.LG","Shi Zhao, Ying Feng",[]
,http://arxiv.org/abs/2005.14139v1,2020-05-28T16:43:18Z,,arXiv,physics.chem-ph,http://arxiv.org/pdf/2005.14139v1.pdf,10.1088/2632-2153/ab9c3e,Machine learning and excited-state molecular dynamics,"Machine learning is employed at an increasing rate in the research field of quantum chemistry. While the majority of approaches target the investigation of chemical systems in their electronic ground state, the inclusion of light into the processes leads to electronically excited states and gives rise to several new challenges. Here, we survey recent advances for excited-state dynamics based on machine learning. In doing so, we highlight successes, pitfalls, challenges and future avenues for machine learning approaches for light-induced molecular processes.","physics.chem-ph, stat.ML","Julia Westermayr, Philipp Marquetand",[]
,http://arxiv.org/abs/2011.04890v1,2020-11-10T04:45:52Z,,arXiv,quant-ph,http://arxiv.org/pdf/2011.04890v1.pdf,2011.04890v1,Quantum reservoir computing: a reservoir approach toward quantum machine   learning on near-term quantum devices,"Quantum systems have an exponentially large degree of freedom in the number of particles and hence provide a rich dynamics that could not be simulated on conventional computers. Quantum reservoir computing is an approach to use such a complex and rich dynamics on the quantum systems as it is for temporal machine learning. In this chapter, we explain quantum reservoir computing and related approaches, quantum extreme learning machine and quantum circuit learning, starting from a pedagogical introduction to quantum mechanics and machine learning. All these quantum machine learning approaches are experimentally feasible and effective on the state-of-the-art quantum devices.","quant-ph, nlin.AO","Keisuke Fujii, Kohei Nakajima",[]
,http://arxiv.org/abs/2306.04748v2,2023-06-07T19:54:56Z,,arXiv,cs.LG,http://arxiv.org/pdf/2306.04748v2.pdf,10.4236/oalib.1111135,"Analysis, Identification and Prediction of Parkinson Disease Sub-Types   and Progression through Machine Learning","This paper represents a groundbreaking advancement in Parkinson disease (PD) research by employing a novel machine learning framework to categorize PD into distinct subtypes and predict its progression. Utilizing a comprehensive dataset encompassing both clinical and neurological parameters, the research applies advanced supervised and unsupervised learning techniques. This innovative approach enables the identification of subtle, yet critical, patterns in PD manifestation, which traditional methodologies often miss. Significantly, this research offers a path toward personalized treatment strategies, marking a major stride in the precision medicine domain and showcasing the transformative potential of integrating machine learning into medical research.",cs.LG,Ashwin Ram,[]
,http://arxiv.org/abs/2306.09624v1,2023-06-16T04:47:01Z,,arXiv,stat.ML,http://arxiv.org/pdf/2306.09624v1.pdf,2306.09624v1,Power-law Dynamic arising from machine learning,"We study a kind of new SDE that was arisen from the research on optimization in machine learning, we call it power-law dynamic because its stationary distribution cannot have sub-Gaussian tail and obeys power-law. We prove that the power-law dynamic is ergodic with unique stationary distribution, provided the learning rate is small enough. We investigate its first exist time. In particular, we compare the exit times of the (continuous) power-law dynamic and its discretization. The comparison can help guide machine learning algorithm.","stat.ML, cs.LG","Wei Chen, Weitao Du, Zhi-Ming Ma, Qi Meng",[]
,http://arxiv.org/abs/2309.02532v1,2023-09-05T18:58:10Z,,arXiv,cond-mat.dis-nn,http://arxiv.org/pdf/2309.02532v1.pdf,2309.02532v1,Design of Oscillatory Neural Networks by Machine Learning,"We demonstrate the utility of machine learning algorithms for the design of Oscillatory Neural Networks (ONNs). After constructing a circuit model of the oscillators in a machine-learning-enabled simulator and performing Backpropagation through time (BPTT) for determining the coupling resistances between the ring oscillators, we show the design of associative memories and multi-layered ONN classifiers. The machine-learning-designed ONNs show superior performance compared to other design methods (such as Hebbian learning) and they also enable significant simplifications in the circuit topology. We demonstrate the design of multi-layered ONNs that show superior performance compared to single-layer ones. We argue Machine learning can unlock the true computing potential of ONNs hardware.",cond-mat.dis-nn,"Tamas Rudner, Wolfgang Porod, Gyorgy Csaba",[]
,http://arxiv.org/abs/2403.10175v2,2024-03-15T10:31:46Z,,arXiv,cs.LG,http://arxiv.org/pdf/2403.10175v2.pdf,2403.10175v2,A Short Survey on Importance Weighting for Machine Learning,"Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.","cs.LG, cs.AI, stat.ML","Masanari Kimura, Hideitsu Hino",[]
,http://arxiv.org/abs/2405.20620v1,2024-05-31T05:10:30Z,,arXiv,cs.LG,http://arxiv.org/pdf/2405.20620v1.pdf,2405.20620v1,"""Forgetting"" in Machine Learning and Beyond: A Survey","This survey investigates the multifaceted nature of forgetting in machine learning, drawing insights from neuroscientific research that posits forgetting as an adaptive function rather than a defect, enhancing the learning process and preventing overfitting. This survey focuses on the benefits of forgetting and its applications across various machine learning sub-fields that can help improve model performance and enhance data privacy. Moreover, the paper discusses current challenges, future directions, and ethical considerations regarding the integration of forgetting mechanisms into machine learning models.",cs.LG,"Alyssa Shuang Sha, Bernardo Pereira Nunes, Armin Haller",[]
,http://arxiv.org/abs/2409.09537v1,2024-09-14T21:39:17Z,,arXiv,cs.LG,http://arxiv.org/pdf/2409.09537v1.pdf,2409.09537v1,Deep Fast Machine Learning Utils: A Python Library for Streamlined   Machine Learning Prototyping,"Machine learning (ML) research and application often involve time-consuming steps such as model architecture prototyping, feature selection, and dataset preparation. To support these tasks, we introduce the Deep Fast Machine Learning Utils (DFMLU) library, which provides tools designed to automate and enhance aspects of these processes. Compatible with frameworks like TensorFlow, Keras, and Scikit-learn, DFMLU offers functionalities that support model development and data handling. The library includes methods for dense neural network search, advanced feature selection, and utilities for data management and visualization of training outcomes. This manuscript presents an overview of DFMLU's functionalities, providing Python examples for each tool.",cs.LG,Fabi Prezja,[]
,http://arxiv.org/abs/2107.11921v4,2021-07-26T01:41:25Z,,arXiv,cs.LG,http://arxiv.org/pdf/2107.11921v4.pdf,2107.11921v4,Compensation Learning,"Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert lower weights on samples which are likely to be noisy or quite hard. This study reveals another undiscovered strategy, namely, compensating. Various incarnations of compensating have been utilized but it has not been explicitly revealed. Learning with compensating is called compensation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, compensation learning is divided on the basis of the compensation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be viewed or understood at least partially as compensation techniques. Furthermore, a family of new learning algorithms can be obtained by plugging the compensation learning into existing learning algorithms. Specifically, two concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the two new algorithms. Compensation learning can also be used in other various learning scenarios, such as imbalance learning, clustering, regression, and so on.",cs.LG,"Rujing Yao, Ou Wu",[]
,http://arxiv.org/abs/1904.05061v2,2019-04-10T08:36:46Z,,arXiv,cs.NE,http://arxiv.org/pdf/1904.05061v2.pdf,1904.05061v2,A review on Neural Turing Machine,"One of the major objectives of Artificial Intelligence is to design learning algorithms that are executed on a general purposes computational machines such as human brain. Neural Turing Machine (NTM) is a step towards realizing such a computational machine. The attempt is made here to run a systematic review on Neural Turing Machine. First, the mind-map and taxonomy of machine learning, neural networks, and Turing machine are introduced. Next, NTM is inspected in terms of concepts, structure, variety of versions, implemented tasks, comparisons, etc. Finally, the paper discusses on issues and ends up with several future works.","cs.NE, cs.LG, 68T01","Soroor Malekmohammadi Faradonbeh, Faramarz Safi-Esfahani",[]
,http://arxiv.org/abs/1805.05409v2,2018-05-11T14:30:30Z,,arXiv,cs.CY,http://arxiv.org/pdf/1805.05409v2.pdf,1805.05409v2,"Machine Learning for Public Administration Research, with Application to   Organizational Reputation","Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories of public administration and to develop new theories. Despite the excitement surrounding machine learning methods, clarity regarding their proper use and potential pitfalls is lacking. This paper attempts to fill this gap in the literature through providing a machine learning ""guide to practice"" for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner. We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.","cs.CY, cs.LG, stat.ML","L. Jason Anastasopoulos, Andrew B. Whitford",[]
,http://arxiv.org/abs/1911.03886v2,2019-11-10T09:59:34Z,,arXiv,eess.SP,http://arxiv.org/pdf/1911.03886v2.pdf,10.1109/tcomm.2021.3083597,Performance Analysis on Machine Learning-Based Channel Estimation,"Recently, machine learning-based channel estimation has attracted much attention. The performance of machine learning-based estimation has been validated by simulation experiments. However, little attention has been paid to the theoretical performance analysis. In this paper, we investigate the mean square error (MSE) performance of machine learning-based estimation. Hypothesis testing is employed to analyze its MSE upper bound. Furthermore, we build a statistical model for hypothesis testing, which holds when the linear learning module with a low input dimension is used in machine learning-based channel estimation, and derive a clear analytical relation between the size of the training data and performance. Then, we simulate the machine learning-based channel estimation in orthogonal frequency division multiplexing (OFDM) systems to verify our analysis results. Finally, the design considerations for the situation where only limited training data is available are discussed. In this situation, our analysis results can be applied to assess the performance and support the design of machine learning-based channel estimation.","eess.SP, cs.LG","Kai Mei, Jun Liu, Xiaochen Zhang, Nandana Rajatheva, Jibo Wei",[]
,http://arxiv.org/abs/1711.02038v1,2017-11-06T17:44:03Z,,arXiv,quant-ph,http://arxiv.org/pdf/1711.02038v1.pdf,1711.02038v1,An efficient quantum algorithm for generative machine learning,"A central task in the field of quantum computing is to find applications where quantum computer could provide exponential speedup over any classical computer. Machine learning represents an important field with broad applications where quantum computer may offer significant speedup. Several quantum algorithms for discriminative machine learning have been found based on efficient solving of linear algebraic problems, with potential exponential speedup in runtime under the assumption of effective input from a quantum random access memory. In machine learning, generative models represent another large class which is widely used for both supervised and unsupervised learning. Here, we propose an efficient quantum algorithm for machine learning based on a quantum generative model. We prove that our proposed model is exponentially more powerful to represent probability distributions compared with classical generative models and has exponential speedup in training and inference at least for some instances under a reasonable assumption in computational complexity theory. Our result opens a new direction for quantum machine learning and offers a remarkable example in which a quantum algorithm shows exponential improvement over any classical algorithm in an important application field.","quant-ph, cs.LG, stat.ML","Xun Gao, Zhengyu Zhang, Luming Duan",[]
,http://arxiv.org/abs/1802.05351v3,2018-02-14T22:58:31Z,,arXiv,cs.CR,http://arxiv.org/pdf/1802.05351v3.pdf,1802.05351v3,Stealing Hyperparameters in Machine Learning,"Hyperparameters are critical in machine learning, as different hyperparameters often result in models with significantly different performance. Hyperparameters may be deemed confidential because of their commercial value and the confidentiality of the proprietary algorithms that the learner uses to learn them. In this work, we propose attacks on stealing the hyperparameters that are learned by a learner. We call our attacks hyperparameter stealing attacks. Our attacks are applicable to a variety of popular machine learning algorithms such as ridge regression, logistic regression, support vector machine, and neural network. We evaluate the effectiveness of our attacks both theoretically and empirically. For instance, we evaluate our attacks on Amazon Machine Learning. Our results demonstrate that our attacks can accurately steal hyperparameters. We also study countermeasures. Our results highlight the need for new defenses against our hyperparameter stealing attacks for certain machine learning algorithms.","cs.CR, cs.LG, stat.ML","Binghui Wang, Neil Zhenqiang Gong",[]
,http://arxiv.org/abs/1907.00678v1,2019-07-01T12:06:18Z,,arXiv,cs.LG,http://arxiv.org/pdf/1907.00678v1.pdf,1907.00678v1,Two-stage Optimization for Machine Learning Workflow,"Machines learning techniques plays a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners.   For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as autoML.   In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.","cs.LG, cs.AI",Alexandre Quemy,[]
,http://arxiv.org/abs/1812.03057v1,2018-12-07T15:02:40Z,,arXiv,cs.CY,http://arxiv.org/pdf/1812.03057v1.pdf,1812.03057v1,Open Problems in Engineering and Quality Assurance of Safety Critical   Machine Learning Systems,"Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems using machine-learning and deep-learning models, such as automated-driving vehicles. Quality assurance frameworks are required for such machine learning systems, but there are no widely accepted and established quality-assurance concepts and techniques. At the same time, open problems and the relevant technical fields are not organized. To establish standard quality assurance frameworks, it is necessary to visualize and organize these open problems in an interdisciplinary way, so that the experts from many different technical fields may discuss these problems in depth and develop solutions. In the present study, we identify, classify, and explore the open problems in quality assurance of safety-critical machine-learning systems, and their relevant corresponding industry and technological trends, using automated-driving vehicles as an example. Our results show that addressing these open problems requires incorporating knowledge from several different technological and industrial fields, including the automobile industry, statistics, software engineering, and machine learning.","cs.CY, cs.LG, stat.ML","Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae",[]
,http://arxiv.org/abs/1904.05961v3,2019-04-11T21:39:10Z,,arXiv,cs.LG,http://arxiv.org/pdf/1904.05961v3.pdf,1904.05961v3,Robust Coreset Construction for Distributed Machine Learning,"Coreset, which is a summary of the original dataset in the form of a small weighted set in the same sample space, provides a promising approach to enable machine learning over distributed data. Although viewed as a proxy of the original dataset, each coreset is only designed to approximate the cost function of a specific machine learning problem, and thus different coresets are often required to solve different machine learning problems, increasing the communication overhead. We resolve this dilemma by developing robust coreset construction algorithms that can support a variety of machine learning problems. Motivated by empirical evidence that suitably-weighted k-clustering centers provide a robust coreset, we harden the observation by establishing theoretical conditions under which the coreset provides a guaranteed approximation for a broad range of machine learning problems, and developing both centralized and distributed algorithms to generate coresets satisfying the conditions. The robustness of the proposed algorithms is verified through extensive experiments on diverse datasets with respect to both supervised and unsupervised learning problems.","cs.LG, cs.DS, stat.ML","Hanlin Lu, Ming-Ju Li, Ting He, Shiqiang Wang, Vijaykrishnan Narayanan, Kevin S Chan",[]
,http://arxiv.org/abs/2006.09271v2,2020-06-15T17:46:12Z,,arXiv,cs.CR,http://arxiv.org/pdf/2006.09271v2.pdf,2006.09271v2,A Survey of Machine Learning Methods and Challenges for Windows Malware   Classification,"Malware classification is a difficult problem, to which machine learning methods have been applied for decades. Yet progress has often been slow, in part due to a number of unique difficulties with the task that occur through all stages of the developing a machine learning system: data collection, labeling, feature creation and selection, model selection, and evaluation. In this survey we will review a number of the current methods and challenges related to malware classification, including data collection, feature extraction, and model construction, and evaluation. Our discussion will include thoughts on the constraints that must be considered for machine learning based solutions in this domain, and yet to be tackled problems for which machine learning could also provide a solution. This survey aims to be useful both to cybersecurity practitioners who wish to learn more about how machine learning can be applied to the malware problem, and to give data scientists the necessary background into the challenges in this uniquely complicated space.","cs.CR, cs.LG, stat.AP, stat.ML","Edward Raff, Charles Nicholas",[]
,http://arxiv.org/abs/2006.16789v2,2020-06-27T13:01:28Z,,arXiv,cs.LG,http://arxiv.org/pdf/2006.16789v2.pdf,2006.16789v2,Causality Learning: A New Perspective for Interpretable Machine Learning,"Recent years have witnessed the rapid growth of machine learning in a wide range of fields such as image recognition, text classification, credit scoring prediction, recommendation system, etc. In spite of their great performance in different sectors, researchers still concern about the mechanism under any machine learning (ML) techniques that are inherently black-box and becoming more complex to achieve higher accuracy. Therefore, interpreting machine learning model is currently a mainstream topic in the research community. However, the traditional interpretable machine learning focuses on the association instead of the causality. This paper provides an overview of causal analysis with the fundamental background and key concepts, and then summarizes most recent causal approaches for interpretable machine learning. The evaluation techniques for assessing method quality, and open problems in causal interpretability are also discussed in this paper.","cs.LG, cs.AI, stat.ML","Guandong Xu, Tri Dung Duong, Qian Li, Shaowu Liu, Xianzhi Wang",[]
,http://arxiv.org/abs/2009.04661v1,2020-09-10T04:07:10Z,,arXiv,cs.CY,http://arxiv.org/pdf/2009.04661v1.pdf,2009.04661v1,A Framework for Fairer Machine Learning in Organizations,"With the increase in adoption of machine learning tools by organizations risks of unfairness abound, especially when human decision processes in outcomes of socio-economic importance such as hiring, housing, lending, and admissions are automated. We reveal sources of unfair machine learning, review fairness criteria, and provide a framework which, if implemented, would enable an organization to both avoid implementing an unfair machine learning model, but also to avoid the common situation that as an algorithm learns with more data it can become unfair over time. Issues of behavioral ethics in machine learning implementations by organizations have not been thoroughly addressed in the literature, because many of the necessary concepts are dispersed across three literatures: ethics, machine learning, and management. Further, tradeoffs between fairness criteria in machine learning have not been addressed with regards to organizations. We advance the research by introducing an organizing framework for selecting and implementing fair algorithms in organizations.","cs.CY, cs.LG, 68T05, I.2.6","Lily Morse, Mike H. M. Teodorescu, Yazeed Awwad, Gerald Kane",[]
,http://arxiv.org/abs/2112.01998v1,2021-12-03T16:25:26Z,,arXiv,cs.LG,http://arxiv.org/pdf/2112.01998v1.pdf,2112.01998v1,"Application of Machine Learning in understanding plant virus   pathogenesis: Trends and perspectives on emergence, diagnosis, host-virus   interplay and management","Inclusion of high throughput technologies in the field of biology has generated massive amounts of biological data in the recent years. Now, transforming these huge volumes of data into knowledge is the primary challenge in computational biology. The traditional methods of data analysis have failed to carry out the task. Hence, researchers are turning to machine learning based approaches for the analysis of high-dimensional big data. In machine learning, once a model is trained with a training dataset, it can be applied on a testing dataset which is independent. In current times, deep learning algorithms further promote the application of machine learning in several field of biology including plant virology. Considering a significant progress in the application of machine learning in understanding plant virology, this review highlights an introductory note on machine learning and comprehensively discusses the trends and prospects of machine learning in diagnosis of viral diseases, understanding host-virus interplay and emergence of plant viruses.",cs.LG,"Dibyendu Ghosh, Srija Chakraborty, Hariprasad Kodamana, Supriya Chakraborty",[]
,http://arxiv.org/abs/2211.10708v1,2022-11-19T14:20:53Z,,arXiv,cs.LG,http://arxiv.org/pdf/2211.10708v1.pdf,2211.10708v1,A Survey on Differential Privacy with Machine Learning and Future   Outlook,"Nowadays, machine learning models and applications have become increasingly pervasive. With this rapid increase in the development and employment of machine learning models, a concern regarding privacy has risen. Thus, there is a legitimate need to protect the data from leaking and from any attacks. One of the strongest and most prevalent privacy models that can be used to protect machine learning models from any attacks and vulnerabilities is differential privacy (DP). DP is strict and rigid definition of privacy, where it can guarantee that an adversary is not capable to reliably predict if a specific participant is included in the dataset or not. It works by injecting a noise to the data whether to the inputs, the outputs, the ground truth labels, the objective functions, or even to the gradients to alleviate the privacy issue and protect the data. To this end, this survey paper presents different differentially private machine learning algorithms categorized into two main categories (traditional machine learning models vs. deep learning models). Moreover, future research directions for differential privacy with machine learning algorithms are outlined.","cs.LG, cs.CR","Samah Baraheem, Zhongmei Yao",[]
,http://arxiv.org/abs/2302.02926v2,2023-02-06T16:59:25Z,,arXiv,cs.LG,http://arxiv.org/pdf/2302.02926v2.pdf,2302.02926v2,Curriculum Graph Machine Learning: A Survey,"Graph machine learning has been extensively studied in both academia and industry. However, in the literature, most existing graph machine learning models are designed to conduct training with data samples in a random order, which may suffer from suboptimal performance due to ignoring the importance of different graph data samples and their training orders for the model optimization status. To tackle this critical problem, curriculum graph machine learning (Graph CL), which integrates the strength of graph machine learning and curriculum learning, arises and attracts an increasing amount of attention from the research community. Therefore, in this paper, we comprehensively overview approaches on Graph CL and present a detailed survey of recent advances in this direction. Specifically, we first discuss the key challenges of Graph CL and provide its formal problem definition. Then, we categorize and summarize existing methods into three classes based on three kinds of graph machine learning tasks, i.e., node-level, link-level, and graph-level tasks. Finally, we share our thoughts on future research directions. To the best of our knowledge, this paper is the first survey for curriculum graph machine learning.",cs.LG,"Haoyang Li, Xin Wang, Wenwu Zhu",[]
,http://arxiv.org/abs/2306.04646v1,2023-06-01T19:35:13Z,,arXiv,cs.LG,http://arxiv.org/pdf/2306.04646v1.pdf,2306.04646v1,Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO   Data by Leveraging A Simple Spatial-Aware Technique,"Accurate yield forecasting is essential for making informed policies and long-term decisions for food security. Earth Observation (EO) data and machine learning algorithms play a key role in providing a comprehensive and timely view of crop conditions from field to national scales. However, machine learning algorithms' prediction accuracy is often harmed by spatial heterogeneity caused by exogenous factors not reflected in remote sensing data, such as differences in crop management strategies. In this paper, we propose and investigate a simple technique called state-wise additive bias to explicitly address the cross-region yield heterogeneity in Kazakhstan. Compared to baseline machine learning models (Random Forest, CatBoost, XGBoost), our method reduces the overall RMSE by 8.9\% and the highest state-wise RMSE by 28.37\%. The effectiveness of state-wise additive bias indicates machine learning's performance can be significantly improved by explicitly addressing the spatial heterogeneity, motivating future work on spatial-aware machine learning algorithms for yield forecasts as well as for general geospatial forecasting problems.","cs.LG, cs.CY","Anh Nhat Nhu, Ritvik Sahajpal, Christina Justice, Inbal Becker-Reshef",[]
,http://arxiv.org/abs/2402.14694v1,2024-02-22T16:48:17Z,,arXiv,quant-ph,http://arxiv.org/pdf/2402.14694v1.pdf,2402.14694v1,A Quick Introduction to Quantum Machine Learning for Non-Practitioners,"This paper provides an introduction to quantum machine learning, exploring the potential benefits of using quantum computing principles and algorithms that may improve upon classical machine learning approaches. Quantum computing utilizes particles governed by quantum mechanics for computational purposes, leveraging properties like superposition and entanglement for information representation and manipulation. Quantum machine learning applies these principles to enhance classical machine learning models, potentially reducing network size and training time on quantum hardware. The paper covers basic quantum mechanics principles, including superposition, phase space, and entanglement, and introduces the concept of quantum gates that exploit these properties. It also reviews classical deep learning concepts, such as artificial neural networks, gradient descent, and backpropagation, before delving into trainable quantum circuits as neural networks. An example problem demonstrates the potential advantages of quantum neural networks, and the appendices provide detailed derivations. The paper aims to help researchers new to quantum mechanics and machine learning develop their expertise more efficiently.","quant-ph, cs.ET, cs.LG","Ethan N. Evans, Dominic Byrne, Matthew G. Cook",[]
,http://arxiv.org/abs/1910.07012v1,2019-10-15T19:26:31Z,,arXiv,cs.LG,http://arxiv.org/pdf/1910.07012v1.pdf,1910.07012v1,Transfer Learning for Algorithm Recommendation,"Meta-Learning is a subarea of Machine Learning that aims to take advantage of prior knowledge to learn faster and with fewer data [1]. There are different scenarios where meta-learning can be applied, and one of the most common is algorithm recommendation, where previous experience on applying machine learning algorithms for several datasets can be used to learn which algorithm, from a set of options, would be more suitable for a new dataset [2]. Perhaps the most popular form of meta-learning is transfer learning, which consists of transferring knowledge acquired by a machine learning algorithm in a previous learning task to increase its performance faster in another and similar task [3]. Transfer Learning has been widely applied in a variety of complex tasks such as image classification, machine translation and, speech recognition, achieving remarkable results [4,5,6,7,8]. Although transfer learning is very used in traditional or base-learning, it is still unknown if it is useful in a meta-learning setup. For that purpose, in this paper, we investigate the effects of transferring knowledge in the meta-level instead of base-level. Thus, we train a neural network on meta-datasets related to algorithm recommendation, and then using transfer learning, we reuse the knowledge learned by the neural network in other similar datasets from the same domain, to verify how transferable is the acquired meta-knowledge.","cs.LG, cs.NE, stat.ML","Gean Trindade Pereira, Moisés dos Santos, Edesio Alcobaça, Rafael Mantovani, André Carvalho",[]
,http://arxiv.org/abs/2404.19370v1,2024-04-30T08:58:47Z,,arXiv,cs.AI,http://arxiv.org/pdf/2404.19370v1.pdf,2404.19370v1,Numeric Reward Machines,"Reward machines inform reinforcement learning agents about the reward structure of the environment and often drastically speed up the learning process. However, reward machines only accept Boolean features such as robot-reached-gold. Consequently, many inherently numeric tasks cannot profit from the guidance offered by reward machines. To address this gap, we aim to extend reward machines with numeric features such as distance-to-gold. For this, we present two types of reward machines: numeric-Boolean and numeric. In a numeric-Boolean reward machine, distance-to-gold is emulated by two Boolean features distance-to-gold-decreased and robot-reached-gold. In a numeric reward machine, distance-to-gold is used directly alongside the Boolean feature robot-reached-gold. We compare our new approaches to a baseline reward machine in the Craft domain, where the numeric feature is the agent-to-target distance. We use cross-product Q-learning, Q-learning with counter-factual experiences, and the options framework for learning. Our experimental results show that our new approaches significantly outperform the baseline approach. Extending reward machines with numeric features opens up new possibilities of using reward machines in inherently numeric tasks.","cs.AI, cs.LG","Kristina Levina, Nikolaos Pappas, Athanasios Karapantelakis, Aneta Vulgarakis Feljan, Jendrik Seipp",[]
,http://arxiv.org/abs/1911.02621v3,2019-11-06T20:29:56Z,,arXiv,cs.CR,http://arxiv.org/pdf/1911.02621v3.pdf,1911.02621v3,The Threat of Adversarial Attacks on Machine Learning in Network   Security -- A Survey,"Machine learning models have made many decision support systems to be faster, more accurate, and more efficient. However, applications of machine learning in network security face a more disproportionate threat of active adversarial attacks compared to other domains. This is because machine learning applications in network security such as malware detection, intrusion detection, and spam filtering are by themselves adversarial in nature. In what could be considered an arm's race between attackers and defenders, adversaries constantly probe machine learning systems with inputs that are explicitly designed to bypass the system and induce a wrong prediction. In this survey, we first provide a taxonomy of machine learning techniques, tasks, and depth. We then introduce a classification of machine learning in network security applications. Next, we examine various adversarial attacks against machine learning in network security and introduce two classification approaches for adversarial attacks in network security. First, we classify adversarial attacks in network security based on a taxonomy of network security applications. Secondly, we categorize adversarial attacks in network security into a problem space vs feature space dimensional classification model. We then analyze the various defenses against adversarial attacks on machine learning-based network security applications. We conclude by introducing an adversarial risk grid map and evaluating several existing adversarial attacks against machine learning in network security using the risk grid map. We also identify where each attack classification resides within the adversarial risk grid map.","cs.CR, cs.LG, cs.NI","Olakunle Ibitoye, Rana Abou-Khamis, Mohamed el Shehaby, Ashraf Matrawy, M. Omair Shafiq",[]
,http://arxiv.org/abs/1711.04708v1,2017-11-13T17:16:38Z,,arXiv,cs.LG,http://arxiv.org/pdf/1711.04708v1.pdf,1711.04708v1,Machine Learning for the Geosciences: Challenges and Opportunities,"Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML) -- that has been widely successful in commercial domains -- offers immense potential to contribute to problems in geosciences. However, problems in geosciences have several unique challenges that are seldom found in traditional applications, requiring novel problem formulations and methodologies in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their properties that make it challenging to use traditional machine learning techniques. We then describe some of the common categories of geoscience problems where machine learning can play a role, and discuss some of the existing efforts and promising directions for methodological development in machine learning. We conclude by discussing some of the emerging research themes in machine learning that are applicable across all problems in the geosciences, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.","cs.LG, cs.AI, cs.CV, physics.geo-ph","Anuj Karpatne, Imme Ebert-Uphoff, Sai Ravela, Hassan Ali Babaie, Vipin Kumar",[]
,http://arxiv.org/abs/2007.15745v3,2020-07-30T21:11:01Z,,arXiv,cs.LG,http://arxiv.org/pdf/2007.15745v3.pdf,10.1016/j.neucom.2020.07.061,On Hyperparameter Optimization of Machine Learning Algorithms: Theory   and Practice,"Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.","cs.LG, stat.ML, 68T01, 90C31, I.2.0; I.2.2; C.2.0","Li Yang, Abdallah Shami",[]
,http://arxiv.org/abs/2101.11948v2,2021-01-28T11:57:08Z,,arXiv,econ.EM,http://arxiv.org/pdf/2101.11948v2.pdf,10.1016/j.jocm.2021.100340,Choice modelling in the age of machine learning -- discussion paper,"Since its inception, the choice modelling field has been dominated by theory-driven modelling approaches. Machine learning offers an alternative data-driven approach for modelling choice behaviour and is increasingly drawing interest in our field. Cross-pollination of machine learning models, techniques and practices could help overcome problems and limitations encountered in the current theory-driven modelling paradigm, such as subjective labour-intensive search processes for model selection, and the inability to work with text and image data. However, despite the potential benefits of using the advances of machine learning to improve choice modelling practices, the choice modelling field has been hesitant to embrace machine learning. This discussion paper aims to consolidate knowledge on the use of machine learning models, techniques and practices for choice modelling, and discuss their potential. Thereby, we hope not only to make the case that further integration of machine learning in choice modelling is beneficial, but also to further facilitate it. To this end, we clarify the similarities and differences between the two modelling paradigms; we review the use of machine learning for choice modelling; and we explore areas of opportunities for embracing machine learning models and techniques to improve our practices. To conclude this discussion paper, we put forward a set of research questions which must be addressed to better understand if and how machine learning can benefit choice modelling.","econ.EM, cs.LG","S. Van Cranenburgh, S. Wang, A. Vij, F. Pereira, J. Walker",[]
,http://arxiv.org/abs/1811.01315v2,2018-11-04T02:55:49Z,,arXiv,cs.LG,http://arxiv.org/pdf/1811.01315v2.pdf,1811.01315v2,Modeling Stated Preference for Mobility-on-Demand Transit: A Comparison   of Machine Learning and Logit Models,"Logit models are usually applied when studying individual travel behavior, i.e., to predict travel mode choice and to gain behavioral insights on traveler preferences. Recently, some studies have applied machine learning to model travel mode choice and reported higher out-of-sample predictive accuracy than traditional logit models (e.g., multinomial logit). However, little research focuses on comparing the interpretability of machine learning with logit models. In other words, how to draw behavioral insights from the high-performance ""black-box"" machine-learning models remains largely unsolved in the field of travel behavior modeling.   This paper aims at providing a comprehensive comparison between the two approaches by examining the key similarities and differences in model development, evaluation, and behavioral interpretation between logit and machine-learning models for travel mode choice modeling. To complement the theoretical discussions, the paper also empirically evaluates the two approaches on the stated-preference survey data for a new type of transit system integrating high-frequency fixed-route services and ridesourcing. The results show that machine learning can produce significantly higher predictive accuracy than logit models. Moreover, machine learning and logit models largely agree on many aspects of behavioral interpretations. In addition, machine learning can automatically capture the nonlinear relationship between the input features and choice outcomes. The paper concludes that there is great potential in merging ideas from machine learning and conventional statistical methods to develop refined models for travel behavior research and suggests some new research directions.","cs.LG, cs.AI, stat.AP, stat.ML","Xilei Zhao, Xiang Yan, Alan Yu, Pascal Van Hentenryck",[]
,http://arxiv.org/abs/2006.13311v3,2020-06-16T16:32:27Z,,arXiv,physics.geo-ph,http://arxiv.org/pdf/2006.13311v3.pdf,10.1016/bs.agph.2020.08.002,70 years of machine learning in geoscience in review,"This review gives an overview of the development of machine learning in geoscience. A thorough analysis of the co-developments of machine learning applications throughout the last 70 years relates the recent enthusiasm for machine learning to developments in geoscience. I explore the shift of kriging towards a mainstream machine learning method and the historic application of neural networks in geoscience, following the general trend of machine learning enthusiasm through the decades. Furthermore, this chapter explores the shift from mathematical fundamentals and knowledge in software development towards skills in model validation, applied statistics, and integrated subject matter expertise. The review is interspersed with code examples to complement the theoretical foundations and illustrate model validation and machine learning explainability for science. The scope of this review includes various shallow machine learning methods, e.g. Decision Trees, Random Forests, Support-Vector Machines, and Gaussian Processes, as well as, deep neural networks, including feed-forward neural networks, convolutional neural networks, recurrent neural networks and generative adversarial networks. Regarding geoscience, the review has a bias towards geophysics but aims to strike a balance with geochemistry, geostatistics, and geology, however excludes remote sensing, as this would exceed the scope. In general, I aim to provide context for the recent enthusiasm surrounding deep learning with respect to research, hardware, and software developments that enable successful application of shallow and deep machine learning in all disciplines of Earth science.","physics.geo-ph, cs.CV, cs.LG, eess.IV",Jesper Sören Dramsch,[]
,http://arxiv.org/abs/2107.01238v1,2021-07-02T18:52:50Z,,arXiv,cs.LG,http://arxiv.org/pdf/2107.01238v1.pdf,2107.01238v1,Solving Machine Learning Problems,"Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT's 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system demonstrates an overall accuracy of 96% for open-response questions and 97% for multiple-choice questions, compared with MIT students' average of 93%, achieving grade A performance in the course, all in real-time. Questions cover all 12 topics taught in the course, excluding coding questions or questions with images. Topics include: (i) basic machine learning principles; (ii) perceptrons; (iii) feature extraction and selection; (iv) logistic regression; (v) regression; (vi) neural networks; (vii) advanced neural networks; (viii) convolutional neural networks; (ix) recurrent neural networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii) decision trees. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations. An important aspect of our approach is a data-augmentation scheme for generating new example problems. We also train a machine learning model to generate problem hints. Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.",cs.LG,"Sunny Tran, Pranav Krishna, Ishan Pakuwal, Prabhakar Kafle, Nikhil Singh, Jayson Lynch, Iddo Drori",[]
,http://arxiv.org/abs/2208.02030v1,2022-08-02T10:36:00Z,,arXiv,cs.SE,http://arxiv.org/pdf/2208.02030v1.pdf,2208.02030v1,BPMN4sML: A BPMN Extension for Serverless Machine Learning. Technology   Independent and Interoperable Modeling of Machine Learning Workflows and   their Serverless Deployment Orchestration,"Machine learning (ML) continues to permeate all layers of academia, industry and society. Despite its successes, mental frameworks to capture and represent machine learning workflows in a consistent and coherent manner are lacking. For instance, the de facto process modeling standard, Business Process Model and Notation (BPMN), managed by the Object Management Group, is widely accepted and applied. However, it is short of specific support to represent machine learning workflows. Further, the number of heterogeneous tools for deployment of machine learning solutions can easily overwhelm practitioners. Research is needed to align the process from modeling to deploying ML workflows.   We analyze requirements for standard based conceptual modeling for machine learning workflows and their serverless deployment. Confronting the shortcomings with respect to consistent and coherent modeling of ML workflows in a technology independent and interoperable manner, we extend BPMN's Meta-Object Facility (MOF) metamodel and the corresponding notation and introduce BPMN4sML (BPMN for serverless machine learning). Our extension BPMN4sML follows the same outline referenced by the Object Management Group (OMG) for BPMN. We further address the heterogeneity in deployment by proposing a conceptual mapping to convert BPMN4sML models to corresponding deployment models using TOSCA.   BPMN4sML allows technology-independent and interoperable modeling of machine learning workflows of various granularity and complexity across the entire machine learning lifecycle. It aids in arriving at a shared and standardized language to communicate ML solutions. Moreover, it takes the first steps toward enabling conversion of ML workflow model diagrams to corresponding deployment models for serverless deployment via TOSCA.","cs.SE, cs.LG",Laurens Martin Tetzlaff,[]
,http://arxiv.org/abs/2107.02378v3,2021-07-06T04:05:08Z,,arXiv,cs.LG,http://arxiv.org/pdf/2107.02378v3.pdf,2107.02378v3,Learning an Explicit Hyperparameter Prediction Function Conditioned on   Tasks,"Meta learning has attracted much attention recently in machine learning community. Contrary to conventional machine learning aiming to learn inherent prediction rules to predict labels for new query data, meta learning aims to learn the learning methodology for machine learning from observed tasks, so as to generalize to new query tasks by leveraging the meta-learned learning methodology. In this study, we interpret such learning methodology as learning an explicit hyper-parameter prediction function shared by all training tasks. Specifically, this function is represented as a parameterized function called meta-learner, mapping from a training/test task to its suitable hyper-parameter setting, extracted from a pre-specified function set called meta learning machine. Such setting guarantees that the meta-learned learning methodology is able to flexibly fit diverse query tasks, instead of only obtaining fixed hyper-parameters by many current meta learning methods, with less adaptability to query task's variations. Such understanding of meta learning also makes it easily succeed from traditional learning theory for analyzing its generalization bounds with general losses/tasks/models. The theory naturally leads to some feasible controlling strategies for ameliorating the quality of the extracted meta-learner, verified to be able to finely ameliorate its generalization capability in some typical meta learning applications, including few-shot regression, few-shot classification and domain generalization.",cs.LG,"Jun Shu, Deyu Meng, Zongben Xu",[]
,http://arxiv.org/abs/2004.11694v1,2020-04-18T19:39:58Z,,arXiv,cs.IR,http://arxiv.org/pdf/2004.11694v1.pdf,2004.11694v1,Identifying Semantically Duplicate Questions Using Data Science   Approach: A Quora Case Study,"Identifying semantically identical questions on, Question and Answering social media platforms like Quora is exceptionally significant to ensure that the quality and the quantity of content are presented to users, based on the intent of the question and thus enriching overall user experience. Detecting duplicate questions is a challenging problem because natural language is very expressive, and a unique intent can be conveyed using different words, phrases, and sentence structuring. Machine learning and deep learning methods are known to have accomplished superior results over traditional natural language processing techniques in identifying similar texts. In this paper, taking Quora for our case study, we explored and applied different machine learning and deep learning techniques on the task of identifying duplicate questions on Quora's dataset. By using feature engineering, feature importance techniques, and experimenting with seven selected machine learning classifiers, we demonstrated that our models outperformed previous studies on this task. Xgboost model with character level term frequency and inverse term frequency is our best machine learning model that has also outperformed a few of the Deep learning baseline models. We applied deep learning techniques to model four different deep neural networks of multiple layers consisting of Glove embeddings, Long Short Term Memory, Convolution, Max pooling, Dense, Batch Normalization, Activation functions, and model merge. Our deep learning models achieved better accuracy than machine learning models. Three out of four proposed architectures outperformed the accuracy from previous machine learning and deep learning research work, two out of four models outperformed accuracy from previous deep learning study on Quora's question pair dataset, and our best model achieved accuracy of 85.82% which is close to Quora state of the art accuracy.","cs.IR, cs.LG, stat.ML","Navedanjum Ansari, Rajesh Sharma",[]
,http://arxiv.org/abs/1607.01354v1,2016-03-22T18:46:13Z,,arXiv,cs.LG,http://arxiv.org/pdf/1607.01354v1.pdf,1607.01354v1,Learning Discriminative Features using Encoder-Decoder type Deep Neural   Nets,"As machine learning is applied to an increasing variety of complex problems, which are defined by high dimensional and complex data sets, the necessity for task oriented feature learning grows in importance. With the advancement of Deep Learning algorithms, various successful feature learning techniques have evolved. In this paper, we present a novel way of learning discriminative features by training Deep Neural Nets which have Encoder or Decoder type architecture similar to an Autoencoder. We demonstrate that our approach can learn discriminative features which can perform better at pattern classification tasks when the number of training samples is relatively small in size.","cs.LG, stat.ML, I.5; I.5.3","Vishwajeet Singh, Killamsetti Ravi Kumar, K Eswaran",[]
,http://arxiv.org/abs/1809.02591v2,2018-09-07T17:32:19Z,,arXiv,cs.LG,http://arxiv.org/pdf/1809.02591v2.pdf,1809.02591v2,Learning Invariances for Policy Generalization,"While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.","cs.LG, cs.AI, stat.ML","Remi Tachet, Philip Bachman, Harm van Seijen",[]
,http://arxiv.org/abs/1801.07883v2,2018-01-24T07:32:29Z,,arXiv,cs.CL,http://arxiv.org/pdf/1801.07883v2.pdf,1801.07883v2,Deep Learning for Sentiment Analysis : A Survey,"Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many other application domains, deep learning is also popularly used in sentiment analysis in recent years. This paper first gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.","cs.CL, cs.IR, cs.LG, stat.ML","Lei Zhang, Shuai Wang, Bing Liu",[]
,,,,Crossref,,,10.25080/majora-92bf1922-00a,,,,,[]
,,,,Crossref,,,10.1117/12.795737,,,,,[]
,,,,Crossref,,,10.1117/12.795570,,,,,[]
,,,,Crossref,,,10.1038/nphys3272,,,,,[]
,,,,Crossref,,,10.1137/18m120275x,,,,,[]
,,,,Crossref,,,10.1145/3313276.3316378,,,,,[]
,,,,Crossref,,,10.1162/089976698300017746,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/ac3e54,,,,,[]
,,,,Crossref,,,10.1088/2632-2153/abf3ac,,,,,[]
,,,,Crossref,,,10.1140/epjc/s10052-019-6607-9,,,,,[]
,,,,Crossref,,,10.22331/q-2021-10-05-558,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/ac7d06,,,,,[]
,,,,Crossref,,,10.1088/1367-2630/17/12/123010,,,,,[]
,,,,Crossref,,,10.22331/q-2020-04-20-256,,,,,[]
,,,,Crossref,,,10.21203/rs.3.rs-3023549/v1,,,,,[]
,,,,Crossref,,,10.1038/s41534-019-0157-8,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.3.033083,,,,,[]
,,,,Crossref,,,10.1038/nature23474,,,,,[]
,,,,Crossref,,,10.1007/s42484-023-00132-1,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.127.120502,,,,,[]
,,,,Crossref,,,10.22331/q-2023-11-22-1188,,,,,[]
,,,,Crossref,,,10.1103/physreva.105.062431,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/acb56a,,,,,[]
,,,,Crossref,,,10.1038/s41534-023-00801-w,,,,,[]
,,,,Crossref,,,10.22331/q-2023-07-13-1060,,,,,[]
,,,,Crossref,,,10.1038/s41467-021-21728-w,,,,,[]
,,,,Crossref,,,10.1038/s41534-022-00611-6,,,,,[]
,,,,Crossref,,,10.22331/q-2023-04-13-974,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.2.030348,,,,,[]
,,,,Crossref,,,10.3390/e20080583,,,,,[]
,,,,Crossref,,,10.1145/3357713.3384314,,,,,[]
,,,,Crossref,,,10.1137/16m1087072,,,,,[]
,,,,Crossref,,,10.1098/rspa.2017.0551,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.2.010324,,,,,[]
,,,,Crossref,,,10.1007/jhep11(2017)048,,,,,[]
,,,,Crossref,,,10.22331/q-2021-11-26-592,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.3.040329,,,,,[]
,,,,Crossref,,,10.22331/qv-2020-03-17-32,,,,,[]
,,,,Crossref,,,10.1038/s42254-022-00535-2,,,,,[]
,,,,Crossref,,,10.7566/jpsj.90.032001,,,,,[]
,,,,Crossref,,,10.1038/s41534-019-0240-1,,,,,[]
,,,,Crossref,,,10.1145/3313276.3316366,,,,,[]
,,,,Crossref,,,10.1088/1367-2630/14/10/103013,,,,,[]
,,,,Crossref,,,10.1038/s41467-019-10988-2,,,,,[]
,,,,Crossref,,,10.1145/237814.237866,,,,,[]
,,,,Crossref,,,10.1126/science.1257219,,,,,[]
,,,,Crossref,,,10.1016/j.physrep.2008.09.003,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.4.043100,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.123.250501,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.126.140502,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.103.150502,,,,,[]
,,,,Crossref,,,10.1137/1.9781611974782.105,,,,,[]
,,,,Crossref,,,10.1038/s41586-019-0980-2,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.3.010313,,,,,[]
,,,,Crossref,,,10.1038/s41567-020-0932-7,,,,,[]
,,,,Crossref,,,10.1038/s41467-021-22539-9,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.127.030503,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.126.190505,,,,,[]
,,,,Crossref,,,10.1126/science.abn7293,,,,,[]
,,,,Crossref,,,10.1126/science.abk3333,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/abdbc9,,,,,[]
,,,,Crossref,,,10.1561/2200000058,,,,,[]
,,,,Crossref,,,10.1038/s41467-023-36159-y,,,,,[]
,,,,Crossref,,,10.1038/nature23879,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.120.110501,,,,,[]
,,,,Crossref,,,10.22331/q-2022-08-16-776,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/abfc94,,,,,[]
,,,,Crossref,,,10.1063/1.5089550,,,,,[]
,,,,Crossref,,,10.1038/s42254-020-0230-4,,,,,[]
,,,,Crossref,,,10.1038/s43588-023-00467-6,,,,,[]
,,,,Crossref,,,10.1103/physreva.102.032420,,,,,[]
,,,,Crossref,,,10.1038/s41534-019-0167-6,,,,,[]
,,,,Crossref,,,10.1021/acs.jctc.8b01004,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.2.023074,,,,,[]
,,,,Crossref,,,10.1103/physrevd.98.086026,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.2.043164,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.3.030323,,,,,[]
,,,,Crossref,,,10.1073/pnas.2026805118,,,,,[]
,,,,Crossref,,,10.1038/s41567-021-01287-z,,,,,[]
,,,,Crossref,,,10.1126/science.273.5278.1073,,,,,[]
,,,,Crossref,,,10.1038/nphys3029,,,,,[]
,,,,Crossref,,,10.1103/physreva.101.010301,,,,,[]
,,,,Crossref,,,10.1038/s41534-019-0187-2,,,,,[]
,,,,Crossref,,,10.1088/1367-2630/18/2/023023,,,,,[]
,,,,Crossref,,,10.1038/s41467-018-07090-4,,,,,[]
,,,,Crossref,,,10.1038/nature00801,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.1.013006,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/aab822,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.2.043158,,,,,[]
,,,,Crossref,,,10.1038/nature08967,,,,,[]
,,,,Crossref,,,10.22331/q-2021-01-28-391,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.3.033090,,,,,[]
,,,,Crossref,,,10.22331/q-2020-02-06-226,,,,,[]
,,,,Crossref,,,10.1038/ncomms5213,,,,,[]
,,,,Crossref,,,10.22331/q-2018-08-06-79,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.113.130503,,,,,[]
,,,,Crossref,,,10.1007/jhep04(2017)121,,,,,[]
,,,,Crossref,,,10.1080/00107514.2019.1667078,,,,,[]
,,,,Crossref,,,10.1103/physreva.94.022342,,,,,[]
,,,,Crossref,,,10.1088/1367-2630/ab784c,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.128.180505,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.128.070501,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.128.070501aaaa,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.119.180511,,,,,[]
,,,,Crossref,,,10.22331/q-2020-05-25-269,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.2.010307,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.122.060504,,,,,[]
,,,,Crossref,,,10.1007/bf01609348,,,,,[]
,,,,Crossref,,,10.22331/q-2020-08-31-314,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.127.060503,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.2.043364,,,,,[]
,,,,Crossref,,,10.1002/qua.21198,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.119.180509,,,,,[]
,,,,Crossref,,,10.1038/s41467-024-49287-w,,,,,[]
,,,,Crossref,,,10.1088/1751-8121/abfac7,,,,,[]
,,,,Crossref,,,10.1038/s41467-021-27045-6,,,,,[]
,,,,Crossref,,,10.22331/q-2021-08-30-531,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.2.033446,,,,,[]
,,,,Crossref,,,10.1103/physreva.92.042303,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.112.190501,,,,,[]
,,,,Crossref,,,10.1088/1367-2630/17/2/022005,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.120.050502,,,,,[]
,,,,Crossref,,,10.21203/rs.3.rs-3691498/v1,,,,,[]
,,,,Crossref,,,10.1109/tkde.2019.2937491,,,,,[]
,,,,Crossref,,,10.1038/srep03589,,,,,[]
,,,,Crossref,,,10.1088/2632-2153/abcb50,,,,,[]
,,,,Crossref,,,10.1287/opre.13.1.82,,,,,[]
,,,,Crossref,,,10.1145/1961189.1961199,,,,,[]
,,,,Crossref,,,10.7551/mitpress/7503.003.0032,,,,,[]
,,,,Crossref,,,10.1016/s0167-5060(08)70731-3,,,,,[]
,,,,Crossref,,,10.1007/978-3-642-15880-3_28,,,,,[]
,,,,Crossref,,,10.1002/net.3230130205,,,,,[]
,,,,Crossref,,,10.1007/3-540-46014-4_31,,,,,[]
,,,,Crossref,,,10.1287/opre.38.4.619,,,,,[]
,,,,Crossref,,,10.1109/tpami.2014.2299812,,,,,[]
,,,,Crossref,,,10.1007/978-1-4419-9154-6,,,,,[]
,,,,Crossref,,,10.1287/opre.28.6.1450,,,,,[]
,,,,Crossref,,,10.1098/rsta.1909.0016,,,,,[]
,,,,Crossref,,,10.1145/1150402.1150486,,,,,[]
,,,,Crossref,,,10.1287/opre.39.4.553,,,,,[]
,,,,Crossref,,,10.1162/089976698300017467,,,,,[]
,,,,Crossref,,,10.1016/0305-0548(87)90035-9,,,,,[]
,,,,Crossref,,,10.2307/1914133,,,,,[]
,,,,Crossref,,,10.1109/ijcnn.2005.1555965,,,,,[]
,,,,Crossref,,,10.1145/956750.956786,,,,,[]
,,,,Crossref,,,10.1007/s10618-005-0005-7,,,,,[]
,,,,Crossref,,,10.1145/233269.233324,,,,,[]
,,,,Crossref,,,10.1111/j.2517-6161.1972.tb00899.x,,,,,[]
,,,,Crossref,,,10.5281/zenodo.4457577,,,,,[]
,,,,Crossref,,,10.18637/jss.v040.i08,,,,,[]
,,,,Crossref,,,10.1002/(sici)1097-0258(19990915/30)18:17/18<2529::aid-sim274>3.0.co;2-5,,,,,[]
,,,,Crossref,,,10.1001/jama.1982.03320430047030,,,,,[]
,,,,Crossref,,,10.1111/j.0006-341x.2000.00337.x,,,,,[]
,,,,Crossref,,,10.1214/08-aoas169,,,,,[]
,,,,Crossref,,,10.21105/joss.01903,,,,,[]
,,,,Crossref,,,10.1002/sim.1203,,,,,[]
,,,,Crossref,,,10.1111/j.1541-0420.2010.01459.x,,,,,[]
,,,,Crossref,,,10.1002/sim.4154,,,,,[]
,,,,Crossref,,,10.1016/j.artmed.2011.06.006,,,,,[]
,,,,Crossref,,,10.1002/1097-0258(20001230)19:24<3401::aid-sim554>3.0.co;2-2,,,,,[]
,,,,Crossref,,,10.1109/access.2018.2870052,,,,,[]
,,,,Crossref,,,10.1016/j.ijhm.2019.01.003,,,,,[]
,,,,Crossref,,,10.24963/ijcai.2019/932,,,,,[]
,,,,Crossref,,,10.1016/j.matcom.2008.01.028,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2005.177,,,,,[]
,,,,Crossref,,,10.1016/0167-8655(94)90052-3,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2018.00175,,,,,[]
,,,,Crossref,,,10.1007/s12525-019-00384-5,,,,,[]
,,,,Crossref,,,10.1145/2523813,,,,,[]
,,,,Crossref,,,10.1016/j.cirpj.2015.05.004,,,,,[]
,,,,Crossref,,,10.1002/rob.21918,,,,,[]
,,,,Crossref,,,10.1002/9780470939376.ch25,,,,,[]
,,,,Crossref,,,10.1016/j.dss.2021.113494,,,,,[]
,,,,Crossref,,,10.1109/arso.2017.8025197,,,,,[]
,,,,Crossref,,,10.1016/j.procs.2018.10.340,,,,,[]
,,,,Crossref,,,10.1126/science.aaa8415,,,,,[]
,,,,Crossref,,,10.1007/s10462-007-9052-3,,,,,[]
,,,,Crossref,,,10.1007/s12525-019-00351-0,,,,,[]
,,,,Crossref,,,10.1038/nature14539,,,,,[]
,,,,Crossref,,,10.3390/proceedings47010009,,,,,[]
,,,,Crossref,,,10.1007/978-981-15-5573-2,,,,,[]
,,,,Crossref,,,10.1023/b:visi.0000029664.99615.94,,,,,[]
,,,,Crossref,,,10.1038/s41746-017-0013-1,,,,,[]
,,,,Crossref,,,10.1016/j.artint.2018.07.007,,,,,[]
,,,,Crossref,,,10.1109/access.2019.2905015,,,,,[]
,,,,Crossref,,,10.1109/icmla.2016.0172,,,,,[]
,,,,Crossref,,,10.25300/misq/2020/14458,,,,,[]
,,,,Crossref,,,10.1007/s10994-013-5340-0,,,,,[]
,,,,Crossref,,,10.1145/3234150,,,,,[]
,,,,Crossref,,,10.1016/j.procs.2018.10.326,,,,,[]
,,,,Crossref,,,10.1038/s42256-019-0048-x,,,,,[]
,,,,Crossref,,,10.1016/0306-4573(88)90021-0,,,,,[]
,,,,Crossref,,,10.1016/j.neunet.2014.09.003,,,,,[]
,,,,Crossref,,,10.1017/s0140525x00005756,,,,,[]
,,,,Crossref,,,10.1007/s12525-019-00393-4,,,,,[]
,,,,Crossref,,,10.2307/23042796,,,,,[]
,,,,Crossref,,,10.1016/j.jbusres.2020.09.068,,,,,[]
,,,,Crossref,,,10.1126/science.aar6404,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2001.990517,,,,,[]
,,,,Crossref,,,10.1109/tsc.2020.3000900,,,,,[]
,,,,Crossref,,,10.22215/timreview/1282,,,,,[]
,,,,Crossref,,,10.1007/bf00116900,,,,,[]
,,,,Crossref,,,10.18653/v1/d18-1310,,,,,[]
,,,,Crossref,,,10.1109/mci.2018.2840738,,,,,[]
,,,,Crossref,,,10.1038/s41524-018-0081-z,,,,,[]
,,,,Crossref,,,10.1007/bf00994018,,,,,[]
,,,,Crossref,,,10.1016/0040-5809(70)90039-0,,,,,[]
,,,,Crossref,,,10.1016/0040-5809(90)90025-q,,,,,[]
,,,,Crossref,,,10.1103/physreve.99.052111,,,,,[]
,,,,Crossref,,,10.1086/282505,,,,,[]
,,,,Crossref,,,10.2307/1934144,,,,,[]
,,,,Crossref,,,10.1023/b:mach.0000008084.60811.49,,,,,[]
,,,,Crossref,,,10.1609/aaai.v33i01.33013991,,,,,[]
,,,,Crossref,,,10.1017/cbo9780511804441,,,,,[]
,,,,Crossref,,,10.1111/j.1600-0706.2012.20830.x,,,,,[]
,,,,Crossref,,,10.1016/s0169-5347(02)02495-3,,,,,[]
,,,,Crossref,,,10.1073/pnas.87.24.9610,,,,,[]
,,,,Crossref,,,10.1109/nnsp.2003.1318049,,,,,[]
,,,,Crossref,,,10.1109/icpr.2018.8545819,,,,,[]
,,,,Crossref,,,10.1038/s41467-018-04593-y,,,,,[]
,,,,Crossref,,,10.1109/5.726791,,,,,[]
,,,,Crossref,,,10.1016/j.jmb.2015.10.019,,,,,[]
,,,,Crossref,,,10.1098/rspa.2016.0822,,,,,[]
,,,,Crossref,,,10.1080/00107514.2014.964942,,,,,[]
,,,,Crossref,,,10.1038/s41586-019-1666-5,,,,,[]
,,,,Crossref,,,10.1088/1367-2630/aae94a,,,,,[]
,,,,Crossref,,,10.1038/s41467-018-06847-1,,,,,[]
,,,,Crossref,,,10.1038/s41567-019-0648-8,,,,,[]
,,,,Crossref,,,10.1145/3313276.3316310,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.2.040321,,,,,[]
,,,,Crossref,,,10.1103/revmodphys.89.035002,,,,,[]
,,,,Crossref,,,10.1038/nphoton.2011.35,,,,,[]
,,,,Crossref,,,10.1103/physreva.80.022339,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.122.040504,,,,,[]
,,,,Crossref,,,10.1103/physreva.103.032430,,,,,[]
,,,,Crossref,,,10.1103/physreva.52.r2493,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-83098-4_6,,,,,[]
,,,,Crossref,,,10.1038/s41586-021-03242-7,,,,,[]
,,,,Crossref,,,10.22331/q-2022-05-24-720,,,,,[]
,,,,Crossref,,,10.1038/s41467-022-32550-3,,,,,[]
,,,,Crossref,,,10.2172/2377336,,,,,[]
,,,,Crossref,,,10.22331/q-2021-11-17-582,,,,,[]
,,,,Crossref,,,10.1038/s42254-021-00348-9,,,,,[]
,,,,Crossref,,,10.1038/s41534-017-0032-4,,,,,[]
,,,,Crossref,,,10.1038/s41467-020-14454-2,,,,,[]
,,,,Crossref,,,10.1007/s11128-014-0809-8,,,,,[]
,,,,Crossref,,,10.1103/physreva.98.012324,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.1.033063,,,,,[]
,,,,Crossref,,,10.22331/q-2020-05-11-263,,,,,[]
,,,,Crossref,,,10.1038/s43588-021-00084-1,,,,,[]
,,,,Crossref,,,10.1038/323533a0,,,,,[]
,,,,Crossref,,,10.1109/focs52979.2021.00063,,,,,[]
,,,,Crossref,,,10.1038/s41597-022-01639-1,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/abf51a,,,,,[]
,,,,Crossref,,,10.1088/2058-9565/abd891,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.126.190501,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.2.040316,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.3.030341,,,,,[]
,,,,Crossref,,,10.1038/s41534-023-00710-y,,,,,[]
,,,,Crossref,,,10.22331/q-2022-09-29-824,,,,,[]
,,,,Crossref,,,10.1038/s41534-021-00425-y,,,,,[]
,,,,Crossref,,,10.1103/physrevresearch.4.013083,,,,,[]
,,,,Crossref,,,10.1038/nphys4074,,,,,[]
,,,,Crossref,,,10.1038/s41534-018-0082-2,,,,,[]
,,,,Crossref,,,10.1038/s41534-020-00302-0,,,,,[]
,,,,Crossref,,,10.1038/s41467-021-27922-0,,,,,[]
,,,,Crossref,,,10.1103/physrev.136.b864,,,,,[]
,,,,Crossref,,,10.1103/revmodphys.71.1253,,,,,[]
,,,,Crossref,,,10.1088/2632-2153/ab9009,,,,,[]
,,,,Crossref,,,10.1201/9780203881095,,,,,[]
,,,,Crossref,,,10.1137/s0097539796300921,,,,,[]
,,,,Crossref,,,10.1103/prxquantum.2.010103,,,,,[]
,,,,Crossref,,,10.1103/revmodphys.86.153,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.114.090502,,,,,[]
,,,,Crossref,,,10.1038/nphoton.2009.231,,,,,[]
,,,,Crossref,,,10.1126/science.aat2663,,,,,[]
,,,,Crossref,,,10.1161/circulationaha.115.001593,,,,,[]
,,,,Crossref,,,10.1145/2347736.2347755,,,,,[]
,,,,Crossref,,,10.1038/d41586-019-02307-y,,,,,[]
,,,,Crossref,,,10.1126/science.293.5537.2051,,,,,[]
,,,,Crossref,,,10.1137/141000671,,,,,[]
,,,,Crossref,,,10.1038/d41586-019-02310-3,,,,,[]
,,,,Crossref,,,10.1016/j.jpdc.2020.01.003,,,,,[]
,,,,Crossref,,,10.1109/tpds.2018.2872064,,,,,[]
,,,,Crossref,,,10.1109/bigdata47090.2019.9005453,,,,,[]
,,,,Crossref,,,10.1137/16m1081063,,,,,[]
,,,,Crossref,,,10.1016/j.neuroimage.2007.01.058,,,,,[]
,,,,Crossref,,,10.1371/journal.pone.0193981,,,,,[]
,,,,Crossref,,,10.1186/s12859-019-2824-3,,,,,[]
,,,,Crossref,,,10.18637/jss.v035.i04,,,,,[]
,,,,Crossref,,,10.1016/j.jneumeth.2011.10.025,,,,,[]
,,,,Crossref,,,10.1016/j.ijar.2018.11.002,,,,,[]
,,,,Crossref,,,10.1088/1749-4699/8/1/014008,,,,,[]
,,,,Crossref,,,10.1023/a:1010933404324,,,,,[]
,,,,Crossref,,,10.3389/fgene.2019.00579,,,,,[]
,,,,Crossref,,,10.1016/j.landurbplan.2017.05.010,,,,,[]
,,,,Crossref,,,10.1007/s10107-010-0420-4,,,,,[]
,,,,Crossref,,,10.1109/jas.2019.1911825,,,,,[]
,,,,Crossref,,,10.1049/iet-its.2018.5144,,,,,[]
,,,,Crossref,,,10.3389/fninf.2014.00014,,,,,[]
,,,,Crossref,,,10.1016/j.neucom.2019.08.022,,,,,[]
,,,,Crossref,,,10.1109/tits.2019.2892405,,,,,[]
,,,,Crossref,,,10.1007/3-540-63930-6_173,,,,,[]
,,,,Crossref,,,10.1016/j.compbiomed.2017.12.003,,,,,[]
,,,,Crossref,,,10.3847/1538-3881/aa68a1,,,,,[]
,,,,Crossref,,,10.1080/10556788.2018.1435651,,,,,[]
,,,,Crossref,,,10.1016/j.ins.2018.06.056,,,,,[]
,,,,Crossref,,,10.1109/tpami.2011.255,,,,,[]
,,,,Crossref,,,10.1109/jiot.2019.2903191,,,,,[]
,,,,Crossref,,,10.1093/nar/16.22.10881,,,,,[]
,,,,Crossref,,,10.1007/bf02289588,,,,,[]
,,,,Crossref,,,10.1109/2.781637,,,,,[]
,,,,Crossref,,,10.1093/bioinformatics/btt626,,,,,[]
,,,,Crossref,,,10.1109/lra.2019.2926677,,,,,[]
,,,,Crossref,,,10.1016/j.compbiomed.2007.11.001,,,,,[]
,,,,Crossref,,,10.1016/j.csda.2018.01.014,,,,,[]
,,,,Crossref,,,10.1093/bioinformatics/bty056,,,,,[]
,,,,Crossref,,,10.1007/s00422-001-0298-6,,,,,[]
,,,,Crossref,,,10.1007/s11517-006-0051-3,,,,,[]
,,,,Crossref,,,10.1016/0167-2789(90)90081-y,,,,,[]
,,,,Crossref,,,10.1145/3065386,,,,,[]
,,,,Crossref,,,10.1186/s12859-015-0793-8,,,,,[]
,,,,Crossref,,,10.1109/tnnls.2015.2424995,,,,,[]
,,,,Crossref,,,10.1109/tmc.2019.2892451,,,,,[]
,,,,Crossref,,,10.1016/j.bushor.2015.03.008,,,,,[]
,,,,Crossref,,,10.1016/j.future.2013.01.010,,,,,[]
,,,,Crossref,,,10.1016/j.comnet.2010.05.010,,,,,[]
,,,,Crossref,,,10.1109/comst.2018.2844341,,,,,[]
,,,,Crossref,,,10.1016/j.dcan.2017.10.002,,,,,[]
,,,,Crossref,,,10.1083/jcb.201610026,,,,,[]
,,,,Crossref,,,10.1073/pnas.1619003114,,,,,[]
,,,,Crossref,,,10.1016/j.gsf.2014.10.005,,,,,[]
,,,,Crossref,,,10.1016/j.conbuildmat.2017.09.110,,,,,[]
,,,,Crossref,,,10.1016/j.engstruct.2016.11.038,,,,,[]
,,,,Crossref,,,10.1126/science.aaa8685,,,,,[]
,,,,Crossref,,,10.1016/j.knosys.2016.06.009,,,,,[]
,,,,Crossref,,,10.1016/j.neucom.2019.08.096,,,,,[]
,,,,Crossref,,,10.1016/j.neucom.2016.12.038,,,,,[]
,,,,Crossref,,,10.1007/s11042-018-6986-1,,,,,[]
,,,,Crossref,,,10.1109/tits.2011.2157145,,,,,[]
,,,,Crossref,,,10.1049/iet-com.2016.1290,,,,,[]
,,,,Crossref,,,10.1016/j.patcog.2007.08.018,,,,,[]
,,,,Crossref,,,10.1111/rssb.12377,,,,,[]
,,,,Crossref,,,10.1145/3351095.3375624,,,,,[]
,,,,Crossref,,,10.1097/acm.0000000000001294,,,,,[]
,,,,Crossref,,,10.23915/distill.00015,,,,,[]
,,,,Crossref,,,10.1145/3511299,,,,,[]
,,,,Crossref,,,10.1086/709729,,,,,[]
,,,,Crossref,,,10.1145/3561048,,,,,[]
,,,,Crossref,,,10.1093/acprof:oso/9780199381104.001.0001,,,,,[]
,,,,Crossref,,,10.1214/aos/1013203451,,,,,[]
,,,,Crossref,,,10.1609/aaai.v33i01.33013681,,,,,[]
,,,,Crossref,,,10.1007/s10618-022-00831-6,,,,,[]
,,,,Crossref,,,10.1145/3351095.3372826,,,,,[]
,,,,Crossref,,,10.1093/acprof:oso/9780199892631.001.0001,,,,,[]
,,,,Crossref,,,10.1007/s11098-014-0434-5,,,,,[]
,,,,Crossref,,,10.1111/josp.12373,,,,,[]
,,,,Crossref,,,10.1080/1369118x.2019.1573912,,,,,[]
,,,,Crossref,,,10.1007/978-3-031-04083-2_2,,,,,[]
,,,,Crossref,,,10.1016/j.artint.2021.103571,,,,,[]
,,,,Crossref,,,10.1145/3442188.3445866,,,,,[]
,,,,Crossref,,,10.1145/3514094.3534188,,,,,[]
,,,,Crossref,,,10.1145/3461702.3462606,,,,,[]
,,,,Crossref,,,10.1145/3442188.3445886,,,,,[]
,,,,Crossref,,,10.1038/d41586-019-03228-6,,,,,[]
,,,,Crossref,,,10.2139/ssrn.3837493,,,,,[]
,,,,Crossref,,,10.5206/fpq/2022.3/4.14191,,,,,[]
,,,,Crossref,,,10.1145/3236386.3241340,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-64949-4_1,,,,,[]
,,,,Crossref,,,10.1145/3287560.3287574,,,,,[]
,,,,Crossref,,,10.1007/978-3-031-04083-2_4,,,,,[]
,,,,Crossref,,,10.1145/3359992.3366639,,,,,[]
,,,,Crossref,,,10.1145/3351095.3372850,,,,,[]
,,,,Crossref,,,10.1126/science.aax2342,,,,,[]
,,,,Crossref,,,10.1056/nejmp1811499,,,,,[]
,,,,Crossref,,,10.1145/3351095.3372873,,,,,[]
,,,,Crossref,,,10.1109/satml54575.2023.00039,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-56286-1_3,,,,,[]
,,,,Crossref,,,10.1145/2939672.2939778,,,,,[]
,,,,Crossref,,,10.1093/biostatistics/kxz040,,,,,[]
,,,,Crossref,,,10.1016/j.knosys.2023.110273,,,,,[]
,,,,Crossref,,,10.2307/j.ctv2g591sq,,,,,[]
,,,,Crossref,,,10.1007/s11229-022-03485-5,,,,,[]
,,,,Crossref,,,10.1017/s0265052506060043,,,,,[]
,,,,Crossref,,,10.1145/96267.96279,,,,,[]
,,,,Crossref,,,10.1109/msp.2005.55,,,,,[]
,,,,Crossref,,,10.1109/acsac.2007.27,,,,,[]
,,,,Crossref,,,10.1145/3140587.3062349,,,,,[]
,,,,Crossref,,,10.1145/2970276.2970321,,,,,[]
,,,,Crossref,,,10.1007/s10664-015-9422-4,,,,,[]
,,,,Crossref,,,10.1186/s42400-018-0002-y,,,,,[]
,,,,Crossref,,,10.1145/3180445.3180453,,,,,[]
,,,,Crossref,,,10.1145/2857705.2857720,,,,,[]
,,,,Crossref,,,10.1109/compcomm.2017.8322752,,,,,[]
,,,,Crossref,,,10.1109/ase.2017.8115618,,,,,[]
,,,,Crossref,,,10.1109/sp.2019.00052,,,,,[]
,,,,Crossref,,,10.1109/sp.2017.23,,,,,[]
,,,,Crossref,,,10.1145/1064978.1065034,,,,,[]
,,,,Crossref,,,10.1145/2499370.2462173,,,,,[]
,,,,Crossref,,,10.1049/sej.1995.0010,,,,,[]
,,,,Crossref,,,10.1145/1180405.1180445,,,,,[]
,,,,Crossref,,,10.1145/1961295.1950396,,,,,[]
,,,,Crossref,,,10.1145/1379022.1375607,,,,,[]
,,,,Crossref,,,10.1145/2699026.2699098,,,,,[]
,,,,Crossref,,,10.1145/360248.360252,,,,,[]
,,,,Crossref,,,10.1109/dsn.2009.5270315,,,,,[]
,,,,Crossref,,,10.1145/2560217.2560219,,,,,[]
,,,,Crossref,,,10.1109/sp.2012.31,,,,,[]
,,,,Crossref,,,10.14722/ndss.2015.23294,,,,,[]
,,,,Crossref,,,10.1145/2927924,,,,,[]
,,,,Crossref,,,10.1145/1966445.1966463,,,,,[]
,,,,Crossref,,,10.1109/icnidc.2014.7000307,,,,,[]
,,,,Crossref,,,10.1007/978-3-642-13986-4_7,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-72389-1_24,,,,,[]
,,,,Crossref,,,10.1109/pac.2017.10,,,,,[]
,,,,Crossref,,,10.1109/apsec.2017.30,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-89500-0_53,,,,,[]
,,,,Crossref,,,10.1145/3203217.3203241,,,,,[]
,,,,Crossref,,,10.1145/3270101.3270108,,,,,[]
,,,,Crossref,,,10.1145/3213846.3213848,,,,,[]
,,,,Crossref,,,10.1109/compsac.2018.00098,,,,,[]
,,,,Crossref,,,10.1109/spw.2018.00026,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-12146-4_22,,,,,[]
,,,,Crossref,,,10.5220/0006836604720481,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-98989-1_2,,,,,[]
,,,,Crossref,,,10.1109/tencon.2018.8650188,,,,,[]
,,,,Crossref,,,10.1109/icstw.2019.00065,,,,,[]
,,,,Crossref,,,10.1109/tcyb.2020.3013675,,,,,[]
,,,,Crossref,,,10.1109/access.2019.2911121,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-29959-0_13,,,,,[]
,,,,Crossref,,,10.1609/aaai.v33i01.33011044,,,,,[]
,,,,Crossref,,,10.1109/access.2019.2903291,,,,,[]
,,,,Crossref,,,10.1109/icst.2019.00016,,,,,[]
,,,,Crossref,,,10.14722/ndss.2019.23525,,,,,[]
,,,,Crossref,,,10.1145/3319535.3363230,,,,,[]
,,,,Crossref,,,10.1109/ukrcon.2019.8879997,,,,,[]
,,,,Crossref,,,10.1109/iccsnt47585.2019.8962499,,,,,[]
,,,,Crossref,,,10.1109/icse-companion.2019.00096,,,,,[]
,,,,Crossref,,,10.1109/ase.2019.00093,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-42921-8_8,,,,,[]
,,,,Crossref,,,10.1109/icst.2019.00029,,,,,[]
,,,,Crossref,,,10.3390/s20072040,,,,,[]
,,,,Crossref,,,10.1561/2000000039,,,,,[]
,,,,Crossref,,,10.1109/tnn.1998.712192,,,,,[]
,,,,Crossref,,,10.1109/72.788640,,,,,[]
,,,,Crossref,,,10.1198/tech.2007.s518,,,,,[]
,,,,Crossref,,,10.1145/319382.319388,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-40667-1_20,,,,,[]
,,,,Crossref,,,10.1016/j.asoc.2019.105598,,,,,[]
,,,,Crossref,,,10.4108/eai.3-12-2015.2262516,,,,,[]
,,,,Crossref,,,10.1145/1299015.1299021,,,,,[]
,,,,Crossref,,,10.1145/1242572.1242660,,,,,[]
,,,,Crossref,,,10.1109/sere.2013.26,,,,,[]
,,,,Crossref,,,10.1109/3pgcic.2014.100,,,,,[]
,,,,Crossref,,,10.1098/rsta.1894.0003,,,,,[]
,,,,Crossref,,,10.1198/tas.2009.0033,,,,,[]
,,,,Crossref,,,10.1145/383952.383995,,,,,[]
,,,,Crossref,,,10.1109/sp.2016.15,,,,,[]
,,,,Crossref,,,10.1016/s0375-9601(00)00725-8,,,,,[]
,,,,Crossref,,,10.1145/1993498.1993532,,,,,[]
,,,,Crossref,,,10.14722/ndss.2019.23371,,,,,[]
,,,,Crossref,,,10.14722/ndss.2019.23504,,,,,[]
,,,,Crossref,,,10.1109/sp.2018.00046,,,,,[]
,,,,Crossref,,,10.1109/access.2019.2894178,,,,,[]
,,,,Crossref,,,10.1109/sp.2018.00056,,,,,[]
,,,,Crossref,,,10.1109/access.2018.2851237,,,,,[]
,,,,Crossref,,,10.1016/j.infsof.2011.09.002,,,,,[]
,,,,Crossref,,,10.1016/j.eswa.2011.03.041,,,,,[]
,,,,Crossref,,,10.1038/521435a,,,,,[]
,,,,Crossref,,,10.1126/science.349.6245.248,,,,,[]
,,,,Crossref,,,10.1038/nature16961,,,,,[]
,,,,Crossref,,,10.1007/s10994-012-5316-5,,,,,[]
,,,,Crossref,,,10.1109/tsmcb.2008.925743,,,,,[]
,,,,Crossref,,,10.1103/physrevx.4.031002,,,,,[]
,,,,Crossref,,,10.1103/physreva.90.032310,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.110.220501,,,,,[]
,,,,Crossref,,,10.1038/srep12874,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.116.090405,,,,,[]
,,,,Crossref,,,10.1137/040605072,,,,,[]
,,,,Crossref,,,10.1038/srep00400,,,,,[]
,,,,Crossref,,,10.1109/access.2016.2556579,,,,,[]
,,,,Crossref,,,10.1002/(sici)1521-3978(199806)46:4/5&lt;493::aid-prop493&gt;3.0.co;2-p,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.113.210501,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.100.160501,,,,,[]
,,,,Crossref,,,10.1561/2200000044,,,,,[]
,,,,Crossref,,,10.1109/tpami.2009.167,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2013.251,,,,,[]
,,,,Crossref,,,10.1145/1143844.1143980,,,,,[]
,,,,Crossref,,,10.1109/tgrs.2017.2730583,,,,,[]
,,,,Crossref,,,10.1109/iccv.2011.6126552,,,,,[]
,,,,Crossref,,,10.1007/978-3-642-33715-4_1,,,,,[]
,,,,Crossref,,,10.1109/tgrs.2017.2675902,,,,,[]
,,,,Crossref,,,10.1109/tgrs.2017.2748120,,,,,[]
,,,,Crossref,,,10.1145/2783258.2783264,,,,,[]
,,,,Crossref,,,10.1023/b:visi.0000013087.49260.fb,,,,,[]
,,,,Crossref,,,10.1109/taslp.2016.2536478,,,,,[]
,,,,Crossref,,,10.1109/tgrs.2019.2893180,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2018.00831,,,,,[]
,,,,Crossref,,,10.3390/rs11010076,,,,,[]
,,,,Crossref,,,10.1145/1963405.1963415,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-46466-4_20,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-01219-9_32,,,,,[]
,,,,Crossref,,,10.1016/j.patcog.2018.03.003,,,,,[]
,,,,Crossref,,,10.1016/j.sigpro.2014.11.010,,,,,[]
,,,,Crossref,,,10.1109/tpami.2014.2353628,,,,,[]
,,,,Crossref,,,10.1109/tpami.2010.231,,,,,[]
,,,,Crossref,,,10.1145/2124295.2124329,,,,,[]
,,,,Crossref,,,10.1145/2396761.2396857,,,,,[]
,,,,Crossref,,,10.1145/2020408.2020479,,,,,[]
,,,,Crossref,,,10.1109/tpami.2016.2562626,,,,,[]
,,,,Crossref,,,10.1109/iccv.2015.312,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2017.779,,,,,[]
,,,,Crossref,,,10.1145/1390334.1390446,,,,,[]
,,,,Crossref,,,10.1016/j.inffus.2018.01.006,,,,,[]
,,,,Crossref,,,10.1109/icip.2016.7533164,,,,,[]
,,,,Crossref,,,10.1007/978-3-030-01252-6_2,,,,,[]
,,,,Crossref,,,10.1145/2020408.2020573,,,,,[]
,,,,Crossref,,,10.1145/1060745.1060754,,,,,[]
,,,,Crossref,,,10.1016/j.inffus.2018.07.011,,,,,[]
,,,,Crossref,,,10.24963/ijcai.2017/296,,,,,[]
,,,,Crossref,,,10.1023/a:1022859003006,,,,,[]
,,,,Crossref,,,10.1109/icmla.2016.0021,,,,,[]
,,,,Crossref,,,10.1145/2339530.2339617,,,,,[]
,,,,Crossref,,,10.3389/fncom.2015.00104,,,,,[]
,,,,Crossref,,,10.1016/j.neucom.2016.08.060,,,,,[]
,,,,Crossref,,,10.1109/ssp.2016.7551844,,,,,[]
,,,,Crossref,,,10.1109/tpami.2009.144,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-23528-8_38,,,,,[]
,,,,Crossref,,,10.1109/icccyb.2008.4721376,,,,,[]
,,,,Crossref,,,10.1023/a:1007607513941,,,,,[]
,,,,Crossref,,,10.1016/s0262-8856(01)00045-2,,,,,[]
,,,,Crossref,,,10.1109/tcsvt.2013.2276704,,,,,[]
,,,,Crossref,,,10.1109/34.709601,,,,,[]
,,,,Crossref,,,10.1016/j.patcog.2016.07.038,,,,,[]
,,,,Crossref,,,10.1109/iccv.2017.59,,,,,[]
,,,,Crossref,,,10.1109/igarss.2018.8517748,,,,,[]
,,,,Crossref,,,10.1016/j.ins.2013.12.016,,,,,[]
,,,,Crossref,,,10.1007/s10489-012-0342-3,,,,,[]
,,,,Crossref,,,10.1109/tip.2011.2169274,,,,,[]
,,,,Crossref,,,10.1109/tkde.2015.2433262,,,,,[]
,,,,Crossref,,,10.1016/j.patcog.2016.08.026,,,,,[]
,,,,Crossref,,,10.1145/1869790.1869829,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-11179-7_53,,,,,[]
,,,,Crossref,,,10.1109/ijcnn.2016.7727538,,,,,[]
,,,,Crossref,,,10.1111/rssb.12096,,,,,[]
,,,,Crossref,,,10.1109/tgrs.2018.2886022,,,,,[]
,,,,Crossref,,,10.24963/ijcai.2017/241,,,,,[]
,,,,Crossref,,,10.1109/tcsvt.2015.2400779,,,,,[]
,,,,Crossref,,,10.1007/s10994-006-9449-2,,,,,[]
,,,,Crossref,,,10.1109/tnnls.2017.2785403,,,,,[]
,,,,Crossref,,,10.1109/tpami.2015.2400461,,,,,[]
,,,,Crossref,,,10.1016/j.inffus.2004.04.005,,,,,[]
,,,,Crossref,,,10.1016/j.inffus.2004.04.004,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2017.36,,,,,[]
,,,,Crossref,,,10.1016/j.patcog.2017.01.012,,,,,[]
,,,,Crossref,,,10.1007/s10044-002-0173-7,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.110.018701,,,,,[]
,,,,Crossref,,,10.1109/iccv.2015.211,,,,,[]
,,,,Crossref,,,10.1007/bf01588971,,,,,[]
,,,,Crossref,,,10.1007/978-3-319-20248-8_18,,,,,[]
,,,,Crossref,,,10.1145/956750.956769,,,,,[]
,,,,Crossref,,,10.1007/s11263-011-0439-x,,,,,[]
,,,,Crossref,,,10.1109/tpami.2012.79,,,,,[]
,,,,Crossref,,,10.1145/1498759.1498766,,,,,[]
,,,,Crossref,,,10.1109/tkde.2012.170,,,,,[]
,,,,Crossref,,,10.1145/1860702.1860709,,,,,[]
,,,,Crossref,,,10.1109/tip.2014.2327805,,,,,[]
,,,,Crossref,,,10.1007/s11263-014-0781-x,,,,,[]
,,,,Crossref,,,10.1126/science.287.5456.1273,,,,,[]
,,,,Crossref,,,10.1080/095400996116820,,,,,[]
,,,,Crossref,,,10.1038/381607a0,,,,,[]
,,,,Crossref,,,10.1371/journal.pcbi.1002250,,,,,[]
,,,,Crossref,,,10.1016/j.inffus.2013.11.003,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2018.00564,,,,,[]
,,,,Crossref,,,10.1109/mgrs.2013.2244672,,,,,[]
,,,,Crossref,,,10.1007/s10618-011-0243-9,,,,,[]
,,,,Crossref,,,10.1016/s0004-3702(02)00190-x,,,,,[]
,,,,Crossref,,,10.1109/ijcnn.2017.7966354,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2013.377,,,,,[]
,,,,Crossref,,,10.1109/cvpr.2014.146,,,,,[]
,,,,Crossref,,,10.1109/infocom41043.2020.9155363,,,,,[]
,,,,Crossref,,,10.32614/cran.package.doubleml,,,,,[]
,,,,Crossref,,,10.1214/17-aos1671,,,,,[]
,,,,Crossref,,,10.1093/restud/rdt044,,,,,[]
,,,,Crossref,,,10.1093/biomet/asu056,,,,,[]
,,,,Crossref,,,10.1145/3357223.3362711,,,,,[]
,,,,Crossref,,,10.1093/ectj/utaa001,,,,,[]
,,,,Crossref,,,10.1111/ectj.12097,,,,,[]
,,,,Crossref,,,10.1016/j.jeconom.2020.09.003,,,,,[]
,,,,Crossref,,,10.1111/jofi.12883,,,,,[]
,,,,Crossref,,,10.1109/ic2e.2018.00052,,,,,[]
,,,,Crossref,,,10.1145/3127479.3128601,,,,,[]
,,,,Crossref,,,10.1145/3341105.3373948,,,,,[]
,,,,Crossref,,,10.1109/ic2e48712.2020.00023,,,,,[]
,,,,Crossref,,,10.2307/1912705,,,,,[]
,,,,Crossref,,,10.1073/pnas.2015577118,,,,,[]
,,,,Crossref,,,10.18653/v1/n18-5002,,,,,[]
,,,,Crossref,,,10.1145/3185768.3186308,,,,,[]
,,,,Crossref,,,10.1145/3154847.3154848,,,,,[]
,,,,Crossref,,,10.1109/infocom.2019.8737391,,,,,[]
,,,,Crossref,,,10.1021/acs.chemrev.1c00107,,,,,[]
,,,,Crossref,,,10.1038/s41524-019-0221-0,,,,,[]
,,,,Crossref,,,10.1038/s41467-020-18556-9,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.98.146401,,,,,[]
,,,,Crossref,,,10.1063/1.5019779,,,,,[]
,,,,Crossref,,,10.1039/c6sc05720a,,,,,[]
,,,,Crossref,,,10.1103/physrevb.96.014112,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.104.136403,,,,,[]
,,,,Crossref,,,10.1126/sciadv.1603015,,,,,[]
,,,,Crossref,,,10.1021/acs.chemrev.0c01111,,,,,[]
,,,,Crossref,,,10.1038/ncomms15679,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.120.145301,,,,,[]
,,,,Crossref,,,10.1021/cm100795d,,,,,[]
,,,,Crossref,,,10.1038/npjcompumats.2016.28,,,,,[]
,,,,Crossref,,,10.1038/s41524-018-0128-1,,,,,[]
,,,,Crossref,,,10.1016/j.matt.2019.08.017,,,,,[]
,,,,Crossref,,,10.1021/jp960669l,,,,,[]
,,,,Crossref,,,10.1063/1.4704546,,,,,[]
,,,,Crossref,,,10.1021/ct800531s,,,,,[]
,,,,Crossref,,,10.1021/cr200107z,,,,,[]
,,,,Crossref,,,10.1063/1.4869598,,,,,[]
,,,,Crossref,,,10.1016/j.trechm.2020.02.005,,,,,[]
,,,,Crossref,,,10.1002/wcms.1631,,,,,[]
,,,,Crossref,,,10.1103/physrevb.45.13244,,,,,[]
,,,,Crossref,,,10.1103/physrevb.23.5048,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.45.566,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.100.136406,,,,,[]
,,,,Crossref,,,10.1103/physrevb.73.235116,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.80.890,,,,,[]
,,,,Crossref,,,10.1103/physrevb.59.7413,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.91.146401,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.115.036402,,,,,[]
,,,,Crossref,,,10.1073/pnas.1705670114,,,,,[]
,,,,Crossref,,,10.1021/acs.jctc.8b00288,,,,,[]
,,,,Crossref,,,10.1002/jcc.26732,,,,,[]
,,,,Crossref,,,10.1103/physrevmaterials.6.083803,,,,,[]
,,,,Crossref,,,10.1063/1.464304,,,,,[]
,,,,Crossref,,,10.1063/1.1383587,,,,,[]
,,,,Crossref,,,10.1016/s0009-2614(97)00758-6,,,,,[]
,,,,Crossref,,,10.1063/1.1564060,,,,,[]
,,,,Crossref,,,10.1103/physrevb.100.035439,,,,,[]
,,,,Crossref,,,10.1063/1.1794633,,,,,[]
,,,,Crossref,,,10.1021/acs.jpclett.5b00733,,,,,[]
,,,,Crossref,,,10.1063/1.440939,,,,,[]
,,,,Crossref,,,10.1002/qua.560520414,,,,,[]
,,,,Crossref,,,10.1103/physrev.82.625,,,,,[]
,,,,Crossref,,,10.1063/1.3317437,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.49.1691,,,,,[]
,,,,Crossref,,,10.1063/1.2403848,,,,,[]
,,,,Crossref,,,10.1103/physrevb.44.943,,,,,[]
,,,,Crossref,,,10.1103/physrevb.49.14211,,,,,[]
,,,,Crossref,,,10.1103/physrevb.52.r5467,,,,,[]
,,,,Crossref,,,10.1103/physrevb.71.035105,,,,,[]
,,,,Crossref,,,10.1103/physrevb.73.195107,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.100.146401,,,,,[]
,,,,Crossref,,,10.1103/physrevb.82.115106,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.76.102,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.92.246401,,,,,[]
,,,,Crossref,,,10.1063/1.3521275,,,,,[]
,,,,Crossref,,,10.1002/jcc.20495,,,,,[]
,,,,Crossref,,,10.1063/1.3382344,,,,,[]
,,,,Crossref,,,10.1063/1.5090222,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.102.073005,,,,,[]
,,,,Crossref,,,10.1063/1.3269802,,,,,[]
,,,,Crossref,,,10.1063/1.3545985,,,,,[]
,,,,Crossref,,,10.1063/1.477422,,,,,[]
,,,,Crossref,,,10.1063/1.2436888,,,,,[]
,,,,Crossref,,,10.1063/1.481336,,,,,[]
,,,,Crossref,,,10.1063/1.2039080,,,,,[]
,,,,Crossref,,,10.1038/sdata.2014.22,,,,,[]
,,,,Crossref,,,10.1063/1.2770701,,,,,[]
,,,,Crossref,,,10.1021/ci300415d,,,,,[]
,,,,Crossref,,,10.1063/1.2348881,,,,,[]
,,,,Crossref,,,10.1063/1.2755751,,,,,[]
,,,,Crossref,,,10.1016/j.cplett.2011.05.007,,,,,[]
,,,,Crossref,,,10.1002/jcc.24854,,,,,[]
,,,,Crossref,,,10.1021/ct2002946,,,,,[]
,,,,Crossref,,,10.1021/ct200523a,,,,,[]
,,,,Crossref,,,10.1063/1.1727484,,,,,[]
,,,,Crossref,,,10.1039/c6cp00688d,,,,,[]
,,,,Crossref,,,10.1021/ct600281g,,,,,[]
,,,,Crossref,,,10.1021/ct800568m,,,,,[]
,,,,Crossref,,,10.1021/jp049908s,,,,,[]
,,,,Crossref,,,10.1021/jp045141s,,,,,[]
,,,,Crossref,,,10.1039/c7cp04913g,,,,,[]
,,,,Crossref,,,10.1080/00268976.2017.1333644,,,,,[]
,,,,Crossref,,,10.1021/acs.jctc.9b00239,,,,,[]
,,,,Crossref,,,10.1103/physrevb.84.045115,,,,,[]
,,,,Crossref,,,10.1063/1.4812323,,,,,[]
,,,,Crossref,,,10.1038/npjcompumats.2015.10,,,,,[]
,,,,Crossref,,,10.1103/physrevb.93.235162,,,,,[]
,,,,Crossref,,,10.1088/1367-2630/aac7f0,,,,,[]
,,,,Crossref,,,10.1063/1.1626543,,,,,[]
,,,,Crossref,,,10.1063/1.1321305,,,,,[]
,,,,Crossref,,,10.1103/physrevb.63.224115,,,,,[]
,,,,Crossref,,,10.1103/physrevb.85.014111,,,,,[]
,,,,Crossref,,,10.1103/physrevlett.77.3865,,,,,[]
,,,,Crossref,,,10.1002/jcc.26872,,,,,[]
,,,,Crossref,,,10.1016/j.susc.2015.03.023,,,,,[]
,,,,Crossref,,,10.1021/acs.jpcc.7b05677,,,,,[]
